api.md:
  cross_links: []
  hash: 4512e518bca21bfdbbc97752e007d64f
  references: []
  summary: 'The API Reference Guide provides a thorough overview of various components
    related to instructors, validation, iteration, and function calls within a programming
    framework. Key topics include OpenAI instructors, DSL validators, iterable structures,
    partial applications, parallel processing, and optional operations through the
    ''maybe'' moniker. It also delves into function call mechanisms, offering developers
    essential information for implementing efficient and robust APIs. This guide serves
    as a vital resource for those seeking to enhance their understanding and application
    of API-related functionalities. Keywords: API reference, instructors, validation,
    iteration, function calls, OpenAI, DSL validators, parallel processing.'
architecture.md:
  ai_references: []
  cross_links: []
  hash: 141a2c4c63d93091402d5bf4e39b04f8
  keywords:
  - Instructor
  - LLM providers
  - Pydantic Model
  - Schema Converter
  - API Request
  - Response Parser
  - Validator
  - Retry Mechanism
  references: []
  summary: The Instructor Architecture document elucidates the internal workings of
    the Instructor system and its integration with various Large Language Model (LLM)
    providers. It details the core components that facilitate seamless interactions
    and structured data handling in a consistent manner across different providers.
  topics:
  - Core Components
  - Request Flow
  - Data Validation
  - LLM Integration
  - Structured Output
blog/index.md:
  cross_links:
  - blog/posts/aisummit-2023.md
  - blog/posts/announcing-unified-provider-interface.md
  - blog/posts/caching.md
  - blog/posts/chain-of-density.md
  - blog/posts/citations.md
  - blog/posts/distilation-part1.md
  - blog/posts/generator.md
  - blog/posts/langsmith.md
  - blog/posts/learn-async.md
  - blog/posts/llms-txt-adoption.md
  - blog/posts/logfire.md
  - blog/posts/rag-and-beyond.md
  - blog/posts/validation-part1.md
  - concepts/partial.md
  - examples/batch_job_oai.md
  - examples/bulk_classification.md
  - examples/image_to_ad_copy.md
  - integrations/llama-cpp-python.md
  - integrations/ollama.md
  - integrations/together.md
  - prompting/decomposition/least_to_most.md
  - prompting/self_criticism/chain_of_verification.md
  - prompting/self_criticism/cumulative_reason.md
  - prompting/self_criticism/reversecot.md
  hash: 04ec2689ed366f014bc3f15ce4fd0b42
  references:
  - blog/posts/announcing-unified-provider-interface.md
  - blog/posts/llms-txt-adoption.md
  - blog/posts/rag-and-beyond.md
  - blog/posts/chain-of-density.md
  - blog/posts/validation-part1.md
  - blog/posts/citations.md
  - blog/posts/distilation-part1.md
  - blog/posts/langsmith.md
  - blog/posts/logfire.md
  - blog/posts/caching.md
  - blog/posts/learn-async.md
  - blog/posts/generator.md
  - examples/batch_job_oai.md
  - examples/bulk_classification.md
  - examples/image_to_ad_copy.md
  - prompting/decomposition/least_to_most.md
  - prompting/self_criticism/chain_of_verification.md
  - prompting/self_criticism/cumulative_reason.md
  - prompting/self_criticism/reversecot.md
  - integrations/ollama.md
  - integrations/llama-cpp-python.md
  - integrations/together.md
  - concepts/partial.md
  - blog/posts/aisummit-2023.md
  summary: This document outlines various resources and updates available for users
    interested in AI development, optimization, and language model techniques. It
    encourages subscribing to a newsletter to receive updates on new features and
    tips for using "Instructor." The content includes topics on advanced AI techniques
    like the Unified Provider Interface, llms.txt adoption, and GPT-4 level summaries
    using GPT-3.5-turbo. It also covers AI model validation, function caching in Python,
    batch processing, and integrations with tools like Logfire and Pandas. Additionally,
    it introduces prompting techniques such as Least-to-Most prompting and the Reverse
    Chain of Thought (RCoT) for enhancing language model performance. Key objectives
    are to keep users informed with the latest advancements and provide practical
    tips for AI model refinement and deployment. Keywords include AI development,
    language models, optimization, Python, integrations, and prompting techniques.
blog/posts/aisummit-2023.md:
  ai_references:
  - '[AI Engineer Summit](https://www.ai.engineer/summit)'
  - '[Pydantic Documentation](https://docs.pydantic.dev/latest/)'
  - '[full talk](https://www.youtube.com/watch?v=yj-wSRJwrrc)'
  cross_links: []
  hash: f0b52aac48499d18ab5101d10da676ed
  keywords:
  - Pydantic
  - Prompt Engineering
  - AI Summit
  - Machine Learning
  - Data Validation
  references: []
  summary: This document provides insights from a keynote at the AI Engineer Summit
    on utilizing Pydantic for effective prompt engineering. The talk includes a deep
    dive into the related documentation and aims to refine the art of prompt engineering
    in AI applications.
  topics:
  - Pydantic usage
  - Prompt engineering techniques
  - AI in engineering
  - Machine learning applications
blog/posts/announcing-gemini-tool-calling-support.md:
  cross_links: []
  hash: 9918d92d63a5005bc11f4df8593d1411
  references: []
  summary: "This article introduces the latest support for structured outputs via\
    \ tool calling in the instructor library for both Gemini and VertexAI SDKs, enhancing\
    \ AI model interactions. It highlights easy installation options for Gemini (`instructor[google-generativeai]`)\
    \ and VertexAI (`instructor[vertexai]`), emphasizing Gemini\u2019s advantages\
    \ such as a higher free token quota and simpler setup with just a Google API key.\
    \ The guide provides step-by-step examples of using instructor with Gemini and\
    \ VertexAI models (`gemini-1.5-flash-latest`, `gemini-1.5-pro-latest`) for chat\
    \ completions and structured output extraction, focusing on AI SDKs, tool calling,\
    \ structured outputs, and generative models for AI developers."
blog/posts/announcing-instructor-responses-support.md:
  cross_links:
  - integrations/openai-responses.md
  hash: 8ce4314b2dee3e0af9a37baeee08ed87
  references:
  - integrations/openai-responses.md
  - integrations/openai-responses.md
  summary: The announcement highlights Instructor's integration with OpenAI's new
    Responses API, providing a streamlined, type-safe interface for structured outputs,
    web search, and citation tools. Key features include easy client initialization,
    full Pydantic validation, built-in tools for real-time information retrieval,
    and async support. This integration enhances LLM applications by simplifying external
    data referencing, maintaining compatibility with existing chat workflows, and
    enabling powerful capabilities like file search and citations without additional
    complexity. Core keywords include Instructor, Responses API, OpenAI, structured
    outputs, type safety, web search, citations, Pydantic, async support, LLM development.
blog/posts/announcing-unified-provider-interface.md:
  ai_references:
  - '[../../integrations/anthropic.md#caching'
  - ../posts/anthropic-prompt-caching.md
  - ../../concepts/prompt_caching.md
  - ../../concepts/multimodal.md
  - /concepts/patching
  - /integrations/
  - string-based-init
  - best_framework
  - introduction]
  cross_links:
  - blog/posts/anthropic-prompt-caching.md
  - blog/posts/best_framework.md
  - blog/posts/string-based-init.md
  - concepts/multimodal.md
  - concepts/prompt_caching.md
  - integrations/anthropic.md
  hash: c88097d85ac482f5383e301293764cea
  keywords:
  - from_provider
  - LLM providers
  - client initialization
  - synchronous
  - asynchronous
  - model comparison
  - structured outputs
  - multi-provider strategies
  - rapid prototyping
  references:
  - blog/posts/anthropic-prompt-caching.md
  - concepts/prompt_caching.md
  - concepts/multimodal.md
  - blog/posts/concepts/patching/index.md
  - blog/posts/integrations/index.md
  - blog/posts/string-based-init/index.md
  - blog/posts/best_framework/index.md
  - blog/posts/introduction/index.md
  summary: The `from_provider()` function in the Instructor library allows users to
    easily switch between various LLM providers using a single string identifier,
    simplifying client initialization and model experimentation. This enhancement
    automates setup procedures and supports both synchronous and asynchronous operations,
    improving efficiency for developers working with multiple language models.
  topics:
  - Functionality of from_provider
  - Key benefits of using from_provider
  - Internal workings of from_provider
  - Example usage of from_provider
  - Future improvements in LLM integration
blog/posts/anthropic-prompt-caching.md:
  ai_references:
  - '[Caching Strategies](/concepts/caching)'
  - '[Anthropic Integration](/integrations/anthropic)'
  - '[Anthropic Structured Outputs](structured-output-anthropic)'
  - '[Response Caching](caching)'
  - '[Performance Monitoring](logfire)'
  cross_links: []
  hash: 54da38a45472225872357555af50eb10
  keywords:
  - prompt caching
  - Anthropic
  - API optimization
  - cost reduction
  - latency improvement
  - caching limitations
  - developer guide
  references:
  - blog/posts/concepts/caching/index.md
  - blog/posts/integrations/anthropic/index.md
  - blog/posts/structured-output-anthropic/index.md
  - blog/posts/caching/index.md
  - blog/posts/logfire/index.md
  summary: This document explores the benefits of using prompt caching with Anthropic,
    highlighting its ability to improve response times and reduce costs for applications
    requiring large context management. It includes a quickstart guide, implementation
    examples, and discusses key limitations and considerations for developers eager
    to optimize API interactions.
  topics:
  - prompt caching implementation
  - API usage optimization
  - caching limitations
  - character extraction example
  - performance monitoring
blog/posts/anthropic-web-search-structured.md:
  cross_links: []
  hash: 9a5a79e8e389eb7265944a8968db3fa9
  references: []
  summary: Learn how to leverage Anthropic's web search tool with Instructor to access
    real-time, structured data from the web. This powerful combination enables AI
    models like Claude to fetch the latest information, generate organized responses
    using Pydantic models, and cite sources for verification. Key features include
    enhanced accuracy, reduced hallucinations, and customizable search configurations
    like domain restrictions and search limits. Ideal for building dynamic applications
    that require up-to-date data on topics such as sports, news, or market trends.
blog/posts/anthropic.md:
  cross_links: []
  hash: 44073f09c95cb56e33653923ef4e83c8
  references: []
  summary: This article discusses integrating Anthropic's powerful language models
    with Instructor and Pydantic for structured output generation in Python. It provides
    step-by-step guidance on installing the `instructor[anthropic]` package, configuring
    the Anthropic client with enhanced capabilities, and creating custom data models
    for precise JSON responses. Key topics include handling nested types, leveraging
    the `anthropic` client, and supporting models like Claude-3 for AI-driven applications.
    The content highlights ongoing feature development, including streaming support,
    and encourages community feedback to improve compatibility and functionality in
    API development and LLM techniques.
blog/posts/bad-schemas-could-break-llms.md:
  cross_links:
  - blog/posts/matching-language.md
  - blog/posts/timestamp.md
  - examples/index.md
  - index.md
  hash: 8d3274500a88eb0bfe0171d9f00504f8
  references:
  - blog/posts/matching-language.md
  - blog/posts/timestamp.md
  - index.md
  - examples/index.md
  summary: This article emphasizes the critical impact of response models and schemas
    on Large Language Model (LLM) performance, particularly with Claude and GPT-4o.
    Key insights include how field naming, chain-of-thought reasoning, and response
    mode choices (JSON vs. Tool Calling) significantly influence accuracy, with performance
    gains of up to 60% through optimized schemas. The content highlights the importance
    of designing well-structured response models, testing different permutations systematically,
    and using tools like Instructor for prototyping. Core keywords include LLM response
    models, structured outputs, JSON mode, tool calling, GPT-4o, Claude, reasoning
    prompts, and model performance optimization.
blog/posts/best_framework.md:
  cross_links:
  - blog/posts/introduction.md
  - concepts/iterable.md
  - concepts/parallel.md
  - concepts/partial.md
  - concepts/patching.md
  - concepts/philosophy.md
  - concepts/reask_validation.md
  - concepts/retrying.md
  - concepts/types.md
  - concepts/unions.md
  - examples/index.md
  - integrations/groq.md
  - integrations/index.md
  - integrations/llama-cpp-python.md
  - integrations/ollama.md
  - integrations/together.md
  hash: 41b529a5e2d92400da24c6f6c1e8146f
  references:
  - concepts/retrying.md
  - concepts/reask_validation.md
  - concepts/parallel.md
  - concepts/partial.md
  - concepts/iterable.md
  - concepts/types.md
  - concepts/unions.md
  - examples/index.md
  - integrations/index.md
  - integrations/together.md
  - integrations/ollama.md
  - integrations/groq.md
  - integrations/llama-cpp-python.md
  - concepts/philosophy.md
  - concepts/patching.md
  - concepts/retrying.md
  - concepts/partial.md
  - blog/posts/introduction.md
  - integrations/index.md
  - concepts/types.md
  summary: Instructor is a lightweight Python library that enhances the OpenAI SDK
    by enabling seamless mapping of LLM outputs to structured, type-safe data using
    Pydantic models and Python type annotations. It simplifies extracting structured
    data from GPTs and other compatible providers, supports features like retrying,
    validation, streaming, and parallel tool calling, and allows direct access to
    message parameters for advanced prompt engineering. Designed for easy integration
    and incremental adoption, Instructor helps teams convert unstructured LLM text
    into validated data, making it ideal for improving data consistency and reducing
    "string hell" in AI applications. Key keywords include LLM outputs, structured
    data, Python, Pydantic, OpenAI SDK, GPT, data mapping, response_model.
blog/posts/caching.md:
  cross_links:
  - blog/posts/anthropic-prompt-caching.md
  - blog/posts/learn-async.md
  - concepts/caching.md
  - concepts/parallel.md
  - concepts/prompt_caching.md
  - examples/batch_job_oai.md
  hash: 11fdb88f500185d84f0a06cc2a4b4c41
  references:
  - concepts/caching.md
  - concepts/prompt_caching.md
  - concepts/parallel.md
  - blog/posts/anthropic-prompt-caching.md
  - blog/posts/learn-async.md
  - examples/batch_job_oai.md
  summary: This article explores advanced caching techniques in Python to optimize
    performance when working with Pydantic models and language model APIs like OpenAI.
    It covers in-memory caching with `functools.cache`, persistent caching with `diskcache`,
    and distributed caching using `redis`. The content emphasizes creating custom
    decorators to cache API responses effectively, with a focus on serialization,
    cache invalidation considerations, and selecting appropriate caching strategies
    for small and large-scale applications. Keywords include Python caching, Pydantic
    models, performance optimization, in-memory caching, diskcache, Redis, API response
    caching, and distributed systems.
blog/posts/chain-of-density.md:
  cross_links:
  - blog/posts/validation-part1.md
  - cli/finetune.md
  hash: 1ff99278946f900cba0eb4b22d8c663a
  references:
  - blog/posts/validation-part1.md
  - cli/finetune.md
  summary: "This article explores advanced AI summarization techniques, focusing on\
    \ the Chain of Density method with GPT-3.5 and GPT-4. It details how to implement\
    \ iterative, entity-dense summaries, fine-tune GPT-3.5 models for improved performance,\
    \ and achieve significant efficiency gains\u2014up to 20x faster and 50x cost\
    \ savings. The guide covers data modeling, validation with Pydantic, and custom\
    \ prompting for high-quality summaries. Keywords include GPT-3.5, GPT-4, Chain\
    \ of Density, summarization, fine-tuning, LLM techniques, entity density, AI text\
    \ summarization, Instructor library, model distillation, OpenAI, cost efficiency,\
    \ latency reduction."
blog/posts/chat-with-your-pdf-with-gemini.md:
  ai_references:
  - '[multimodal-gemini.md'
  - generating-pdf-citations.md
  - rag-and-beyond.md
  - ../../concepts/retrying.md
  - ../../index.md]
  cross_links:
  - blog/posts/generating-pdf-citations.md
  - blog/posts/multimodal-gemini.md
  - blog/posts/rag-and-beyond.md
  - concepts/retrying.md
  - index.md
  hash: 902b85d5f28f8de856e9e59b6bb79faf
  keywords:
  - '[Google Gemini'
  - Document Processing
  - PDF Analysis
  - Pydantic
  - Python
  - Multimodal Capabilities
  - Structured Output]
  references:
  - concepts/retrying.md
  - blog/posts/multimodal-gemini.md
  - blog/posts/concepts/multimodal/index.md
  - blog/posts/multimodal-gemini/index.md
  - blog/posts/generating-pdf-citations/index.md
  - blog/posts/rag-and-beyond/index.md
  - index.md
  summary: This documentation provides a comprehensive guide on using Google's Gemini
    model with Instructor to efficiently process PDFs and extract structured information.
    The integration simplifies typical document processing challenges, allowing users
    to leverage multimodal capabilities to streamline data extraction into a structured
    format easily.
  topics:
  - '[PDF Processing'
  - Google Gemini Model
  - Instructor Integration
  - Multimodal Data Extraction
  - Benefits of Structured Outputs]
blog/posts/citations.md:
  ai_references:
  - '[Validation Guide](/concepts/validation)'
  - '[RAG Techniques](rag-and-beyond)'
  - '[PDF Citations](generating-pdf-citations)'
  - '[Validation Basics](validation-part1)'
  - '[finetuning a better summarizer](https://jxnl.github.io/instructor/blog/2023/11/05/chain-of-density/)'
  cross_links: []
  hash: bdc9538dce76ab09cb897edab533e546
  keywords:
  - Pydantic
  - LLM
  - Citation Verification
  - Data Accuracy
  - Python
  - Validation
  - Error Handling
  - Context Validation
  - Model Validation
  references:
  - blog/posts/concepts/validation/index.md
  - blog/posts/rag-and-beyond/index.md
  - blog/posts/generating-pdf-citations/index.md
  - blog/posts/validation-part1/index.md
  summary: This blog post explores how Pydantic can be utilized to enhance the verification
    of citations in large language models (LLMs) to improve data accuracy and reliability.
    It provides practical examples of using substring checks and LLMs for citation
    validation, as well as techniques for aligning answers with their corresponding
    citations.
  topics:
  - Citation Verification
  - Data Accuracy
  - Pydantic Validators
  - LLM Integration
  - Error Handling Techniques
blog/posts/consistent-stories.md:
  cross_links: []
  hash: b11eb15649a2a818d4d6bfcf26507cdb
  references: []
  summary: 'This article discusses how to generate complex Directed Acyclic Graphs
    (DAGs) using GPT-4o, focusing on creating consistent and coherent Choose Your
    Own Adventure stories. The challenge of generating large graphs is addressed with
    a two-phase approach: first generating a story outline, then expanding choices
    in parallel to manage context limitations and allow deeper story branches. Key
    benefits include path-specific context, parallel generation, controlled growth
    via a max_depth parameter, and rate-limiting using semaphores. The article emphasizes
    structured validation, using Pydantic models, and highlights the efficiency of
    parallel processing for content generation in large-scale language models, applicable
    through tools like instructor with OpenAI''s API. Keywords: DAGs, GPT-4o, Choose
    Your Own Adventure, story generation, language models, parallel processing, Pydantic,
    OpenAI.'
blog/posts/course.md:
  cross_links: []
  hash: 8424fc0d6b49b24ad11707b30daaddde
  references: []
  summary: 'Discover a free, one-hour course on Weights and Biases, exploring essential
    techniques for steering language models in machine learning. This comprehensive
    course covers material from detailed tutorials and is accessible to everyone interested
    in AI and machine learning. Perfect for both beginners and experienced practitioners,
    it offers valuable insights and practical tools for leveraging language models
    effectively. Access this open resource at [wandb.courses](https://www.wandb.courses/courses/steering-language-models).
    Keywords: Weights and Biases, language models, machine learning, AI course, free
    resources.'
blog/posts/cursor-rules.md:
  ai_references:
  - '[version-control-for-the-vibe-coder-part-1.md'
  - version-control-for-the-vibe-coder-part-2.md]
  cross_links: []
  hash: fccc7d93ee9d7b15bbfb41e09fd91660
  keywords:
  - '[Cursor rules'
  - Git workflows
  - AI-assisted coding
  - small commits
  - pull requests]
  references: []
  summary: This documentation discusses how Instructor's Cursor rules enhance Git
    workflows for contributors by promoting AI-assisted coding practices. It emphasizes
    the importance of small, frequent commits and provides guidance for managing pull
    requests, making contributions to projects simpler and more organized.
  topics:
  - '[Git practices'
  - AI coding
  - contributor guidelines
  - version control
  - pull request management]
blog/posts/distilation-part1.md:
  cross_links: []
  hash: 2b0cffc5cf2701d20f0f294b843aaf1e
  references: []
  summary: This guide explores using the `Instructor` library to enhance Python functions
    through fine-tuning and distillation. The library streamlines the process of developing
    task-specific language models by simplifying function calls and managing data
    preparation. Key features include automatic dataset generation for fine-tuning,
    efficient function integration, and backward compatibility. The guide covers logging
    outputs, the importance of structured outputs, and future plans for function implementation.
    Essential keywords include Instructor, fine-tuning, distillation, language models,
    Python, and dataset generation.
blog/posts/extract-model-looks.md:
  cross_links: []
  hash: 1a96f01876050a880e6d2f67bee23cb2
  references: []
  summary: "This article presents a two-phase, parallel approach to generating complex,\
    \ consistent Directed Acyclic Graphs (DAGs) and stories with GPT-4o, overcoming\
    \ limitations of large graph sizes and context window constraints. By first creating\
    \ a detailed story outline\u2014including setting, plot, choices, and visual style\u2014\
    and then expanding branches concurrently while maintaining path-specific context,\
    \ the method ensures coherence and efficiency. Key concepts include state isolation,\
    \ parallel processing, structured validation with Pydantic, and controllable story\
    \ depth. Ideal for generating large, interconnected content at scale, this approach\
    \ enhances story and graph generation speed, consistency, and complexity using\
    \ AI models like OpenAI\u2019s GPT-4o."
blog/posts/extracting-model-metadata.md:
  ai_references:
  - '[../../concepts/multimodal.md]'
  cross_links:
  - concepts/multimodal.md
  hash: caa1adf0f1bb9d67726b3f7cf6b332a4
  keywords:
  - '[metadata extraction'
  - structured extraction
  - gpt-4o
  - multimodal
  - taxonomy
  - product recommendations
  - e-commerce
  - personalization
  - instructor]
  references:
  - concepts/multimodal.md
  summary: This documentation explains how to effectively extract structured metadata
    from images using the Structured Extraction technique in conjunction with multimodal
    language models like gpt-4o. It provides insights into creating a taxonomy for
    e-commerce product categorization and demonstrates practical implementations using
    Python, making it essential for enhancing personalized recommendations in online
    retail settings.
  topics:
  - '[metadata extraction'
  - product taxonomy
  - multimodal language models
  - Python implementation
  - e-commerce personalization]
blog/posts/fake-data.md:
  ai_references: []
  cross_links: []
  hash: e94f325f97c0441ee1cdc670f4feb925
  keywords:
  - '[Synthetic Data'
  - Pydantic
  - OpenAI
  - Data Generation
  - Python
  - data modeling
  - JSON schema
  - AI-generated data]
  references: []
  summary: This documentation provides a comprehensive guide on generating synthetic
    data using Pydantic and OpenAI's models, featuring practical examples and configurations.
    Users can learn to customize synthetic data generation through various methods
    such as example setting, model adjustments, and descriptive influences on data
    output.
  topics:
  - '[Data generation with Pydantic'
  - Using OpenAI models
  - Customizing synthetic data
  - Practical examples in Python
  - JSON schema configurations]
blog/posts/full-fastapi-visibility.md:
  cross_links:
  - blog/posts/learn-async.md
  hash: b86decf8772b03d62dd49c2700936cc3
  references:
  - blog/posts/learn-async.md
  summary: This article demonstrates how Logfire enhances FastAPI applications with
    comprehensive observability and OpenTelemetry integration. It highlights easy
    setup and code integration for logging, profiling, and monitoring API endpoints,
    including handling asynchronous operations with asyncio and streaming responses
    using Instructor's Iterable support. Key topics include FastAPI, Logfire, OpenTelemetry,
    Pydantic, AsyncIO, streaming responses, and performance tracking, providing practical
    examples to improve application visibility, debugging, and error reproduction
    in production environments.
blog/posts/generating-pdf-citations.md:
  cross_links:
  - index.md
  hash: d293a327202394d87adcd15ec894381e
  references:
  - index.md
  summary: This article demonstrates how to leverage Google's Gemini model with Instructor
    and Pydantic for accurate PDF data extraction and citation generation. It highlights
    the importance of structured outputs to reduce hallucinations, ensure source-truthfulness,
    and improve reliability in document processing. The process involves PDF parsing
    with PyMuPDF, uploading files to Gemini, and creating citations for precise referencing,
    making it ideal for legal, academic, and financial applications. Key topics include
    PDF analysis, structured data validation, GPT integration, citation highlighting,
    and reducing errors in AI-generated content, with keywords like Gemini, PDF processing,
    citations, structured outputs, Pydantic, document verification, and AI accuracy.
blog/posts/generator.md:
  cross_links:
  - concepts/fastapi.md
  hash: b9ebcb6883c21f0ba7d87980c45817dd
  references:
  - concepts/fastapi.md
  summary: 'This article explores the use of Python generators to enhance Large Language
    Model (LLM) streaming, improving latency and user experience in applications like
    eCommerce and chat interfaces. It explains how generators enable efficient, real-time
    data processing and extraction, allowing for faster rendering and responsiveness.
    The post demonstrates practical implementations using the Instructor library for
    structured data extraction from streaming LLM responses, highlighting their benefits
    over traditional approaches. Key concepts include Python generators, LLM streaming,
    data pipeline optimization, and fast API integration, emphasizing how real-time
    streaming can boost performance and customer engagement. Core keywords: Python
    generators, LLM streaming, data processing, real-time API, latency reduction,
    fastapi, instructor library, structured extraction, performance optimization.'
blog/posts/google-openai-client.md:
  cross_links:
  - blog/posts/bad-schemas-could-break-llms.md
  - blog/posts/multimodal-gemini.md
  - concepts/retrying.md
  hash: 26e8561156b73b2a9b6da501c1aa7c04
  references:
  - blog/posts/bad-schemas-could-break-llms.md
  - blog/posts/multimodal-gemini.md
  - concepts/retrying.md
  summary: "This article explains why Instructor remains essential despite Google's\
    \ recent OpenAI compatibility for Gemini models. While the new integration simplifies\
    \ interactions with Gemini via OpenAI's API, it has limitations such as limited\
    \ schema support, lack of streaming, and no multimodal capabilities. Instructor\
    \ offers a provider-agnostic API, advanced schema management, streaming, multimodal\
    \ support, automatic validation, retries, and seamless provider switching\u2014\
    features crucial for building reliable, production-grade LLM applications. Keywords\
    \ include Gemini, OpenAI integration, Instructor, multimodal support, schema management,\
    \ streaming, provider agnostic, robust AI applications."
blog/posts/introducing-structured-outputs-with-cerebras-inference.md:
  cross_links: []
  hash: 9cae7568e3f7431ca1ee3b73b8a7a1b0
  references: []
  summary: Explore how to leverage Cerebras Inference for structured outputs and faster
    model processing with seamless Pydantic integration. Cerebras offers up to 20x
    faster inference compared to GPUs, making it an excellent choice for efficient
    API development. The article guides you through setting up a Cerebras Inference
    API key and using the Cerebras SDK with Pydantic models for validated responses.
    Key functionality includes creating instructor clients, using models like "llama3.1-70b",
    and supporting both synchronous and asynchronous operations. Enhance your API
    integration with features such as streaming responses in `CEREBRAS_JSON` mode
    for real-time data processing. Key topics include Cerebras Inference, Pydantic,
    fast inference, structured outputs, and API integration.
blog/posts/introducing-structured-outputs.md:
  ai_references:
  - '[../../concepts/reask_validation.md'
  - ../../concepts/lists.md
  - ../../concepts/partial.md]
  cross_links:
  - concepts/lists.md
  - concepts/partial.md
  - concepts/reask_validation.md
  hash: 85ac9a93f1b6892914274bd21ebc8498
  keywords:
  - '[OpenAI'
  - Structured Outputs
  - instructor
  - Pydantic
  - Data Validation
  - LLM Workflows
  - API
  - Vendor Lock-in]
  references:
  - concepts/reask_validation.md
  - concepts/lists.md
  - concepts/partial.md
  summary: This article explores the challenges associated with OpenAI's Structured
    Outputs and introduces 'instructor' as a solution to enhance LLM workflows. It
    discusses issues such as validation limitations, streaming difficulties, and latency
    problems while highlighting the advantages of using 'instructor' for automatic
    retries and provider flexibility.
  topics:
  - '[OpenAI Structured Outputs'
  - Validation Logic
  - Streaming Challenges
  - Latency Issues
  - instructor Features]
blog/posts/introduction.md:
  cross_links:
  - blog/posts/best_framework.md
  - blog/posts/structured-output-anthropic.md
  - concepts/models.md
  - concepts/reask_validation.md
  - index.md
  - integrations/index.md
  hash: 33cd1df34b63e686b253b5ebca7b433d
  references:
  - index.md
  - integrations/index.md
  - concepts/reask_validation.md
  - concepts/models.md
  - blog/posts/best_framework.md
  - blog/posts/structured-output-anthropic.md
  - examples/chain-of-thought.md
  summary: This article explores how Pydantic simplifies working with Language Learning
    Models (LLMs) in Python, particularly through structured JSON outputs. It highlights
    the difficulties developers face with existing LLM frameworks and showcases how
    the Pydantic-powered Instructor library streamlines interactions with language
    models, focusing on ease of use, widespread adoption, and compatibility with tools
    like OpenAI's Function Calling. By supporting modular schemas, easy validation,
    and relationship definition, Pydantic offers a more organized code structure,
    enhancing the developer experience. The piece also parallels LLM architecture
    with FastAPI, offering simple, Pythonic approaches to utilizing LLMs effectively.
    Key phrases include Pydantic, LLMs, structured JSON, OpenAI, Python, and language
    model interaction.
blog/posts/jinja-proposal.md:
  ai_references: []
  cross_links: []
  hash: c49c3ea11717caead70f820614a48932
  keywords:
  - '[Jinja'
  - Templating
  - Pydantic
  - API Development
  - Data Validation
  - Prompt Formatting
  - Versioning
  - Logging
  - Security]
  references: []
  summary: This document outlines the integration of Jinja templating into the Instructor
    platform to enhance prompt formatting, validation, versioning, and secure logging
    capabilities. By leveraging Jinja's features, Instructor will provide improved
    handling of complex prompts and better data management, ultimately boosting its
    functionality for users.
  topics:
  - '[Integration of Jinja'
  - Enhanced Formatting Capabilities
  - Data Validation
  - Version Control
  - Secure Logging]
blog/posts/langsmith.md:
  cross_links:
  - blog/posts/learn-async.md
  - examples/bulk_classification.md
  hash: 3f9c1608a2030bf77928eb024d6326e4
  references:
  - examples/bulk_classification.md
  - blog/posts/learn-async.md
  summary: "This blog post explores how LangChain's LangSmith can be integrated with\
    \ the OpenAI client to enhance functionality through seamless LLM observability.\
    \ By wrapping the OpenAI client with LangSmith and using the `instructor` package,\
    \ developers can improve their LLM applications by enabling features such as question\
    \ classification and asynchronous processing with `asyncio`. The article provides\
    \ a step-by-step guide on setting up LangSmith, installing necessary SDKs, and\
    \ implementing multi-label classification of questions using Python. It highlights\
    \ LangSmith\u2019s capabilities as a DevOps platform for developing, collaborating,\
    \ deploying, and monitoring language model applications. Key points include the\
    \ use of `wrap_openai`, rate limiting via `asyncio.Semaphore`, and customizing\
    \ the classification prompt to fit specific use cases."
blog/posts/learn-async.md:
  ai_references:
  - '[../concepts/error_handling.md'
  - ../concepts/retrying.md
  - https://docs.python.org/3/library/asyncio.html
  - https://realpython.com/async-io-python/
  - https://python.useinstructor.com
  - https://platform.openai.com/docs/guides/async]
  cross_links:
  - concepts/error_handling.md
  - concepts/retrying.md
  hash: 510b01ac35458a0b82a7f5055913fb4f
  keywords:
  - '[asyncio'
  - asyncio.gather
  - asyncio.as_completed
  - Python
  - LLM processing
  - concurrent processing
  - async programming
  - rate limiting
  - performance optimization]
  references:
  - blog/concepts/error_handling.md
  - blog/concepts/retrying.md
  summary: This documentation provides an in-depth guide on using Python's asyncio.gather
    and asyncio.as_completed for efficient concurrent processing of Large Language
    Models (LLMs). It covers various async programming patterns, rate limiting techniques,
    and performance optimization strategies vital for AI applications.
  topics:
  - '[asyncio methods'
  - concurrent execution
  - performance comparison
  - rate-limited processing
  - error handling]
blog/posts/llm-as-reranker.md:
  ai_references:
  - '[rag-and-beyond'
  - validation-part1
  - logfire]
  cross_links:
  - blog/posts/validation-part1.md
  hash: 67f340dc144300698dca7905ebdefc6b
  keywords:
  - '[LLM'
  - Pydantic
  - Instructor
  - Search Relevance
  - Reranking
  - Retrieval-Augmented Generation
  - synthetic data
  - evaluation pipeline]
  references:
  - blog/posts/rag-and-beyond/index.md
  - blog/posts/validation-part1/index.md
  - blog/posts/logfire/index.md
  summary: This blog post guides you through creating an LLM-based reranker using
    Instructor and Pydantic for enhancing search results relevance in Retrieval-Augmented
    Generation (RAG) pipelines. By utilizing structured outputs and large language
    models, you will learn to label synthetic data for fine-tuning and build an accurate
    evaluation pipeline.
  topics:
  - '[Setting Up the Environment'
  - Defining the Reranking Models
  - Creating the Reranker Function
  - Testing the Reranker]
blog/posts/llms-txt-adoption.md:
  ai_references:
  - '[llms.txt specification](https://github.com/AnswerDotAI/llms-txt)'
  - '[standard format](https://github.com/AnswerDotAI/llms-txt#format)'
  - '[GitHub](https://github.com/instructor-ai/instructor)'
  - '[Twitter](https://x.com/jxnl.co)'
  cross_links: []
  hash: 4c6baf0df522771e1991d14f88965af2
  keywords:
  - llms.txt
  - AI language models
  - documentation accessibility
  - Instructor
  - coding assistants
  - standardization
  - markdown
  - implementation
  references: []
  summary: Instructor has adopted the llms.txt specification to enhance the accessibility
    of its documentation for AI language models. This implementation allows AI tools
    to better interpret and navigate the documentation, resulting in improved code
    suggestions and a cleaner access experience for users.
  topics:
  - llms.txt specification
  - AI-documentation interaction
  - benefits of llms.txt
  - implementation guidelines
  - future of AI in coding
blog/posts/logfire.md:
  cross_links: []
  hash: 7ce79e21910ace0347fba9fd9615cfca
  references: []
  summary: The article introduces **Logfire**, an observability platform developed
    by the creators of **Pydantic**, which integrates seamlessly with libraries like
    **HTTPx** and **Instructor**. It demonstrates how Logfire can enhance application
    performance tracking through examples such as spam email classification, validation
    using `llm_validator`, and data extraction from images with **GPT-4V**. The guide
    details how to set up and use these features with Logfire, emphasizing its ease
    of integration, efficient logging capabilities, and ability to provide in-depth
    insights into application processes. Core components include **OpenAI**, **Logfire**,
    **LLM Observability**, and integration with Pydantic.
blog/posts/matching-language.md:
  ai_references: []
  cross_links: []
  hash: d3478db3ed6545cb29034b23ad22a955
  keywords:
  - '[multilingual summarization'
  - language detection
  - Pydantic
  - langdetect
  - language models
  - data validation
  - summaries
  - language match
  - AI
  - machine learning]
  references: []
  summary: This documentation explores methods to ensure that language models generate
    summaries in the same language as the source text, leveraging Pydantic for validation
    and langdetect for language identification. By integrating these techniques, the
    accuracy of multilingual summarization improves significantly.
  topics:
  - '[language model optimization'
  - summary generation
  - language detection methods
  - Pydantic usage
  - multilingual data handling]
blog/posts/migrating-to-uv.md:
  cross_links: []
  hash: 226ee4a165a8d84023029357089b8443
  references: []
  summary: This article details the migration from Poetry to UV for dependency management
    and build automation in a Python project. The author highlights UV's faster CI/CD
    performance, automatic caching, cargo-style lockfiles, and easier adoption of
    new PEP features. The article provides a step-by-step guide to converting Poetry
    lockfiles using UV, updating build configurations to use hatchling, and modifying
    GitHub Actions workflows to implement UV commands like `uv sync` and `uv run`.
    Overall, the transition resulted in a ~3x speed increase in CI jobs, simplifying
    dependency management and enhancing development efficiency. Keywords include UV,
    Poetry migration, dependency management, CI/CD speedup, Python, build automation,
    UV lockfile, GitHub actions.
blog/posts/multimodal-gemini.md:
  ai_references:
  - '[concepts/multimodal'
  - concepts/images
  - integrations/google
  - openai-multimodal
  - structured-output-anthropic
  - chat-with-your-pdf-with-gemini]
  cross_links:
  - blog/posts/openai-multimodal.md
  - blog/posts/structured-output-anthropic.md
  - integrations/google.md
  hash: 4d4d4773381b446dfd30f7438ec93e7a
  keywords:
  - '[Gemini'
  - Multimodal AI
  - Travel Recommendations
  - Pydantic
  - Python
  - Video Analysis
  - Structured Extraction
  - Recommendations]
  references:
  - blog/posts/concepts/multimodal/index.md
  - blog/posts/concepts/images/index.md
  - blog/posts/integrations/google/index.md
  - blog/posts/openai-multimodal/index.md
  - blog/posts/structured-output-anthropic/index.md
  - blog/posts/chat-with-your-pdf-with-gemini/index.md
  summary: This documentation provides a comprehensive guide on utilizing Google's
    Gemini model for multimodal structured extraction from YouTube travel videos,
    enabling users to derive structured recommendations for tourist destinations.
    By integrating video analysis with Pydantic data models, users can effectively
    extract and organize travel information for enhanced user experiences.
  topics:
  - '[Gemini Model'
  - Video Processing
  - Pydantic Data Models
  - Travel Recommendations
  - Multimodal AI Applications]
blog/posts/open_source.md:
  cross_links:
  - concepts/patching.md
  - integrations/groq.md
  - integrations/llama-cpp-python.md
  - integrations/mistral.md
  - integrations/ollama.md
  - integrations/together.md
  hash: b3cb29bb72d1746982e2bb01087f8cdf
  references:
  - integrations/llama-cpp-python.md
  - concepts/patching.md
  - integrations/ollama.md
  - integrations/groq.md
  - integrations/together.md
  - concepts/patching.md
  - integrations/mistral.md
  summary: This article explores Instructor's enhanced capabilities for integrating
    with a variety of open source and local large language models (LLMs), including
    OpenAI, Ollama, llama-cpp-python, Groq, Together AI, and Mistral. It highlights
    how Instructor supports structured data extraction and outputs through JSON mode
    and JSON schema, utilizing Pydantic for data validation. Key features include
    model patching, multi-platform compatibility, and simplified API interactions
    for in-process and remote models. The content emphasizes adaptability in AI workflows,
    offering practical code examples for implementing structured outputs with different
    providers, aiming to streamline AI development and improve model control. Core
    keywords include Instructor, structured outputs, LLMs, OpenAI, Pydantic, JSON
    schema, Ollama, llama-cpp-python, Groq, Together AI, Mistral, API integration,
    local models, AI development.
blog/posts/openai-distilation-store.md:
  cross_links: []
  hash: f192d6f81e391bb953541405d9656871
  references: []
  summary: OpenAI's API Model Distillation with Instructor enables developers to create
    smaller, efficient, and specialized AI models tailored to specific tasks. By combining
    Instructor's structured output capabilities with API Model Distillation, users
    can produce validated, consistent results while reducing latency and costs. The
    integration supports metadata, proxy kwargs, and seamlessly leverages OpenAI's
    API parameters, enhancing workflow flexibility. This approach improves model efficiency,
    precision, and scalability for AI applications, making it ideal for personalized
    and high-performance implementations. Key words include API Model Distillation,
    Instructor, openAI, structured output, model optimization, AI efficiency, and
    customized AI models.
blog/posts/openai-multimodal.md:
  ai_references:
  - '[Multimodal Guide](/concepts/multimodal)'
  - '[OpenAI Integration](/integrations/openai)'
  - '[Gemini Multimodal](multimodal-gemini)'
  - '[Prompt Caching](anthropic-prompt-caching)'
  - '[Monitoring with Logfire](logfire)'
  cross_links: []
  hash: dfb11af3ff9283e4bd538a1cb2b2b19d
  keywords:
  - OpenAI
  - Chat Completions API
  - audio processing
  - gpt-4o-audio-preview
  - natural voices
  - audio input
  - machine learning
  - accessibility features
  references:
  - blog/posts/concepts/multimodal/index.md
  - blog/posts/integrations/openai/index.md
  - blog/posts/multimodal-gemini/index.md
  - blog/posts/anthropic-prompt-caching/index.md
  - blog/posts/logfire/index.md
  summary: OpenAI has launched audio capabilities in its Chat Completions API, utilizing
    the new `gpt-4o-audio-preview` model. This update allows developers to process
    audio and text inputs flexibly, enhancing user interaction through natural voice
    generation and integrated tool functionality.
  topics:
  - audio support
  - key features
  - practical implementation
  - use cases
  - considerations
blog/posts/pairwise-llm-judge.md:
  cross_links: []
  hash: 306360d9c8a466ffc3083651c8c295df
  references: []
  summary: The article explores how to create a pairwise LLM judge utilizing the Instructor
    library and Pydantic to evaluate text relevance, demonstrating a practical application
    of structured outputs in language model interactions. It provides a detailed guide
    on setting up the environment, defining a `Judgment` model using Pydantic for
    structured results, and developing a function to assess the relevance between
    a question and a text using OpenAI's GPT-4 model. This tool, beneficial for improving
    search relevance, evaluating question-answering systems, and aiding content recommendation
    algorithms, highlights the potential of combining structured outputs with large
    language models for creating intelligent AI systems. Key concepts include LLM,
    text relevance, AI evaluation, structured outputs, and Pydantic.
blog/posts/parea.md:
  cross_links: []
  hash: 3384d1bea79b6e46e8b6c9e6681cc1cf
  references: []
  summary: 'The blog post explores how the Parea platform enhances the OpenAI instructor
    client by improving monitoring, collaboration, testing, and error tracking for
    LLM applications. Core features include automatic grouping of retries into a single
    trace, tracking validation error counts, and providing a UI for labeling JSON
    responses. It demonstrates using Parea with the OpenAI instructor to write emails
    containing links from instructor documentation, emphasizes validation error tracking
    for minimizing costs and latency, and highlights a labeling feature for fine-tuning
    using subject-matter experts. Keywords: Parea, OpenAI, LLM, instructor, validation,
    fine-tuning, error tracking, collaboration.'
blog/posts/pydantic-is-still-all-you-need.md:
  ai_references:
  - '[Data Validation with Pydantic](../../concepts/models.md)'
  - '[Ollama Integration](../../integrations/ollama.md)'
  - '[llama-cpp-python Integration](../../integrations/llama-cpp-python.md)'
  - '[Anthropic Integration](../../integrations/anthropic.md)'
  - '[Cohere Integration](../../integrations/cohere.md)'
  - '[Google Integration](../../integrations/google.md)'
  - '[Vertex AI Integration](../../integrations/vertex.md)'
  - '[Streaming Support](../../concepts/partial.md)'
  - '[Partial Documentation](../../concepts/partial.md)'
  - '[Reasking and Validation](../../concepts/reask_validation.md)'
  - '[Structured Data Extraction from Images](../../examples/image_to_ad_copy.md)'
  - '[examples](../../examples/index.md)'
  - '[Instructor Philosophy](/concepts/philosophy)'
  - '[Validation Guide](/concepts/validation)'
  - '[Validation Deep Dive](validation-part1)'
  - '[Best Framework Comparison](best_framework)'
  - '[Introduction to Instructor](introduction)'
  cross_links:
  - concepts/models.md
  - concepts/partial.md
  - concepts/reask_validation.md
  - examples/image_to_ad_copy.md
  - examples/index.md
  - index.md
  - integrations/anthropic.md
  - integrations/cohere.md
  - integrations/google.md
  - integrations/llama-cpp-python.md
  - integrations/ollama.md
  - integrations/vertex.md
  hash: 7aee5b3518acc01228f94114cd940d56
  keywords:
  - Pydantic
  - Structured Outputs
  - Data Validation
  - LLM Techniques
  - Performance Optimization
  - APIs
  - Function Calling
  - Generative UI
  - Streaming
  references:
  - concepts/models.md
  - integrations/ollama.md
  - integrations/llama-cpp-python.md
  - integrations/anthropic.md
  - integrations/cohere.md
  - integrations/google.md
  - integrations/vertex.md
  - concepts/partial.md
  - concepts/partial.md
  - concepts/reask_validation.md
  - examples/image_to_ad_copy.md
  - examples/index.md
  - blog/posts/concepts/philosophy/index.md
  - blog/posts/concepts/validation/index.md
  - blog/posts/validation-part1/index.md
  - blog/posts/best_framework/index.md
  - blog/posts/introduction/index.md
  summary: This documentation highlights the advantages of using Pydantic for structured
    outputs in language model applications. It emphasizes improved data management,
    reliability, and performance optimization by leveraging Pydantic's features such
    as validation and modular structures.
  topics: []
blog/posts/rag-and-beyond.md:
  ai_references:
  - '[validation.md'
  - llm-as-reranker.md
  - citations.md
  - chat-with-your-pdf-with-gemini.md]
  cross_links:
  - blog/posts/citations.md
  - blog/posts/generating-pdf-citations.md
  - blog/posts/llm-as-reranker.md
  - examples/exact_citations.md
  hash: 6ebc57a8dc30b182b29b88b7b7e09b39
  keywords:
  - '[Retrieval Augmented Generation'
  - query understanding
  - LLMs
  - Pydantic
  - search optimization
  - information retrieval
  - Python
  - data modeling]
  references:
  - blog/posts/concepts/validation/index.md
  - blog/posts/llm-as-reranker/index.md
  - blog/posts/citations/index.md
  - blog/posts/chat-with-your-pdf-with-gemini/index.md
  summary: This documentation explores enhancing Retrieval Augmented Generation (RAG)
    through improved query understanding to facilitate smarter search solutions. It
    outlines the limitations of basic RAG models and introduces advanced techniques
    for crafting tailored queries that leverage multiple search backends, thereby
    improving the retrieval performance in applications like personal assistants and
    search optimizations.
  topics:
  - '[RAG Model'
  - Query Understanding
  - Search Backends
  - Case Studies
  - Pydantic Integration]
blog/posts/rag-timelines.md:
  cross_links: []
  hash: 38763a866b0564e24d4eadb49e515684
  references: []
  summary: This article explores enhancing retrieval-augmented generation (RAG) systems
    with time filtering using the Python library Instructor and Pydantic models. It
    discusses how to effectively handle time-based constraints in queries, such as
    those asking for information "from the past week." By using Pydantic to model
    time filters and Instructor to integrate large language models (LLMs), developers
    can provide accurate, relevant responses to temporal queries. The article also
    addresses the nuances of handling dates and time zones, emphasizing the importance
    of standardizing and validating these aspects for consistent system performance.
    Key techniques include defining structured output models, prompting LLMs to generate
    query objects, and managing date-related complexities.
blog/posts/semantic-validation-structured-outputs.md:
  ai_references:
  - '[Semantic Validation documentation](https://python.useinstructor.com/concepts/semantic_validation/)'
  - '[Validation Fundamentals](/concepts/validation)'
  - '[LLM Validation](/concepts/llm_validation)'
  - '[Validation Deep Dive](validation-part1)'
  - '[Anthropic Prompt Caching](anthropic-prompt-caching)'
  - '[Monitoring with Logfire](logfire)'
  cross_links: []
  hash: dc3c6a4efc89c2c049393c852c9a106a
  keywords:
  - Semantic Validation
  - LLMs
  - Structured Outputs
  - Pydantic
  - Data Quality
  - Instructor API
  - Validation Strategies
  references:
  - blog/posts/concepts/validation/index.md
  - blog/posts/concepts/llm_validation/index.md
  - blog/posts/validation-part1/index.md
  - blog/posts/anthropic-prompt-caching/index.md
  - blog/posts/logfire/index.md
  summary: Discover how semantic validation with LLMs enhances the evaluation of structured
    outputs by incorporating complex, subjective, and contextual criteria beyond traditional
    rule-based systems. This innovative approach is vital for ensuring quality and
    safety in applications leveraging natural language processing.
  topics: []
blog/posts/situate-context.md:
  cross_links:
  - blog/posts/learn-async.md
  hash: 89cec5544c213f53918318c2b2ba37f9
  references:
  - blog/posts/learn-async.md
  summary: 'Learn about implementing Anthropic''s Contextual Retrieval technique to
    enhance Retrieval-Augmented Generation (RAG) systems using async processing for
    performance optimization. The technique addresses context loss when documents
    are chunked, by adding explanatory context before embedding, improving search
    retrieval. The implementation utilizes async processing with Python to process
    document chunks concurrently, achieving significant retrieval failure rate reductions.
    Key features include structured output with Pydantic models, prompt caching, and
    efficient chunking methods. This approach is ideal for optimizing RAG systems
    with improved contextual understanding and retrieval efficiency. Keywords: Contextual
    Retrieval, Async Processing, RAG Systems, Document Chunking, Performance Optimization.'
blog/posts/string-based-init.md:
  ai_references: []
  cross_links: []
  hash: 6f5961ec4076927835b157fad2542b23
  keywords:
  - Unified provider interface
  - string-based initialization
  - LLM providers
  - consistent interface
  - model switching
  - error handling
  - environment variables
  - asynchronous clients
  references: []
  summary: The Unified Provider Interface with String-Based Initialization simplifies
    the process of working with various LLM providers by allowing users to initialize
    models using a consistent string format. This approach increases code portability
    and reduces the complexity of switching between different providers, making it
    easy to manage structured outputs.
  topics:
  - Initialization of LLM providers
  - benefits of string-based initialization
  - supported providers
  - error handling and troubleshooting
  - environment variable support
blog/posts/structured-output-anthropic.md:
  ai_references:
  - '[How Patching Works](/concepts/patching)'
  - '[Anthropic Integration](/integrations/anthropic)'
  - '[Anthropic Prompt Caching](anthropic-prompt-caching)'
  - '[Unified Provider Interface](announcing-unified-provider-interface)'
  - '[Framework Comparison](best_framework)'
  cross_links: []
  hash: fa7532f861f82b3de44245cc6fae6dae
  keywords:
  - Anthropic
  - Claude
  - Instructor
  - structured outputs
  - prompt caching
  - API Development
  - Pydantic
  - Python
  - LLM Techniques
  references:
  - blog/posts/concepts/patching/index.md
  - blog/posts/integrations/anthropic/index.md
  - blog/posts/anthropic-prompt-caching/index.md
  - blog/posts/announcing-unified-provider-interface/index.md
  - blog/posts/best_framework/index.md
  summary: This guide explores how to utilize Anthropic's Claude with Instructor for
    structured outputs and prompt caching, enhancing AI application development. By
    integrating Pydantic models and leveraging prompt caching, developers can achieve
    efficiency and cost savings in their AI projects.
  topics:
  - Structured Outputs
  - Prompt Caching
  - API Integration
  - Pydantic Models
  - AI Application Development
blog/posts/tidy-data-from-messy-tables.md:
  cross_links:
  - index.md
  hash: bb66ca67fa1b7f8e98d10be0f9aff2e1
  references:
  - index.md
  summary: "This article discusses how to convert messy, unstructured tables into\
    \ tidy data using the instructor tool with structured outputs, simplifying data\
    \ cleaning and analysis. It highlights common issues with messy exports\u2014\
    such as merged cells, implicit relationships, and mixed data types\u2014and demonstrates\
    \ how defining custom types and leveraging AI-powered extraction can automatically\
    \ produce clean pandas DataFrames. The approach enables efficient processing of\
    \ multiple tables from images, facilitating seamless integration with data analysis\
    \ and visualization workflows. Key concepts include data tidying, structured outputs,\
    \ pandas, AI-driven data extraction, and productivity in data analysis pipelines."
blog/posts/timestamp.md:
  cross_links:
  - blog/posts/matching-language.md
  hash: 1c148db378a535746af59ac0dd3c1cfb
  references:
  - blog/posts/matching-language.md
  summary: This article discusses solving timestamp format inconsistencies in video
    content parsing using Pydantic for data validation and a custom parser. It addresses
    the challenge of varying timestamp formats like "HH:MM:SS" and "MM:SS," which
    can cause errors in language model outputs, especially in video processing and
    NLP tasks. The solution involves defining expected formats and using a custom
    validator to normalize timestamps to a consistent "HH:MM:SS" structure, which
    reduces ambiguity and parsing errors. This method offers a robust framework for
    handling this common issue, outperforming alternative approaches like constrained
    sampling and simple JSON schema validation. The post includes test cases to demonstrate
    the solution's effectiveness. Key terms include timestamp, Pydantic, data validation,
    video processing, and NLP.
blog/posts/using_json.md:
  cross_links:
  - concepts/lists.md
  - concepts/partial.md
  - concepts/reask_validation.md
  - concepts/retrying.md
  - integrations/llama-cpp-python.md
  - integrations/ollama.md
  - integrations/together.md
  hash: c38638ce4dbfc143d9de932bda098e96
  references:
  - integrations/together.md
  - integrations/ollama.md
  - integrations/llama-cpp-python.md
  - concepts/reask_validation.md
  - concepts/retrying.md
  - concepts/lists.md
  - concepts/partial.md
  summary: Instructor is a Python library that simplifies extracting well-structured
    JSON data from Large Language Models (LLMs) like GPT-3.5, GPT-4, and open-source
    models using Pydantic models. It offers seamless integration with the OpenAI SDK,
    enabling developers to map LLM outputs to validated, type-enforced JSON structures
    with minimal syntax learning. Instructor emphasizes ease of use, validation, and
    serialization, making it ideal for working with complex JSON data in LLM applications.
    Key features include support for multiple programming languages, validation, retries,
    streaming responses, and compatibility with various LLM platforms, making it a
    powerful tool for developers seeking reliable JSON output extraction from LLMs.
blog/posts/validation-part1.md:
  ai_references:
  - '[concepts/validation'
  - concepts/reask_validation
  - semantic-validation-structured-outputs
  - bad-schemas-could-break-llms
  - pydantic-is-still-all-you-need]
  cross_links:
  - blog/posts/bad-schemas-could-break-llms.md
  - blog/posts/semantic-validation-structured-outputs.md
  - concepts/reask_validation.md
  hash: c4181c084569e3181494b163bdc2af05
  keywords:
  - '[Pydantic'
  - validation
  - machine learning
  - software reliability
  - dynamic validation
  - Instructor
  - LLM
  - Python
  - software development]
  references:
  - blog/posts/concepts/validation/index.md
  - blog/posts/concepts/reask_validation/index.md
  - blog/posts/semantic-validation-structured-outputs/index.md
  - blog/posts/bad-schemas-could-break-llms/index.md
  - blog/posts/pydantic-is-still-all-you-need/index.md
  summary: This documentation discusses the integration of dynamic, machine learning-driven
    validation using Python's Pydantic and Instructor to improve software reliability.
    It outlines methods to enhance validation processes, including the creation of
    custom validators powered by language models, thereby transitioning from traditional
    static validation techniques to a more adaptive approach.
  topics:
  - '[dynamic validation'
  - Pydantic usage
  - LLM integration
  - software reliability
  - error handling]
blog/posts/version-1.md:
  cross_links:
  - blog/posts/best_framework.md
  - concepts/reask_validation.md
  - concepts/retrying.md
  - contributing.md
  - why.md
  hash: a3436323e8334df26966f3b6ecf07788
  references:
  - why.md
  - blog/posts/best_framework.md
  - concepts/retrying.md
  - concepts/reask_validation.md
  - contributing.md
  summary: The announcement introduces Instructor 1.0.0, a simplified API for interfacing
    with OpenAI that enhances usability by providing improved typing support, data
    validation, and streamlined integration while maintaining compatibility with existing
    standards. Key features include the introduction of `instructor.from_openai` for
    client creation, consistent handling of default arguments, and support for type
    inference with methods like `create_with_completion`, `create_partial`, and `create_iterable`.
    With robust validation and error handling, the tool is designed to support multiple
    languages, maintaining ease of use across platforms. Popular amongst developers,
    Instructor boasts over 4000 GitHub stars and 120k monthly downloads. Key keywords
    include API Development, OpenAI, Data Validation, Python, and LLM Techniques.
blog/posts/why-care-about-mcps.md:
  cross_links: []
  hash: 12f0fc031ffca52b4b3526c950d51777
  references: []
  summary: "The article provides a detailed overview of the Model Context Protocol\
    \ (MCP), a standardized protocol developed by Anthropic to facilitate the interaction\
    \ between AI models and external systems. It highlights the importance of MCP\
    \ in solving integration challenges by transforming the complex M\xD7N problem\
    \ into a simplified M+N problem, allowing seamless integration of AI applications\
    \ with various tools. The article compares MCP with OpenAPI, underscoring MCP's\
    \ role in enabling AI models to autonomously discover and utilize tools with semantic\
    \ understanding, as opposed to OpenAPI's focus on human developers. Additionally,\
    \ it outlines growing adoption, development tips, and the practical applications\
    \ of MCP with platforms like Claude Desktop, Cursor, and OpenAI's Agent SDK. Keywords\
    \ include Model Context Protocol, MCP, AI integration, OpenAI, Anthropic, OpenAPI,\
    \ and AI standardization."
blog/posts/writer-support.md:
  cross_links: []
  hash: 90cad38cf2523db99ce9dd0f6d00fcb3
  references: []
  summary: The article announces the integration of Writer's enterprise-grade LLMs,
    including the Palmyra X 004 model, with the Instructor platform to enable structured
    outputs and enterprise AI workflows. It explains how to set up the integration,
    generate structured data extraction, and stream responses for improved responsiveness.
    Key features include automatic request retries, support for async processing,
    and usage examples for data extraction, classification, and validation. Keywords
    include Writer, Instructor, enterprise AI, structured outputs, Palmyra X 004,
    API integration, streaming, retries, and AI workflows.
blog/posts/youtube-flashcards.md:
  ai_references:
  - '[youtube-transcripts.md'
  - ../../examples/exact_citations.md
  - ../../examples/knowledge_graph.md
  - ../../concepts/retrying.md
  - https://burr.dagworks.io/examples/deployment/web-server/
  - https://burr.dagworks.io/concepts/state-persistence/
  - https://burr.dagworks.io/concepts/additional-visibility/
  - https://burr.dagworks.io/concepts/streaming-actions/]
  cross_links:
  - blog/posts/youtube-transcripts.md
  - concepts/retrying.md
  - examples/exact_citations.md
  - examples/knowledge_graph.md
  hash: 885c1f1a27cca5ec2eeaa7d0bad3951f
  keywords:
  - flashcard generator
  - Instructor
  - Burr
  - LLM
  - YouTube transcripts
  - OpenAI
  - data processing
  - observability
  - application development
  - Python
  references:
  - blog/posts/youtube-transcripts.md
  - examples/exact_citations.md
  - examples/knowledge_graph.md
  - concepts/retrying.md
  summary: This blog post demonstrates how to create a flashcard generator application
    using Instructor and Burr, leveraging LLMs to produce structured question-answer
    pairs from YouTube transcripts. The process involves defining output models, retrieving
    video transcripts, and utilizing the Burr framework to build an interactive application
    for enhanced learning experiences.
  topics: []
blog/posts/youtube-transcripts.md:
  cross_links: []
  hash: f6904e13b76dc8a15942b76c76104f90
  references: []
  summary: This article outlines how to extract and summarize YouTube video transcripts
    into structured chapters using Python, Pydantic, and OpenAI's GPT models. It demonstrates
    how to fetch transcripts with the `youtube_transcript_api`, define Pydantic models
    for chapters and other content types, and generate detailed chapter summaries
    with AI. The tutorial focuses on analyzing video content, creating adaptable data
    models for study notes, content summaries, and quizzes, enhancing content organization
    and application development for video summarization, data processing, and AI-powered
    content analysis. Key keywords include YouTube transcripts, Python, Pydantic,
    GPT, data processing, video summarization, and AI applications.
cli/batch.md:
  ai_references: []
  cross_links: []
  hash: 15ff29a13a9e380bdd9396887977adb9
  keywords:
  - '[OpenAI CLI'
  - batch jobs
  - manage jobs
  - cancel job
  - create job
  - download results
  - Anthropic
  - command line interface]
  references: []
  summary: This documentation provides a guide on managing batch jobs using the OpenAI
    Command Line Interface (CLI), detailing commands for creating, listing, canceling,
    and downloading batch jobs. It highlights dual support for both OpenAI and Anthropic
    platforms, enabling efficient job management suited to user needs.
  topics:
  - '[Batch Job Management'
  - CLI Commands
  - OpenAI
  - Anthropic
  - Job Creation and Handling]
cli/finetune.md:
  ai_references: []
  cross_links: []
  hash: a54a9cf44d3d0e7830eb2d66a854c720
  keywords:
  - Instructor CLI
  - fine-tuning jobs
  - OpenAI
  - command line interface
  - job management
  - upload files
  - training models
  - monitoring jobs
  references: []
  summary: This documentation provides an overview of managing fine-tuning jobs using
    the Instructor CLI for OpenAI, detailing essential commands and options to create,
    view, and manage these jobs effectively. Users can easily upload files for training,
    monitor job statuses, and contribute to the development of the CLI tool.
  topics:
  - Managing Fine-Tuning Jobs
  - Creating Fine-Tuning Jobs
  - Viewing Files and Jobs
  - CLI Commands
cli/index.md:
  cross_links:
  - cli/finetune.md
  - cli/usage.md
  hash: 8331441083b208ef53688aa8ca292269
  references:
  - cli/usage.md
  - cli/finetune.md
  - cli/usage.md
  - cli/finetune.md
  - cli/usage.md
  - cli/finetune.md
  summary: 'The Instructor CLI Tools offer a suite of command-line utilities designed
    to enhance workflows when using OpenAI''s API by monitoring usage, fine-tuning
    models, and accessing documentation. Key features include commands for tracking
    API usage and costs, creating and managing fine-tuned models, and quick access
    to documentation directly from the terminal. Users can install the tools via `pip
    install instructor` and must set the OpenAI API key as an environment variable.
    Additional resources and support are available through GitHub and the community
    Discord. Keywords: Instructor CLI Tools, command-line utilities, OpenAI API, usage
    monitoring, model fine-tuning, documentation access.'
cli/usage.md:
  cross_links: []
  hash: 95aa3f140fe59a144287c98679c27c15
  references: []
  summary: 'The OpenAI API Usage CLI Guide provides detailed instructions on monitoring
    OpenAI API usage using a command-line interface tool. This tool allows users to
    track API usage by model, date, and cost, offering commands like `list` to display
    usage data over the past few days. Key features include listing usage for a specified
    number of days and checking today''s usage. The guide also invites users to contribute
    to the development of this utility via GitHub. Keywords: OpenAI API, CLI tool,
    API usage monitoring, command-line interface, OpenAI models, usage tracking, GitHub
    contribution.'
concepts/alias.md:
  cross_links: []
  hash: 8c7fc8fbbe513d178333a7986a8227bb
  references: []
  summary: This overview highlights the use of aliases in Pydantic for improved data
    validation and model serialization. It explains how aliases enable mapping between
    external data field names and internal model attributes, facilitating seamless
    data parsing. The page emphasizes exploring Pydantic's latest features and documentation
    related to aliases, essential for efficient data handling and validation in Python
    applications. Key concepts include alias definition, usage, and best practices
    for leveraging aliases to enhance data model flexibility.
concepts/caching.md:
  cross_links:
  - blog/posts/caching.md
  hash: ac0e8043ff4b03799692dbd4910d2e64
  references:
  - blog/posts/caching.md
  summary: This guide explores various Python caching techniques including in-memory,
    disk-based, and Redis caching to optimize application performance. It covers the
    use of `functools.cache` for simple in-memory caching, ideal for small to medium
    applications with immutable arguments. Additionally, it demonstrates persistent
    caching with `diskcache` and distributed caching with Redis, both utilizing a
    shared `instructor_cache` decorator that serializes Pydantic models for efficient
    data storage. Key concepts include cache invalidation considerations, cache key
    generation, and serialization techniques, making these methods suitable for reducing
    computation time, handling large datasets, and supporting scalable, distributed
    systems. Core keywords include Python caching, in-memory cache, diskcache, Redis,
    Pydantic, cache decorators, performance optimization, and persistent storage.
concepts/dictionary_operations.md:
  ai_references: []
  cross_links: []
  hash: cb4a0b1f3bdaf4825aea51d32aead1ef
  keywords:
  - dictionary operations
  - performance optimization
  - message extraction
  - retry functions
  - message handler
  - system message handling
  references: []
  summary: This document details the optimizations made to dictionary operations in
    the Instructor codebase, focusing on functions related to message passing and
    configuration management. Enhancements such as direct key lookups and reduced
    overhead have led to significant performance improvements in high-throughput applications.
  topics:
  - dictionary operation optimizations
  - message extraction improvements
  - retry function enhancements
  - performance benchmarks
  - testing methodologies
concepts/distillation.md:
  cross_links: []
  hash: 88f400b35fb27b4235f08e4c61053267
  references: []
  summary: 'The article introduces Instructor''s `Instructions` library for seamless
    fine-tuning of Python functions with language models like GPT-3.5-turbo. It explains
    how to automate dataset creation for model training by annotating functions that
    return Pydantic objects, simplifying the fine-tuning process, and logging outputs
    for efficient data management. The approach enables distilling function behavior
    into model weights, facilitating backward compatibility and model-switching via
    the `dispatch` mode. Key features include streamlined data preparation, automatic
    dataset generation, and easy integration for function-level fine-tuning, making
    Instructor a powerful tool for optimizing language models in Python applications.
    Keywords: Instructor, Instructions, fine-tuning, Python functions, language models,
    GPT-3.5, distillation, Pydantic, model training, dataset automation, function
    calling, backward compatibility.'
concepts/enums.md:
  cross_links: []
  hash: 727e8787171ecd5104e0689e1d83184c
  references: []
  summary: The article discusses using Enums and Literals in Pydantic for effective
    role management, highlighting their role in preventing data misalignment by standardizing
    user roles. Key topics include the implementation of Enums with a fallback "Other"
    option to handle uncertainties, and an alternative approach using Literals for
    role definitions. Core ideas emphasize the importance of standardization and flexibility
    in model design, specifically for roles like "PRINCIPAL", "TEACHER", "STUDENT",
    and "OTHER". Keywords include Enums, Literals, Pydantic, role management, data
    standardization, and fallback options.
concepts/error_handling.md:
  cross_links:
  - concepts/hooks.md
  - concepts/retrying.md
  - concepts/validation.md
  hash: 5007d7c8abe6942912b823c5e9d22130
  references:
  - concepts/retrying.md
  - concepts/validation.md
  - concepts/hooks.md
  summary: This guide on Error Handling in Instructor provides a comprehensive overview
    of managing exceptions and errors when using Instructor for structured outputs.
    It details the exception hierarchy, including `InstructorError` and specific exceptions
    like `IncompleteOutputException`, `InstructorRetryException`, `ValidationError`,
    `ProviderError`, `ConfigurationError`, `ModeError`, and `ClientError`. The content
    offers best practices for catching specific exceptions, handling provider and
    configuration errors, logging, graceful degradation, and integrating hooks for
    error monitoring. Key concepts include exception hierarchy, error handling strategies,
    provider setup issues, validation failures, mode errors, and retry logic, ensuring
    robust and resilient use of Instructor for AI model integrations. Keywords include
    Instructor error handling, exceptions, validation, retries, provider errors, configuration
    issues, hooks, and debugging.
concepts/fastapi.md:
  cross_links: []
  hash: 4a9d66d0b46d7f503078520ae02f08fa
  references: []
  summary: 'This guide explores how to integrate Pydantic models with FastAPI for
    efficient API development. FastAPI is a high-performance Python web framework
    known for its seamless Pydantic integration, automatic OpenAPI documentation,
    and JSON Schema validation. The article provides code examples demonstrating how
    to start a FastAPI app with POST requests, handle data with Pydantic models, and
    implement streaming responses using FastAPI and large language models (LLMs).
    Key features include automatic interactive API documentation accessible via a
    `/docs` page, making API testing straightforward. SEO Keywords: FastAPI, Pydantic
    models, API development, Python, OpenAPI, JSON Schema, streaming responses, AsyncIO.'
concepts/fields.md:
  ai_references:
  - '[fields.md]'
  cross_links: []
  hash: e65b44dd148bbd793a17c362400b05f6
  keywords:
  - Pydantic
  - Field
  - metadata
  - JSON schema
  - default values
  - exclude
  - Annotated
  - customization
  - model generation
  references: []
  summary: This documentation provides comprehensive guidance on customizing Pydantic
    models using field metadata through the `Field` function. It covers setting default
    values, excluding fields, omitting fields from schemas, and customizing JSON schema
    properties to enhance model definitions effectively.
  topics:
  - Default values
  - Exclude parameter
  - Skipping fields in schemas
  - JSON schema customization
  - Using Annotated
concepts/hooks.md:
  ai_references:
  - '[instructor/hooks.py'
  - instructor/retry.py]
  cross_links: []
  hash: 3bfaa1615e24ee4bfe165847f04e2f78
  keywords:
  - '[Instructor library'
  - hooks
  - event handling
  - logging
  - error handling
  - custom hooks
  - completion
  - response]
  references: []
  summary: This documentation explains the use of hooks in the Instructor library
    for managing event handling during API interactions. It details various hook events,
    their implementation, types, and examples of usage for logging, error handling,
    and creating custom hooks to enhance functionality.
  topics:
  - '[Overview of hooks'
  - Supported hook events
  - Implementation details
  - Example usage
  - Advanced custom hooks]
concepts/index.md:
  ai_references:
  - '[models.md'
  - patching.md
  - types.md
  - validation.md
  - prompting.md
  - multimodal.md
  - fields.md
  - lists.md
  - typeddicts.md
  - unions.md
  - enums.md
  - maybe.md
  - alias.md
  - partial.md
  - iterable.md
  - raw_response.md
  - retrying.md
  - reask_validation.md
  - hooks.md
  - caching.md
  - prompt_caching.md
  - usage.md
  - parallel.md
  - fastapi.md
  - typeadapter.md
  - templating.md
  - distillation.md
  - philosophy.md
  - examples/index.md
  - getting-started.md
  - integrations/index.md]
  cross_links:
  - api.md
  - blog/posts/anthropic-prompt-caching.md
  - blog/posts/caching.md
  - blog/posts/openai-multimodal.md
  - cli/usage.md
  - concepts/alias.md
  - concepts/caching.md
  - concepts/distillation.md
  - concepts/enums.md
  - concepts/fastapi.md
  - concepts/fields.md
  - concepts/hooks.md
  - concepts/iterable.md
  - concepts/lists.md
  - concepts/maybe.md
  - concepts/models.md
  - concepts/multimodal.md
  - concepts/parallel.md
  - concepts/partial.md
  - concepts/patching.md
  - concepts/philosophy.md
  - concepts/prompt_caching.md
  - concepts/prompting.md
  - concepts/raw_response.md
  - concepts/reask_validation.md
  - concepts/retrying.md
  - concepts/semantic_validation.md
  - concepts/templating.md
  - concepts/typeadapter.md
  - concepts/typeddicts.md
  - concepts/types.md
  - concepts/unions.md
  - concepts/usage.md
  - concepts/validation.md
  - examples/index.md
  - getting-started.md
  - index.md
  - integrations/index.md
  - learning/patterns/field_validation.md
  - learning/patterns/optional_fields.md
  - learning/streaming/lists.md
  - learning/validation/field_level_validation.md
  - prompting/thought_generation/chain_of_thought_zero_shot/analogical_prompting.md
  - prompting/thought_generation/chain_of_thought_zero_shot/step_back_prompting.md
  - prompting/zero_shot/emotion_prompting.md
  - prompting/zero_shot/role_prompting.md
  - prompting/zero_shot/style_prompting.md
  hash: c930b21dfb81d99009dc6a26057ba894
  keywords:
  - '[Instructor'
  - Pydantic
  - LLM clients
  - data validation
  - performance optimization
  - streaming responses
  - integration features
  - error handling]
  references:
  - concepts/models.md
  - concepts/patching.md
  - concepts/types.md
  - concepts/validation.md
  - concepts/prompting.md
  - concepts/multimodal.md
  - concepts/fields.md
  - concepts/lists.md
  - concepts/typeddicts.md
  - concepts/unions.md
  - concepts/enums.md
  - concepts/maybe.md
  - concepts/alias.md
  - concepts/partial.md
  - concepts/iterable.md
  - concepts/raw_response.md
  - concepts/retrying.md
  - concepts/reask_validation.md
  - concepts/hooks.md
  - concepts/caching.md
  - concepts/prompt_caching.md
  - concepts/usage.md
  - concepts/parallel.md
  - concepts/fastapi.md
  - concepts/typeadapter.md
  - concepts/templating.md
  - concepts/distillation.md
  - concepts/philosophy.md
  - concepts/models.md
  - concepts/patching.md
  - concepts/reask_validation.md
  - concepts/retrying.md
  - concepts/partial.md
  - concepts/iterable.md
  - concepts/caching.md
  - concepts/usage.md
  - examples/index.md
  - getting-started.md
  - examples/index.md
  - integrations/index.md
  summary: The Instructor library provides essential concepts and features for effectively
    utilizing Pydantic models to manage structured outputs and stream responses from
    LLM clients. This documentation covers core concepts, data handling, performance
    optimization, and integration features essential for developers looking to enhance
    their applications with robust validation and error handling.
  topics:
  - '[Core Concepts'
  - Data Handling and Structures
  - Streaming Features
  - Error Handling and Validation
  - Performance Optimization]
concepts/iterable.md:
  ai_references: []
  cross_links: []
  hash: 08ea17041c45f8851c91538db7d24f85
  keywords:
  - '[structured data'
  - Streaming
  - Pydantic
  - OpenAI
  - Iterable
  - create_iterable
  - multi-task outputs
  - asynchronous usage
  - synchronous usage
  - entity extraction]
  references: []
  summary: This document provides guidance on extracting structured data in Python
    using Iterable and streaming techniques with Pydantic and OpenAI. It covers both
    synchronous and asynchronous usage, highlighting best practices for implementing
    the `create_iterable` method for efficient entity extraction and multi-task outputs.
  topics:
  - '[Iterable usage'
  - Pydantic integration
  - Synchronous and Asynchronous methods
  - Entity extraction techniques
  - Best practices for OpenAI API]
concepts/lists.md:
  cross_links: []
  hash: 87115c5871b7f897999d87d86cd68cbd
  references: []
  summary: This article explores advanced techniques for structured data extraction
    in Python using iterable and streaming capabilities with Pydantic and OpenAI.
    It demonstrates how to define schemas and utilize `Iterable[T]` for multi-task
    extraction, enabling dynamic class creation, prompt generation, and efficient
    token streaming. The guide also covers synchronous and asynchronous streaming
    methods, showcasing examples with GPT-3.5 and GPT-4 models. Key concepts include
    data serialization, real-time token processing, and leveraging instructor's API
    for scalable, schema-based entity extraction in Python, making it ideal for developers
    working on AI-driven data parsing and automation.
concepts/logging.md:
  ai_references: []
  cross_links: []
  hash: b617e0bf45b01dbbe95601ea7228f2c9
  keywords:
  - OpenAI
  - Python logging
  - DEBUG level
  - debugging
  - chat completion
  - logging setup
  - user detail extraction
  - instructor library
  references: []
  summary: This document provides a guide on how to enable DEBUG level logging for
    OpenAI requests and responses in Python. By implementing efficient logging practices,
    developers can enhance their debugging process and gain insight into the functionality
    of their OpenAI queries.
  topics:
  - logging configuration
  - debugging OpenAI requests
  - Python implementation
  - user detail model
  - OpenAI chat completion
concepts/maybe.md:
  cross_links: []
  hash: 4e245b781d8f282eb06813ed10498526
  references: []
  summary: The article explores the implementation of the Maybe pattern for error
    handling in functional programming using Python's Pydantic library. It focuses
    on how the Maybe pattern can encapsulate results and potential errors without
    resorting to exceptions or returning `None`, enhancing robust error handling.
    The pattern is implemented in a Pydantic `MaybeUser` class, which includes fields
    for the result, error status, and error message. This approach is particularly
    useful for language model (LLM) calls, reducing hallucinations. A practical example
    is provided, demonstrating how the pattern is used to extract user details from
    text inputs. Key topics include functional programming, error handling, Pydantic,
    Maybe pattern, and structural pattern matching.
concepts/models.md:
  cross_links:
  - blog/posts/rag-and-beyond.md
  hash: 14c6638223e145cb56f78b01ad3c745f
  references:
  - blog/posts/rag-and-beyond.md
  summary: This article explains how to use Pydantic for defining dynamic and static
    response models for Large Language Models (LLMs), including creating schemas with
    `BaseModel`, optional values, and runtime model generation with `create_model`.
    It highlights how to use prompt annotations and docstrings for prompt generation,
    validate API responses, and add custom behaviors or methods to models. Key concepts
    include dynamic model creation based on database or configuration data, omitting
    fields from prompts, and integrating custom logic for tailored LLM responses,
    making Pydantic a flexible tool for managing LLM output schemas and response validation.
concepts/multimodal.md:
  cross_links:
  - integrations/genai.md
  hash: 6b81751a99a294b562c47fcef3e3f496
  references:
  - integrations/genai.md
  summary: 'The article discusses Instructor''s seamless multimodal interface for
    handling images, PDFs, and audio files across various AI models like OpenAI, Anthropic,
    Google GenAI, and Mistral. Key features include creating media instances from
    URLs, file paths, and base64 strings, alongside automatic provider-specific formatting,
    ensuring clean, adaptable code. The Image, Audio, and PDF classes simplify interaction
    by abstracting differences among AI providers, while additional features like
    Anthropic prompt caching and Google GenAI file support enhance functionality.
    This comprehensive approach streamlines application development, emphasizing consistency,
    efficiency, and adaptability across AI technologies. Key terms: multimodal interface,
    AI models, image analysis, PDF parsing, audio processing, Anthropic caching, Google
    GenAI, Instructor API.'
concepts/parallel.md:
  cross_links: []
  hash: ef1722f94742cadf3b5dbfa93d7c62f1
  references: []
  summary: OpenAI's experimental Parallel Function Calling enables developers to call
    multiple functions simultaneously within a single request, significantly reducing
    application latency. Supported currently by Google and OpenAI, this feature allows
    for efficient execution of tools such as weather data retrieval and web searches
    without needing complex parent schemas. Using specific modes like `PARALLEL_TOOLS`
    for OpenAI and `VERTEXAI_PARALLEL_TOOLS` for Vertex AI, developers can specify
    response models as iterables of multiple object types (e.g., Weather, GoogleSearch).
    Key concepts include reduced latency, parallel tool execution, and dynamic response
    handling with Pydantic models, making it an important optimization for AI-powered
    applications.
concepts/partial.md:
  cross_links: []
  hash: d8cf2df0b922d2a39bf024aeabca278e
  references: []
  summary: This article explains how to use instructor and OpenAI for streaming partial
    responses in Python, enabling incremental model outputs suitable for real-time
    applications like UI rendering. It covers field-level streaming with `create_partial`,
    handling incomplete data with `PartialLiteralMixin`, and managing response models
    as generators that yield progressive updates. The guide highlights limitations
    such as unsupported validators during streaming and provides practical examples,
    including extracting conference information with asynchronous streaming support.
    Key concepts include field-level partial responses, model streaming, generator-based
    incremental updates, and integration with OpenAI's APIs for real-time data processing.
concepts/patching.md:
  cross_links:
  - concepts/parallel.md
  - integrations/vertex.md
  hash: 73bf8b99f5d3d3eb6601921d99f93932
  references:
  - integrations/vertex.md
  - concepts/parallel.md
  summary: The document discusses how the Instructor tool enhances Large Language
    Model (LLM) client libraries by patching them to support structured outputs. Core
    features include adding parameters like `response_model`, `max_retries`, and `validation_context`
    to methods in the client, enabling structured responses. It outlines different
    patching modes such as TOOL, GEMINI, and JSON for various LLM providers like OpenAI
    and Gemini, helping ensure compatibility and improved data handling. Patching
    is aimed at facilitating stable tool calling, managing validations, and providing
    JSON outputs. Keywords include structured output, LLM client libraries, Instructor
    tool, OpenAI, Gemini, patching, and tool calling.
concepts/philosophy.md:
  ai_references: []
  cross_links: []
  hash: 9506a8bcecbdedb5e5b9c6098031e787
  keywords:
  - Instructor
  - simplicity
  - Pydantic
  - LLMs
  - composability
  - observability
  - vendor lock-in
  - Python
  references: []
  summary: The Philosophy documentation of Instructor outlines its fundamental principles
    emphasizing simplicity and developer familiarity. By leveraging existing knowledge
    of frameworks like Pydantic, Instructor aims to minimize complexity while enhancing
    observability and composability, ensuring developers maintain control and can
    evolve their code naturally without fear of vendor lock-in.
  topics:
  - Philosophy of Instructor
  - Developer Familiarity
  - Observability and Debugging
  - Composability of Code
  - Avoiding Lock-in
concepts/prompt_caching.md:
  cross_links:
  - blog/posts/anthropic-prompt-caching.md
  hash: 580600c0f70f02c1892b24456a32cdcc
  references:
  - blog/posts/anthropic-prompt-caching.md
  summary: Prompt caching is an optimization feature in OpenAI and Anthropic APIs
    that enhances performance and reduces costs by caching shared prompt segments.
    In OpenAI, prompt caching works automatically for models like gpt-4o and gpt-4o-mini
    with prefix matching, requiring no code changes. Anthropic's prompt caching, now
    generally available, necessitates explicit use of the `cache_control` parameter
    and is especially beneficial for large prompts exceeding token minimums (2048
    tokens for Claude Haiku, 1024 for Claude Sonnet). This feature significantly lowers
    response times and costs by enabling cache reuse during multiple API calls, making
    it essential for efficient, large-scale language model applications. Key keywords
    include prompt caching, API optimization, OpenAI, Anthropic, cost reduction, response
    time, model models, cache management, and large prompt handling.
concepts/prompting.md:
  cross_links: []
  hash: e27dde9b271c8c6944f53125f39a0042
  references: []
  summary: The article provides a comprehensive guide on effective prompt engineering
    using Pydantic and Instructor, focusing on enhancing modularity, flexibility,
    and data integrity in Python models. Key strategies include designing self-descriptive
    and reusable components, employing enums and literals for standardization, and
    handling errors with the Maybe pattern. The guide also recommends using optional
    attributes, reiterating long instructions, managing list lengths, and defining
    entity relationships to improve data quality. By incorporating these practices,
    developers can ensure better structure, clarity, and maintainability in their
    applications.
concepts/raw_response.md:
  cross_links: []
  hash: 44557d68c40cf4d99ef68b41047544ef
  references: []
  summary: This guide provides a tutorial on creating custom models using OpenAI's
    API with Python. It specifically demonstrates how to use the `instructor` library
    to extract user data efficiently by integrating OpenAI's GPT model, such as "gpt-3.5-turbo,"
    with Pydantic for response validation. The example illustrates extracting user
    attributes like name and age from a text input using the `UserExtract` model.
    Additionally, the tutorial explains accessing raw responses from Anthropic models
    for debugging purposes. Key concepts include OpenAI completions, data extraction,
    custom client, and Pydantic models.
concepts/reask_validation.md:
  cross_links:
  - examples/exact_citations.md
  hash: eda13e17af5b47f10ddff3a58680307f
  references:
  - examples/exact_citations.md
  summary: This article explores enhancing AI validation processes using Pydantic's
    flexible validation framework for both code-based and LLM-based outputs. Key techniques
    include defining custom validators, leveraging reasking with retry mechanisms,
    and advanced validation methods like model-level validation and context-aware
    checks. It emphasizes improving AI output accuracy, handling validation errors
    effectively, and optimizing token usage by disabling URL links in error messages.
    Core keywords include Pydantic, AI validation, LLM validation, reasking, validation
    errors, JSON decoding, token optimization, and autonomous system improvement.
concepts/retrying.md:
  ai_references:
  - '[error_handling.md'
  - validation.md
  - async.md]
  cross_links:
  - concepts/error_handling.md
  - concepts/reask_validation.md
  - concepts/semantic_validation.md
  - concepts/validation.md
  - learning/patterns/field_validation.md
  - learning/validation/field_level_validation.md
  hash: 3d4bfd872b30538bfe5f7f3d124da08b
  keywords:
  - tenacity python
  - python retry
  - instructor retry logic
  - exponential backoff
  - python error handling
  - LLM retry
  - API retry
  - python resilience
  - automatic retries
  - circuit breaker pattern
  references:
  - concepts/error_handling.md
  - concepts/validation.md
  - concepts/async.md
  - concepts/error_handling.md
  - concepts/async.md
  summary: This comprehensive guide covers Python retry logic using the Tenacity library
    and Instructor for handling various failure scenarios in LLM applications. It
    details concepts such as exponential backoff, conditional retries, and logging
    practices to ensure robust error handling and resilience in API interactions.
  topics:
  - Tenacity library
  - Python error handling
  - exponential backoff strategies
  - conditional retries
  - robust API integration
concepts/semantic_validation.md:
  ai_references:
  - '[validation.md'
  - custom_validators.md
  - api.md]
  cross_links:
  - api.md
  - concepts/validation.md
  - learning/validation/custom_validators.md
  hash: 5de312bf6c73ce978ffc4ce041c00493
  keywords:
  - '[semantic validation'
  - LLMs
  - natural language criteria
  - Instructor framework
  - content moderation
  - validation criteria]
  references:
  - concepts/validation.md
  - learning/validation/custom_validators.md
  summary: This guide explains how to implement semantic validation using LLMs in
    the Instructor framework, allowing for validation against complex natural language
    criteria. By leveraging LLM capabilities, it addresses situations where traditional
    rule-based validation falls short, including subjective qualities and contextual
    relationships in data.
  topics:
  - '[semantic validation'
  - implementation with LLMs
  - content moderation
  - validation flow
  - advanced validation patterns]
concepts/templating.md:
  cross_links: []
  hash: 8b3f459aae3b028d9cdfc85a670095de
  references: []
  summary: This guide explores effective prompt templating using Jinja and Pydantic
    to create dynamic, secure, and maintainable prompts for AI models. It highlights
    how to pass context variables for prompt rendering and validation, implement complex
    logic with Jinja syntax, and integrate Pydantic validators for context-aware validation,
    including handling sensitive data with SecretStr. Emphasis is placed on security
    through sandboxed Jinja environments and best practices for managing sensitive
    information, enabling flexible, secure, and scalable prompt engineering for AI
    applications. Key keywords include prompt templating, Jinja, Pydantic, context
    variables, validation, security, secrets, and dynamic prompts.
concepts/typeadapter.md:
  cross_links: []
  hash: 40fefdf3e9f6d305e1c2280d9fc8b944
  references: []
  summary: This page provides an overview of Pydantic's Type Adapter concepts, detailing
    ongoing updates and developments. It highlights the core ideas of adapting and
    customizing data validation and serialization using Pydantic's type system. The
    page serves as a work in progress, directing users to the official Pydantic documentation
    for latest information on Type Adapters, a key feature for flexible data modeling
    and type management. Key keywords include Pydantic, Type Adapter, data validation,
    type customization, and Python data modeling.
concepts/typeddicts.md:
  cross_links: []
  hash: 81e543be61c6eae101e7f1fc5bd324ec
  references: []
  summary: The document provides a tutorial on using TypedDicts in Python when working
    with the OpenAI API for structured data responses. It explains how to define a
    TypedDict class to specify structured data types, such as strings and integers,
    and demonstrates its integration with the OpenAI API through the `instructor`
    library. The example provided showcases the creation of a structured response
    model, using a `User` TypedDict to parse a response from the GPT-3.5-turbo model,
    highlighting ease of use and strong typing for better handling API responses.
    Key concepts include Python TypedDicts, OpenAI API integration, structured data
    handling, and typed responses.
concepts/types.md:
  cross_links:
  - concepts/lists.md
  - concepts/partial.md
  hash: 4399736e0701f581b37e9ba09635169b
  references:
  - concepts/lists.md
  - concepts/partial.md
  summary: The article "Working with Types in Instructor" explores how to effectively
    utilize various data types in the Instructor platform, enhancing structured outputs
    from basic primitives to complex structures. Key elements include the use of simple
    types such as `str`, `int`, `float`, and `bool`, as well as complex types like
    `List`, `Dict`, `Union`, `Literal`, and `Enum`. It covers how to employ `pydantic.BaseModel`
    for structuring data and emphasizes the use of `typing.Annotated` for adding context
    and descriptions. The article also delves into advanced examples, such as converting
    markdown data to a pandas DataFrame and using lists of unions for diverse response
    types. These concepts are illustrated with practical code snippets, highlighting
    the versatility and capabilities of the Instructor framework in managing various
    data types for better API response modeling. Keywords include Instructor, data
    types, Pydantic, Python, structured outputs, and API response modeling.
concepts/union.md:
  cross_links:
  - concepts/unions.md
  hash: d19fc6ce0a547f93d856b9a2a64f2f16
  references:
  - concepts/unions.md
  summary: 'This page explains how to implement Union types in Pydantic models to
    manage multiple action types in Python applications. It highlights best practices
    for using Union types to enable flexible data validation and modeling, allowing
    models to accept different data structures. The content emphasizes handling diverse
    input scenarios effectively with Pydantic''s Union feature, providing valuable
    guidance for developers working with complex data validation and type hinting.
    Key keywords include Union types, Pydantic models, data validation, Python, type
    hints, and flexible data handling. Note: the original page has been consolidated
    into a comprehensive Union Types guide for more detailed information.'
concepts/unions.md:
  cross_links: []
  hash: eaaf35658f139d7cce326903aad2e9c2
  references: []
  summary: This guide explores the use of Union types in Instructor to handle multiple
    response formats from language models, emphasizing core concepts like basic, discriminated,
    and nested unions, as well as optional fields. It covers best practices for type
    hints, validation, and documentation, along with practical patterns such as multiple
    response types and dynamic action selection. The content highlights integrating
    Union types with Instructor for validation, streaming, error handling, and type
    checking, providing key examples and workflows for building flexible, robust LLM-based
    applications. Key words include Union types, Instructor, Pydantic, response models,
    discriminated unions, validation, streaming, error handling, dynamic actions,
    AI models, OpenAI, and type safety.
concepts/usage.md:
  cross_links: []
  hash: 80711f0189c13e1c0625c56bf2b16f58
  references: []
  summary: 'This guide explains how to handle non-streaming requests in OpenAI using
    Python, with a focus on tracking token usage and managing exceptions. It demonstrates
    accessing raw response data to monitor token consumption, including detailed usage
    metrics like prompt and completion tokens. The content also covers handling the
    IncompleteOutputException, which occurs when the context length is exceeded, by
    catching the exception and adjusting the prompt accordingly. Key concepts include
    OpenAI API, usage tracking, token management, error handling, and Python implementation.
    Keywords: OpenAI, non-streaming requests, token usage, completion metrics, IncompleteOutputException,
    Python, API management.'
concepts/validation.md:
  ai_references:
  - '[Semantic Validation](./semantic_validation.md)'
  - '[Pydantic Documentation](https://docs.pydantic.dev/)'
  - '[OpenAI Function Calling](https://platform.openai.com/docs/guides/gpt/function-calling)'
  - '[Instructor Examples](../examples/index.md)'
  cross_links:
  - concepts/semantic_validation.md
  - examples/index.md
  - index.md
  hash: f03282574862cea2b03ed9f3e727fa6e
  keywords:
  - validation
  - Instructor
  - Pydantic
  - type safety
  - error handling
  - semantic validation
  - custom validators
  - LLM outputs
  - data consistency
  references:
  - concepts/semantic_validation.md
  - concepts/semantic_validation.md
  - examples/index.md
  summary: This guide details the process of validating outputs from language models
    using the Pydantic library in the Instructor framework, emphasizing the importance
    of type safety, error handling, and maintaining data consistency. It also covers
    various validation strategies, including field validation, semantic validation,
    and the implementation of custom validators.
  topics: []
contributing.md:
  ai_references:
  - '[scripts/README.md]'
  cross_links: []
  hash: 6289db2bfabdfe2f10244ea2a3b7bd7d
  keywords:
  - Instructor library
  - contribute
  - evaluation tests
  - GitHub
  - development environment
  - issues
  - pull requests
  - documentation
  - code style
  references:
  - ../scripts/README.md
  summary: This document outlines how to contribute to the Instructor library, including
    writing evaluation tests, reporting issues, and submitting pull requests on GitHub.
    Contributors are encouraged to set up their development environments, follow code
    style guidelines, and enhance documentation for better collaboration and project
    quality.
  topics: []
examples/action_items.md:
  cross_links: []
  hash: 330c78a61f002ff6c56b77dda4ac62bf
  references: []
  summary: This article explains how to automate the extraction of action items from
    meeting transcripts using OpenAI's API and Pydantic. It details modeling action
    items as Ticket objects with subtasks, priorities, assignees, and dependencies,
    enabling efficient project management. The guide includes code examples for generating
    actionable tasks from transcripts, visualizing data with Graphviz, and emphasizes
    the importance of automating task identification to improve productivity and prevent
    overlooked responsibilities in meetings. Key keywords include action item extraction,
    meeting transcripts, OpenAI API, Pydantic, project management automation, task
    dependency, and GPT-4.
examples/audio_extraction.md:
  ai_references:
  - '[multi_modal_gemini.md'
  - ../integrations/openai.md]
  cross_links:
  - examples/multi_modal_gemini.md
  - integrations/openai.md
  hash: e0963a9b102bdd979542bcde8571c834
  keywords:
  - OpenAI
  - audio information extraction
  - Instructor library
  - Pydantic model
  - WAV format
  - GPT-4 audio
  - audio processing
  - structured information
  references:
  - examples/multi_modal_gemini.md
  - integrations/openai.md
  summary: This documentation provides a comprehensive guide on using OpenAI's audio
    capabilities with the Instructor library to extract structured information from
    audio files. It includes code examples demonstrating the extraction process into
    a defined Pydantic model, highlighting various use cases and best practices for
    effective audio processing.
  topics:
  - Audio processing
  - Information extraction
  - Code examples
  - Use cases
  - Pydantic models
examples/batch_classification_langsmith.md:
  cross_links: []
  hash: 996b30c651684530af4333e94df8f6a7
  references: []
  summary: This article explains how to enhance the OpenAI client with LangSmith and
    Instructor for improved observability, monitoring, and functionality in LLM applications.
    It demonstrates integrating LangSmith's SDK with OpenAI's chat completion API,
    using features like client wrapping and rate limiting. The guide also showcases
    applying Instructor to patch the client in TOOL mode, enabling additional capabilities.
    Key topics include LangSmith, OpenAI client integration, Instructor, rate limiting,
    question classification, and application monitoring, making it ideal for developers
    seeking scalable, observable AI solutions.
examples/batch_job_oai.md:
  cross_links: []
  hash: d13fc5a068b73df1e50ff653f20588b5
  references: []
  summary: This guide explains how to efficiently generate large-scale synthetic question-answer
    pairs using OpenAI's Batch API with Instructor. It covers creating JSONL files
    from datasets like ms-marco, leveraging batch jobs for cost-effective and high-rate
    data generation, and managing batch workflows through CLI commands. Key features
    include using Pydantic models for response parsing, handling batch job creation,
    monitoring progress, and downloading results. Important keywords include synthetic
    data generation, OpenAI Batch API, Instructor, large-scale datasets, ms-marco,
    question-answer pairs, cost-effective AI workflows, and data parsing.
examples/building_knowledge_graphs.md:
  cross_links: []
  hash: 4055c02b7485da53099015c6d456b1fc
  references: []
  summary: This tutorial offers a comprehensive guide to building knowledge graphs
    from textual data using OpenAI's API and Pydantic. It demonstrates how to extract
    structured information from unstructured text, such as identifying entities and
    relationships, and representing them as nodes and edges in a graph. The example
    includes Python code for defining graph models with Pydantic, integrating OpenAI's
    API for text processing, and generating visualizable knowledge graphs. Key concepts
    include automated knowledge graph construction, natural language processing, entity
    and relationship extraction, and Python implementation, making it an essential
    resource for data scientists and developers interested in semantic data modeling
    and knowledge graph automation.
examples/bulk_classification.md:
  cross_links:
  - blog/posts/learn-async.md
  hash: 21849e9a44f226f43e8b94a17846fa12
  references:
  - blog/posts/learn-async.md
  summary: 'This tutorial provides a comprehensive guide on implementing user-provided
    tag classification using FastAPI, Pydantic models, and the OpenAI API with async
    functions for parallel processing. It emphasizes defining flexible tag schemas
    with identifiers, instructions, and optional confidence scores, as well as validating
    tags against context to prevent hallucinations. The core objective is to enable
    effective classification of text snippets with minimal hallucination risk by constraining
    the language model through validation contexts. The tutorial demonstrates creating
    request and response models, parallelizing classification tasks with asyncio.gather,
    and integrating the system into a FastAPI endpoint. Key concepts include asynchronous
    classification, schema validation, multi-class tagging, confidence scores, and
    production deployment considerations. Key phrases: user-defined tags, text classification,
    fastapi, pydantic, openai, async processing, parallel classification, schema validation,
    confidence scoring, API integration.'
examples/classification.md:
  ai_references:
  - '[bulk_classification.md'
  - prompting_guide.md
  - prompting/index.md
  - concepts/prompting.md#literals
  - concepts/prompting.md#chain-of-thought]
  cross_links:
  - concepts/prompting.md
  - examples/bulk_classification.md
  - index.md
  - prompting/index.md
  hash: 36f9aeedada9921ccdab7afbbd6151c5
  keywords:
  - OpenAI
  - text classification
  - Pydantic models
  - single-label classification
  - multi-label classification
  - spam detection
  - NLP
  - Python
  references:
  - examples/bulk_classification.md
  - examples/bulk_classification.md
  - prompting/index.md
  summary: This tutorial provides a comprehensive guide to implementing single-label
    and multi-label text classification using the OpenAI API and Pydantic models in
    Python. By leveraging tips like using Literals for classification labels and including
    few-shot examples, users can enhance the accuracy of their NLP applications such
    as spam detection and support ticket categorization.
  topics:
  - Single-Label Classification
  - Multi-Label Classification
  - Pydantic Models
  - Chain of Thought
  - Few-Shot Examples
examples/document_segmentation.md:
  cross_links: []
  hash: 121491f63507430563385c90fc98a84f
  references: []
  summary: 'This comprehensive guide explores document segmentation using Large Language
    Models (LLMs), particularly Cohere''s command-r-plus model with 128k context length.
    It demonstrates how to organize long, complex texts into meaningful sections centered
    around key concepts by leveraging structured data classes (`Section`, `StructuredDocument`)
    and line numbering preprocessing. The approach enhances understanding of lengthy
    articles, such as tutorials on Transformer architectures, by extracting sections
    with specific topics. Key techniques include using LLMs for segmentation via system
    prompts, and reconstructing section texts based on start and end line indices.
    This method is applicable across domains for breaking down complex documents,
    code snippets, and mathematical content, improving content comprehension, summarization,
    and indexing. Keywords: document segmentation, Large Language Models, Cohere,
    Transformer, structured output, NLP, long documents, LLM-based text splitting,
    AI text organization.'
examples/entity_resolution.md:
  cross_links: []
  hash: b3f456d3d8db72c6526db22f548acca3
  references: []
  summary: This guide explains how to extract, resolve, and visualize entities from
    legal documents and contracts using AI and graph visualization tools. It details
    the data structures for representing entities and their properties, methods for
    utilizing OpenAI's GPT-4 to automate entity extraction and resolution, and techniques
    for creating interactive entity graphs with Graphviz. Key topics include legal
    document analysis, entity resolution, dependency mapping, legal tech applications,
    and data visualization. This approach enhances understanding of complex legal
    contracts by highlighting interconnected clauses, obligations, and key terms for
    improved legal analysis and workflow efficiency.
examples/exact_citations.md:
  ai_references:
  - '[examples/citation_fuzzy_match.py'
  - https://docs.pydantic.dev/usage/validators/#model-validators]
  cross_links: []
  hash: 5aba7f1ff1813838fe1fc55245ce7b53
  keywords:
  - '[AI validation'
  - Python citations
  - Fact class
  - QuestionAnswer class
  - preventing hallucinations
  - OpenAI API
  - data structures
  - model validators]
  references: []
  summary: This documentation outlines how to validate AI-generated answers in Python
    using contextual citations, preventing inaccuracies and misinformation. It introduces
    two Python classes, `Fact` and `QuestionAnswer`, that encapsulate statements and
    their validation, ensuring responses from AI are backed by direct quotes from
    provided context.
  topics:
  - '[AI-generated answers'
  - Python class validation
  - contextual citations
  - preventing hallucinations
  - OpenAI integration]
examples/examples.md:
  cross_links: []
  hash: 44560a6b059cd1c58184b4e7fccc0bb4
  references: []
  summary: This article explains how to incorporate examples into Pydantic models
    using the `json_schema_extra` parameter. By embedding practical examples within
    the model's schema, developers can enhance clarity and usability, especially for
    JSON schema generation and API documentation. The provided example demonstrates
    adding illustrative question-answer pairs to a `SyntheticQA` model, showcasing
    how to improve model documentation and facilitate synthetic data generation with
    OpenAI's GPT models. Keywords include Pydantic, JSON schema, model examples, data
    validation, API documentation, synthetic data, OpenAI, and schema customization.
examples/extract_contact_info.md:
  cross_links: []
  hash: 7a678fb17f5c490628a5f68d70bd67c9
  references: []
  summary: This guide demonstrates how to automate customer lead information extraction
    using OpenAI's API and Pydantic for data validation. It focuses on modeling lead
    data with validated attributes like name and phone number, including handling
    phone number formats with country codes. The tutorial covers creating a function
    to extract multiple leads from user messages, ensuring accurate data collection
    for applications like chatbots. Key concepts include OpenAI integration, Pydantic
    data modeling, phone number validation, and automated lead extraction to streamline
    customer data management.
examples/extract_slides.md:
  ai_references: []
  cross_links: []
  hash: 1a730ef2e3541d3c778bf48e330a7242
  keywords:
  - '[AI'
  - data extraction
  - competitor analysis
  - presentation slides
  - industry categorization]
  references: []
  summary: This guide presents a method for extracting competitor data from presentation
    slides using AI technologies. It outlines the necessary data structures and functions
    needed to categorize competitors by industry, ensuring thorough information gathering
    from both text and images in slides.
  topics:
  - '[Data extraction techniques'
  - Competitor categorization
  - Industry analysis
  - AI implementation
  - Pydantic data models]
examples/extracting_receipts.md:
  cross_links: []
  hash: 1ce877006d4831a5eeeb0b64fb943fd0
  references: []
  summary: This guide demonstrates how to use Python and GPT-4, combined with Pydantic
    for data validation, to extract and validate receipt data from images for automated
    expense tracking. It covers defining structured models for items and receipts,
    implementing custom validation to ensure total amounts match itemized sums, and
    utilizing the OpenAI GPT-4 API through the Instructor library for image analysis.
    Practical examples illustrate extracting receipt details from images, enabling
    efficient financial data processing and expense management. Keywords include GPT-4,
    Python, Pydantic, receipt data extraction, expense tracking, image analysis, data
    validation, OpenAI, automation.
examples/extracting_tables.md:
  cross_links: []
  hash: f7e39386e65d144db40b0549fc836164
  references: []
  summary: This article demonstrates how to extract and convert tables from images
    into Markdown format using Python and OpenAI's GPT-Vision model. It covers building
    custom data types with Pydantic for handling Markdown tables, defining a Table
    class, and utilizing instructor's patched OpenAI client for image-based table
    extraction. Practical examples include extracting top-grossing app data from images,
    facilitating data analysis and automation. Key topics include GPT-Vision, Python
    data processing, image-to-table conversion, Markdown serialization, and leveraging
    AI for automated data extraction from images.
examples/groq.md:
  cross_links: []
  hash: 680f259ac1258ea7fe4eb11dc80babbf
  references: []
  summary: 'Learn how to perform inference using Groq with the mixtral-8x7b model,
    including setup instructions, API key acquisition from GroqCloud, and practical
    Python examples. The guide covers package installations, environment variable
    configuration, and integrating Groq with the instructor library for seamless chat
    completions. Key topics include deploying Groq for AI inference, using the from_groq
    method, and creating structured JSON outputs, making it ideal for developers seeking
    efficient AI deployment solutions with Groq''s hardware and API. Keywords: Groq
    inference, AI deployment, mixtral-8x7b model, GroqCloud API, Python example, structured
    output, chat completions, AI inference setup.'
examples/image_to_ad_copy.md:
  cross_links: []
  hash: 70f33d5dd56c606567dafe15c58c5316
  references: []
  summary: This content demonstrates how to leverage GPT-4 Vision API and ChatGPT
    to automatically generate advertising copy from product images, ideal for e-commerce,
    marketing, and retail teams. It details the process of identifying products within
    images, extracting key features and descriptions using AI models, and creating
    engaging ad headlines and persuasive marketing messages. The approach includes
    defining structured data models for products, error handling, and generating compelling
    ad copy tailored to each product. Key features include dynamic product attribute
    extraction, integration with OpenAI's vision models, and automated ad content
    creation to enhance online marketing efficiency and boost sales potential through
    effective visual-to-text conversion and advertising automation.
examples/index.md:
  cross_links:
  - examples/action_items.md
  - examples/batch_classification_langsmith.md
  - examples/batch_job_oai.md
  - examples/building_knowledge_graphs.md
  - examples/bulk_classification.md
  - examples/classification.md
  - examples/document_segmentation.md
  - examples/entity_resolution.md
  - examples/exact_citations.md
  - examples/examples.md
  - examples/extract_contact_info.md
  - examples/extract_slides.md
  - examples/extracting_receipts.md
  - examples/extracting_tables.md
  - examples/groq.md
  - examples/image_to_ad_copy.md
  - examples/knowledge_graph.md
  - examples/local_classification.md
  - examples/mistral.md
  - examples/moderation.md
  - examples/multi_modal_gemini.md
  - examples/multiple_classification.md
  - examples/ollama.md
  - examples/pandas_df.md
  - examples/partial_streaming.md
  - examples/pii.md
  - examples/planning-tasks.md
  - examples/search.md
  - examples/self_critique.md
  - examples/single_classification.md
  - examples/sqlmodel.md
  - examples/tables_from_vision.md
  - examples/tracing_with_langfuse.md
  - examples/watsonx.md
  - examples/youtube_clips.md
  - tutorials/index.md
  hash: 260e691fbc028547afdea7dfe29cccfe
  references:
  - examples/single_classification.md
  - examples/multiple_classification.md
  - examples/classification.md
  - examples/bulk_classification.md
  - examples/batch_classification_langsmith.md
  - examples/local_classification.md
  - examples/entity_resolution.md
  - examples/extract_contact_info.md
  - examples/pii.md
  - examples/exact_citations.md
  - examples/action_items.md
  - examples/search.md
  - examples/document_segmentation.md
  - examples/planning-tasks.md
  - examples/knowledge_graph.md
  - examples/building_knowledge_graphs.md
  - examples/tables_from_vision.md
  - examples/extracting_tables.md
  - examples/extracting_receipts.md
  - examples/extract_slides.md
  - examples/image_to_ad_copy.md
  - examples/youtube_clips.md
  - examples/multi_modal_gemini.md
  - examples/sqlmodel.md
  - examples/pandas_df.md
  - examples/partial_streaming.md
  - examples/self_critique.md
  - examples/moderation.md
  - examples/batch_job_oai.md
  - examples/examples.md
  - examples/tracing_with_langfuse.md
  - examples/groq.md
  - examples/mistral.md
  - examples/watsonx.md
  - examples/ollama.md
  - tutorials/index.md
  summary: The Instructor Cookbook Collection offers practical examples and recipes
    for solving real-world problems using structured outputs across various domains,
    including text processing, multi-modal media, data tools, and deployment options.
    It features comprehensive guides on text classification, information extraction,
    document processing, vision processing, database integration, streaming, API integration,
    observability, and deployment with model providers like Groq, Mistral, IBM watsonx.ai,
    and Ollama. Designed to assist developers and AI practitioners, these cookbooks
    provide complete code, explanations, and best practices for implementing AI solutions
    effectively in production environments. Key keywords include AI recipes, structured
    outputs, text processing, multi-modal AI, data integration, deployment, model
    APIs, and open-source models.
examples/knowledge_graph.md:
  cross_links: []
  hash: 1a9bafb73950d7297949d435080373a4
  references: []
  summary: This guide demonstrates how to create, visualize, and iteratively update
    knowledge graphs using Python, OpenAI's API, Pydantic, and Graphviz. It covers
    defining data structures with Node and Edge models, generating detailed knowledge
    graphs from complex topics like quantum mechanics, and visualizing these graphs
    with Graphviz. Key techniques include extracting key concepts and relationships
    with GPT-4, updating graphs step-by-step, and deduplicating nodes and edges for
    clarity. The tutorial emphasizes leveraging the Instructor library for structured
    outputs and iterative graph building, making it ideal for understanding complex
    subjects through visualizations. Core keywords include knowledge graphs, Python,
    OpenAI API, Pydantic, Graphviz, data visualization, AI, GPT-4, iterative updates,
    complex topics, and structured data modeling.
examples/local_classification.md:
  cross_links: []
  hash: c0f945e2d931625f632d70b4bfd3c92c
  references: []
  summary: This article explains how to securely classify and handle confidential
    data using local AI models with llama-cpp-python and instructor, ensuring data
    privacy and infrastructure control. It covers setup instructions for installing
    models like Mistral-7B-Instruct-v0.2-GGUF, including GPU and CPU configurations,
    along with example Python code for processing confidential document queries such
    as content analysis, access permissions, and document metadata. The guide emphasizes
    maintaining data security by performing inference locally, making it ideal for
    organizations seeking secure AI solutions for sensitive information. Key keywords
    include local AI models, confidential data classification, llama-cpp-python, instructor,
    privacy-focused AI, and secure document handling.
examples/mistral.md:
  ai_references: []
  cross_links: []
  hash: d9d17c1c67170f2291fa82e49cce4666
  keywords:
  - MistralAI
  - API key
  - inference
  - structured outputs
  - Python example
  - installation
  - pip packages
  - '`from_mistral`'
  - Mistral tools
  references: []
  summary: This documentation provides a comprehensive guide on using MistralAI models
    for generating structured outputs through inference. It covers the steps needed
    for setup, including API key generation, necessary package installations, and
    example code to demonstrate the process.
  topics:
  - MistralAI API setup
  - Package installation
  - Example usage in Python
  - User model implementation
  - Structured output generation
examples/moderation.md:
  cross_links: []
  hash: c0d290b445a8b1d1076bc82a9fd8b361
  references: []
  summary: "This document provides an example of utilizing OpenAI's moderation endpoint\
    \ to ensure content compliance with usage policies by filtering harmful content.\
    \ It explains how to implement an `AfterValidator` to automatically assess messages\
    \ for categories like hate, harassment, self-harm, sexual content, and violence.\
    \ The example includes code snippets demonstrating how to set up the moderation\
    \ validation with OpenAI\u2019s API, highlighting its ability to flag and reject\
    \ harmful or policy-violating messages. Key concepts include OpenAI moderation,\
    \ content filtering, safety validation, Pydantic integration, and ensuring API\
    \ input/output compliance for safe AI interactions."
examples/multi_modal_gemini.md:
  cross_links: []
  hash: d2d5cffd4469c75c6730fa3f130fecd1
  references: []
  summary: 'This guide explains how to utilize Gemini with Google Generative AI for
    multi-modal data processing, specifically focusing on audio files. It details
    three methods: uploading entire audio files as normal messages, passing audio
    segments inline after installing pydub, and using lists of mixed content for flexible
    processing. The instructions emphasize setting the correct mode (GEMINI_JSON),
    uploading files with genai.upload_file, and providing audio data either as file
    objects or inline audio segments. These approaches enable efficient summarization,
    transcription, and analysis of audio recordings, supporting SEO by extracting
    core ideas, objectives, key details, and relevant keywords related to audio content
    processing with Gemini and Generative AI.'
examples/multiple_classification.md:
  cross_links: []
  hash: d80a59dabf71466f2ed5bc4178dc557b
  references: []
  summary: This guide demonstrates how to implement multi-label classification for
    support ticket categorization using OpenAI's API and Pydantic. It introduces a
    custom enum and a Pydantic model to handle multiple labels such as "ACCOUNT,"
    "BILLING," and "GENERAL_QUERY," enabling effective multi-label predictions. The
    example illustrates how to set up the classification process with a tailored prompt
    and retrieve labels indicating multiple relevant categories for a given support
    ticket. Keywords include multi-label classification, OpenAI API, Pydantic, support
    ticket categorization, multi-label prediction, GPT-4, and effective support workflows.
examples/ollama.md:
  cross_links:
  - concepts/models.md
  - concepts/partial.md
  - concepts/patching.md
  - concepts/reask_validation.md
  - examples/index.md
  - index.md
  - prompting/index.md
  - why.md
  hash: 56fe05f28e384bbef8372e921efa4648
  references:
  - concepts/models.md
  - concepts/models.md
  - concepts/reask_validation.md
  - concepts/partial.md
  - examples/index.md
  - concepts/models.md
  - concepts/patching.md
  - index.md
  - why.md
  - why.md
  - concepts/models.md
  - examples/index.md
  - prompting/index.md
  summary: "This article explains how to utilize Ollama's local LLM server with the\
    \ Instructor library to generate structured outputs using Pydantic models. It\
    \ highlights the benefits of Instructor, such as a simple API, validation, reasking,\
    \ streaming support, and prompt control, enabling more precise and reliable AI\
    \ interactions. The guide provides practical steps and code examples for integrating\
    \ Ollama models like Llama 3 with Instructor\u2019s JSON schema validation, making\
    \ it easier to extract structured data from large language models for AI applications\
    \ and development."
examples/open_source.md:
  ai_references:
  - '[instructor_examples.md]'
  cross_links: []
  hash: a3046643d8e10ca464ec3be1302d1cd2
  keywords:
  - OpenAI chat API
  - open source models
  - OpenRouter
  - Perplexity
  - RunPod
  - text-generation-webui
  references: []
  summary: This document provides an overview of open source model providers that
    are compatible with the OpenAI chat API, highlighting options like OpenRouter,
    Perplexity, and RunPod LLMs. It serves as a guide for users looking to explore
    and implement these models in their applications.
  topics:
  - Open source model providers
  - compatibility with OpenAI API
  - implementation examples
  - usage of text-generation-webui
examples/pandas_df.md:
  cross_links: []
  hash: d08c46a6a8d4445ab9bf656ba28f6247
  references: []
  summary: This guide demonstrates how to extract and convert Markdown tables directly
    into Pandas DataFrames in Python. It features techniques for parsing Markdown
    data, validating the DataFrame structure, and serializing it back to Markdown
    format using Pydantic annotations. The code showcases creating functions to extract
    tables with OpenAI's GPT-3.5-turbo model, enabling efficient data extraction from
    formatted Markdown tables. Key concepts include Markdown to DataFrame conversion,
    custom annotations for validation and serialization, and extracting structured
    data like tables with titles. Keywords include Pandas, Markdown parsing, data
    extraction, GPT-3.5-turbo, Python, DataFrame, table extraction, Pydantic, and
    OpenAI.
examples/partial_streaming.md:
  cross_links: []
  hash: b4fa99932aca3dffc93d4dea2b69e036
  references: []
  summary: This article explains how to implement field-level streaming with the Instructor
    library in Python for dynamic UI rendering. It demonstrates using `Partial[T]`
    to create incremental, partial snapshots of model responses, enabling real-time
    updates. The example showcases extracting meeting and participant information
    from a text block using OpenAI's GPT-4, with streaming responses displayed via
    the Rich library. Key concepts include partial responses, stream processing, dynamic
    UI updates, and leveraging Instructor for field-level data handling in Python.
examples/pii.md:
  cross_links: []
  hash: 6cb6a88f6b787857b8da7d9a072b8cab
  references: []
  summary: This guide demonstrates how to extract and scrub Personally Identifiable
    Information (PII) from documents using OpenAI's ChatCompletion model and Python.
    It covers defining Pydantic data models to structure PII data, utilizing OpenAI's
    API to extract sensitive information such as names, emails, phone numbers, addresses,
    and SSNs, and implementing a method to scrub PII by replacing values with placeholders.
    Key features include leveraging AI for accurate PII detection, data sanitization
    techniques, and customizable scrubbing methods to ensure privacy compliance in
    document processing workflows. Suitable keywords include PII extraction, data
    scrubbing, privacy, OpenAI, Python, AI-powered data anonymization, sensitive data
    protection, and document privacy.
examples/planning-tasks.md:
  cross_links:
  - concepts/lists.md
  - examples/knowledge_graph.md
  - examples/recursive.md
  hash: 00bfdb223b5c59a4fcafe1e6e020cfe8
  references:
  - concepts/lists.md
  - examples/knowledge_graph.md
  - examples/recursive.md
  summary: This guide explains how to use OpenAI's Function Call ChatCompletion API
    for query planning in complex question-answering systems. It demonstrates how
    to define structured query models with Pydantic, create a query planner that breaks
    down a main question into dependent sub-questions, and leverages system prompts
    to generate organized query plans. The approach facilitates systematic information
    gathering, iterative querying, workflow automation, and process optimization,
    making it ideal for handling multi-step queries and knowledge graph extraction.
    Key concepts include structured schema design, dependency management, and leveraging
    OpenAI's models for automated query decomposition.
examples/recursive.md:
  cross_links:
  - examples/knowledge_graph.md
  - examples/planning-tasks.md
  hash: 32eb7db1d5fc4dc8fa262770848b0592
  references:
  - examples/planning-tasks.md
  - examples/knowledge_graph.md
  summary: This guide explains how to implement recursive schemas using Pydantic models
    in Instructor, enabling the handling of hierarchical and nested data structures
    such as organizational charts, file systems, comment threads, and task dependencies.
    It covers defining recursive models, best practices like calling `model_rebuild()`,
    validation techniques for limiting recursion depth, and performance tips for managing
    complex data. The content emphasizes the importance of clear structure, validation,
    and practical examples to effectively work with recursive schemas in AI-powered
    applications.
examples/search.md:
  cross_links: []
  hash: 86f8d684546f51c59453bfcfcdf256cc
  references: []
  summary: This article demonstrates how to segment search queries into actionable
    tasks using OpenAI Function Call and Pydantic. It showcases defining data structures
    with Pydantic, leveraging OpenAI's multi-task capabilities to split complex queries
    into multiple sub-queries, and executing them concurrently with asyncio. The example
    emphasizes extracting tasks like web searches, images, and videos from user input
    to improve virtual assistant functionality. Key concepts include OpenAI Function
    Call, Pydantic models, query segmentation, parallel execution, and applications
    in virtual assistants and search optimization.
examples/self_critique.md:
  cross_links: []
  hash: 15eeaa0bb27f7fc4c235f752faee8823
  references: []
  summary: This guide explains how to implement self-correction in NLP applications
    using `llm_validator` for enhanced response accuracy. It demonstrates integrating
    validation callbacks within pydantic models to catch objectionable content, provide
    helpful error messages, and enable automatic retries with corrections. Key concepts
    include the use of `response_model`, custom validation with `llm_validator`, and
    retry mechanisms for self-healing language model outputs, making it a valuable
    resource for improving NLP model safety, reliability, and quality control. Keywords
    include self-correction, NLP validation, `llm_validator`, pydantic validation,
    self-healing AI, response accuracy, and prompt engineering.
examples/single_classification.md:
  cross_links: []
  hash: e57ed79f3f4234a0606723bb8c07d2ee
  references: []
  summary: 'This guide demonstrates how to perform single-label text classification
    using the OpenAI API, specifically with the GPT-3.5-turbo and GPT-4 models. It
    showcases how to classify text as "SPAM" or "NOT_SPAM" with a response model,
    leveraging the instructor library for enhanced functionality. The example includes
    code for setting up the classification function, defining the response schema
    with Pydantic, and verifying predictions through sample inputs. Key features include
    the use of response_model for structured outputs, and the approach emphasizes
    simplicity and accuracy in spam detection and text classification tasks. Keywords:
    OpenAI API, single-label classification, GPT-3.5-turbo, GPT-4, text classification,
    spam detection, machine learning, natural language processing.'
examples/sqlmodel.md:
  ai_references:
  - '[concepts/fastapi.md]'
  cross_links:
  - api.md
  - concepts/fastapi.md
  hash: ef554168dab29e30a9050ba01b8122d8
  keywords:
  - '[Instructor'
  - SQLModel
  - Python
  - database integration
  - API development
  - OpenAI
  - FastAPI
  - models]
  references:
  - concepts/fastapi.md
  summary: This documentation provides a comprehensive guide on how to integrate the
    `Instructor` library with `SQLModel` in Python to facilitate database interactions.
    It includes step-by-step examples on defining models, generating records, and
    saving them to a database, ensuring seamless functionality and improved developer
    experience.
  topics:
  - '[Integration of Instructor and SQLModel'
  - Model Definition
  - Generating Records
  - Inserting data into DB
  - JSON schema management]
examples/tables_from_vision.md:
  cross_links: []
  hash: 02f100035905072561af66bed755ecf7
  references: []
  summary: This guide explains how to extract and convert tables from images into
    markdown format using OpenAI's GPT-4 Vision model. It details the process of analyzing
    images to identify table headers, generate descriptive titles and summaries, and
    output structured markdown tables with captions. The method leverages Python,
    pandas, and pydantic for data handling, emphasizing automatic data extraction,
    table serialization, and effective data presentation from visual content. Key
    concepts include image analysis, data extraction, markdown formatting, and GPT-4's
    powerful vision capabilities for accurate table conversion.
examples/tracing_with_langfuse.md:
  cross_links: []
  hash: 2b1caa40e9da271b66e341c45b463b28
  references: []
  summary: This guide introduces Langfuse, an open-source observability and tracing
    platform for AI applications, showcasing how to integrate it with Instructor and
    OpenAI clients for enhanced monitoring and debugging of large language model (LLM)
    calls. It provides setup instructions, including installation and environment
    configuration for both synchronous and asynchronous OpenAI clients. The content
    highlights key use cases such as tracing API calls, classifying customer feedback,
    scoring relevance, and visualizing detailed traces via the Langfuse dashboard.
    Core keywords include Langfuse, observability, AI monitoring, tracing, LLM, API
    performance, debugging, Instructor, OpenAI, and asynchronous AI integration.
examples/watsonx.md:
  cross_links: []
  hash: dafd5f18905aa8c25b71a9f2f9bc8a65
  references: []
  summary: This guide details how to use IBM watsonx.ai for inference with LiteLLM
    to generate structured outputs. It covers prerequisites such as IBM Cloud account,
    API key, and project ID, and provides installation instructions using Poetry.
    The example demonstrates creating a custom data model and performing JSON-mode
    inference with watsonx.ai, showcasing how to set environment variables, initialize
    the client, and generate structured data like company information from text input.
    Key concepts include IBM watsonx.ai, LiteLLM, inference, structured outputs, setup,
    API integration, and Python coding examples.
examples/youtube_clips.md:
  cross_links: []
  hash: 972f468e337dd6fc72cfc12cbd129226
  references: []
  summary: This guide explains how to generate concise, engaging YouTube clips from
    video transcripts using the `instructor` library and OpenAI models. It demonstrates
    extracting transcript segments with timing information from YouTube videos using
    `youtube_transcript_api`, and then leveraging GPT-4 to identify key moments and
    create specific clip titles and descriptions. The process involves fetching transcripts,
    prompting GPT-4 to produce notable clips, and displaying the results in a structured
    format. Key concepts include transcript extraction, AI-powered clip generation,
    content summarization, and leveraging OpenAI for enhanced video editing and content
    segmentation. This approach helps content creators enhance engagement by recutting
    videos into focused, shareable clips.
faq.md:
  cross_links: []
  hash: bca382d72ff309ba7f12a9213923c7e5
  references:
  - ./integrations/index.md
  - ./concepts/patching.md
  summary: Instructor is a versatile Python library designed to simplify extracting
    structured data from Large Language Models (LLMs) by leveraging Pydantic schemas
    for validation and consistency across various providers like OpenAI, Anthropic,
    Google Gemini, Cohere, and open-source models. It offers multiple modes, such
    as JSON, Tools, and Function Calling, to suit different provider capabilities,
    along with features like response validation, automatic retries, raw response
    access, and streaming support. Ideal for integrating LLMs into applications, Instructor
    also supports fastapi compatibility, async operations, and cost optimization through
    prompt design and caching. Core keywords include LLM, Pydantic, structured data,
    AI integration, OpenAI, Anthropic, Google Gemini, function calling, retries, streaming,
    API, and chat models.
getting-started.md:
  ai_references:
  - '[concepts/patching.md'
  - concepts/reask_validation.md
  - examples/index.md
  - concepts/hooks.md
  - concepts/index.md]
  cross_links:
  - concepts/hooks.md
  - concepts/index.md
  - concepts/patching.md
  - concepts/reask_validation.md
  - examples/index.md
  - index.md
  hash: e7e29e4fba34d06eccbad39d295041eb
  keywords:
  - Instructor
  - structured data
  - language models
  - installation
  - validation
  - API keys
  - LLM providers
  references:
  - ./concepts/patching.md
  - ./concepts/reask_validation.md
  - ./examples/index.md
  - ./concepts/hooks.md
  - ./concepts/index.md
  summary: This guide provides a comprehensive introduction to using Instructor for
    extracting structured data from language models. It covers installation, environment
    setup, and key functionalities including structured output extraction, validation,
    and usage with various LLM providers. By following the steps outlined, users can
    effectively leverage Instructor to enhance data output from language models.
  topics:
  - Installation
  - Environment Setup
  - Structured Output Extraction
  - Validation and Error Handling
  - Streaming Responses
help.md:
  cross_links:
  - blog/index.md
  - concepts/prompting.md
  - examples/index.md
  hash: 8aa79aef3783bdc81724f7d3d6d1b7d1
  references:
  - concepts/prompting.md
  - examples/index.md
  - blog/index.md
  summary: This guide provides essential resources for getting help with Instructor,
    an AI model prompting tool. Key support options include the Discord community,
    detailed concepts on prompting, practical cookbooks with usage examples, and informative
    blog articles. Additionally, users can leverage GitHub Discussions for questions
    and collaboration, report bugs and request features via GitHub Issues, or contact
    the creator on Twitter. These resources ensure users can effectively learn, troubleshoot,
    and optimize their experience with Instructor.
index.md:
  ai_references:
  - '[./concepts/reask_validation.md'
  - ./concepts/retrying.md
  - ./concepts/lists.md
  - ./concepts/partial.md
  - ./integrations/openai.md
  - ./integrations/ollama.md
  - ./integrations/anthropic.md
  - ./integrations/google.md
  - ./integrations/vertex.md
  - ./integrations/cohere.md
  - ./integrations/litellm.md
  - ./integrations/llama-cpp-python.md
  - ./integrations/cerebras.md
  - ./integrations/fireworks.md
  - ./concepts/models.md
  - ./concepts/hooks.md
  - ./concepts/templating.md]
  cross_links:
  - concepts/hooks.md
  - concepts/lists.md
  - concepts/models.md
  - concepts/partial.md
  - concepts/reask_validation.md
  - concepts/retrying.md
  - concepts/templating.md
  - integrations/anthropic.md
  - integrations/cerebras.md
  - integrations/cohere.md
  - integrations/fireworks.md
  - integrations/google.md
  - integrations/litellm.md
  - integrations/llama-cpp-python.md
  - integrations/ollama.md
  - integrations/openai.md
  - integrations/vertex.md
  hash: 77cda4e6af3f3243dd7d6f77c532ad75
  keywords:
  - '[LLM structured outputs'
  - Python library
  - data extraction
  - Pydantic validation
  - OpenAI
  - Anthropic
  - Google
  - streaming support
  - multi-provider API
  - open source models]
  references:
  - ./concepts/reask_validation.md
  - ./concepts/retrying.md
  - ./concepts/lists.md
  - ./concepts/partial.md
  - ./examples/index.md
  - ./prompting/index.md
  - ./integrations/openai.md
  - ./integrations/ollama.md
  - ./integrations/llama-cpp-python.md
  - ./integrations/anthropic.md
  - ./integrations/google.md
  - ./integrations/vertex.md
  - ./integrations/groq.md
  - ./integrations/litellm.md
  - ./integrations/cohere.md
  - ./integrations/cerebras.md
  - ./integrations/fireworks.md
  - ./concepts/models.md
  - ./concepts/reask_validation.md
  - ./concepts/partial.md
  - ./integrations/openai.md
  - ./integrations/anthropic.md
  - ./integrations/google.md
  - ./integrations/vertex.md
  - ./integrations/together.md
  - ./integrations/ollama.md
  - ./integrations/llama-cpp-python.md
  - ./integrations/cohere.md
  - ./integrations/litellm.md
  - ./integrations/index.md
  - ./concepts/hooks.md
  - ./concepts/templating.md
  - ./concepts/retrying.md
  - ./concepts/reask_validation.md
  - ./concepts/reask_validation.md
  - ./concepts/partial.md
  - ./integrations/index.md
  - ./concepts/retrying.md
  - ./concepts/models.md
  summary: Instructor is the leading Python library designed for extracting structured
    outputs from various Large Language Models (LLMs) like OpenAI, Anthropic, and
    Google. Utilizing Pydantic for type safety and validation, it ensures reliable
    data extraction while supporting over 15 providers with features like automatic
    retries and streaming responses.
  topics:
  - '[Python library for LLMs'
  - Structured data extraction
  - Pydantic type validation
  - Multi-provider support
  - Error handling and retries]
installation.md:
  cross_links: []
  hash: a6fe720590b602e1f753c067be9c3121
  references: []
  summary: Learn how to install Instructor, an advanced Python tool for building CLIs,
    using pip. Instructor requires dependencies such as openai, typer, docstring-parser,
    and pydantic, making setup straightforward for Python 3.9 and above. This guide
    provides a simple, quick installation process to enhance your Python projects
    with powerful, type-hint-based CLI development.
integrations/anthropic.md:
  ai_references:
  - '[../concepts/multimodal.md'
  - ../concepts/caching.md
  - https://docs.anthropic.com/en/docs/build-with-claude/tool-use]
  cross_links:
  - concepts/caching.md
  - concepts/multimodal.md
  hash: fe54a665c05aa5770971338d42cef867
  keywords:
  - Anthropic
  - Claude models
  - structured data extraction
  - Python
  - Instructor
  - multimodal inputs
  - streaming support
  - caching
  references:
  - concepts/multimodal.md
  - concepts/caching.md
  summary: This tutorial provides a comprehensive guide on using Anthropic's Claude
    models with the Instructor for structured data extraction in Python. It covers
    installation, basic usage, multimodal inputs, and advanced features such as streaming
    support, caching, and using various response models effectively.
  topics: []
integrations/anyscale.md:
  cross_links: []
  hash: 53e83cd7c07b43d303cb4a8696300408
  references: []
  summary: This guide provides instructions on using Anyscale, a platform offering
    access to open-source LLMs like Mistral and Llama models, with the Instructor
    library to produce structured outputs. It covers installation, API key setup,
    and offers a practical example of extracting structured data using Anyscale's
    API and the Instructor client in JSON schema mode. Supported modes include JSON,
    JSON_SCHEMA, TOOLS, and MD_JSON, and the platform features a variety of models
    such as Mistral and Llama, making it a comprehensive resource for leveraging open-source
    LLMs for structured data extraction and AI development.
integrations/azure.md:
  cross_links: []
  hash: 3a23c67e1ceafad28834395d384f37ff
  references: []
  summary: This comprehensive guide explains how to use Azure OpenAI with Instructor
    for structured outputs, including synchronous and asynchronous implementations,
    streaming, nested models, and response validation. It covers installation, authentication,
    deploying models, and working with various response modes such as JSON, tools,
    and function calling. Key features include streaming partial and iterable responses,
    handling complex nested data, and leveraging different Instructor modes to optimize
    structured output generation. This resource is ideal for developers seeking secure,
    enterprise-grade AI solutions with Azure OpenAI and Instructor for reliable, scalable
    structured data extraction.
integrations/bedrock.md:
  cross_links: []
  hash: 52ede618fbd9c3a9edc9355537e1eb51
  references: []
  summary: This guide explains how to use AWS Bedrock with Instructor and Pydantic
    for generating structured, validated JSON outputs from Amazon's foundational AI
    models. It covers setting up the AWS Bedrock client, implementing type-safe responses
    with Pydantic models, and utilizing different modes like BEDROCK_TOOLS and BEDROCK_JSON
    for flexible output formats. The tutorial also demonstrates handling nested objects
    and complex data structures, enabling developers to create robust, structured
    AI interactions in Python. Core keywords include AWS Bedrock, Instructor, Pydantic,
    JSON outputs, structured responses, AI models, and type safety.
integrations/cerebras.md:
  cross_links: []
  hash: 30881d913bf857193a0b5af812d259c2
  references: []
  summary: This comprehensive guide details how to use Instructor with Cerebras's
    hardware-accelerated AI models for generating structured, type-safe outputs. It
    covers installation, both synchronous and asynchronous usage examples, and advanced
    features like nested outputs and streaming support, including partial and iterable
    streaming modes. The guide highlights customization through Instructor hooks and
    explains different response modes such as CEREBRAS_JSON and CEREBRAS_TOOLS, emphasizing
    the flexibility and future-proofing of these modes for high-performance, validated
    AI responses. Key terms include Cerebras, Instructor, structured outputs, JSON
    parsing, streaming, validation hooks, and AI model integration.
integrations/cohere.md:
  cross_links: []
  hash: bcabf6169d2e18732d09f41a2b03ee9a
  references: []
  summary: This guide provides a comprehensive tutorial on generating structured,
    type-safe outputs with Cohere's command models using the Instructor library in
    Python. It covers setup instructions, including installing the library and obtaining
    an API key. The tutorial demonstrates how to define data models with Pydantic,
    patch the Cohere client with Instructor for enhanced capabilities, and generate
    structured responses such as creating a detailed Group object based on provided
    text. Key features include leveraging Cohere's command models like "command-r-plus"
    to produce accurate, JSON-formatted data, making it ideal for tasks requiring
    structured outputs, data extraction, and automation. This resource is valuable
    for developers seeking to enhance NLP workflows with reliable, structured data
    generation.
integrations/cortex.md:
  cross_links:
  - concepts/index.md
  - concepts/validation.md
  - examples/index.md
  hash: 5dc3985ba626ba07487689f654305962
  references:
  - concepts/index.md
  - concepts/validation.md
  - examples/index.md
  summary: This guide provides a comprehensive overview of using Cortex with Instructor
    to achieve structured outputs from local open-source large language models (LLMs).
    It covers quick setup, both synchronous and asynchronous API usage, and demonstrates
    advanced nested extraction examples with Pydantic models. Key topics include model
    deployment with Cortex, integration with OpenAI clients, and effective prompt
    handling for structured data extraction. Essential keywords include Cortex, Instructor,
    LLM, structured outputs, local models, open-source, API integration, Pydantic,
    and AI prompt engineering.
integrations/databricks.md:
  cross_links: []
  hash: 10a70b86eb06ad1262a58d8050984151
  references: []
  summary: This guide provides a comprehensive overview of using Databricks with the
    Instructor library to obtain structured outputs from AI models. It covers installation,
    setting up environment variables with Databricks API keys and workspace URL, and
    demonstrates a basic example of extracting structured data such as user information
    using Databricks models. The guide highlights supported modes like TOOLS, JSON,
    FUNCTIONS, and more, and explains that Databricks offers access to various models,
    including foundation, fine-tuned, and open-source models deployed on the platform.
    Keywords include Databricks, Instructor, structured outputs, AI models, API integration,
    and machine learning.
integrations/deepseek.md:
  cross_links:
  - concepts/index.md
  - concepts/validation.md
  - examples/index.md
  hash: 8e0bf42ff9f31e84527488ce3b43e8d9
  references:
  - concepts/index.md
  - concepts/validation.md
  - examples/index.md
  summary: This guide provides a comprehensive overview of using DeepSeek models with
    Instructor for type-safe, structured outputs. DeepSeek, a Chinese AI company,
    offers various models including the deepseek coder, chat model, and R1 reasoning
    model. The tutorial demonstrates how to set up and utilize models for both synchronous
    and asynchronous scenarios using the OpenAI API. Key features include creating
    structured outputs with Pydantic, streaming with iterables and partials, and integrating
    reasoning models for detailed completion traces. Essential steps for setting up
    include initializing the `instructor` package, configuring the API key, and using
    the appropriate Instructor modes. Core keywords include DeepSeek, AI models, structured
    outputs, type-safe, OpenAI API, Instructor, Pydantic, synchronous, asynchronous,
    and reasoning models.
integrations/fireworks.md:
  cross_links:
  - concepts/index.md
  - concepts/validation.md
  - examples/index.md
  hash: 542aa4056ddd0ae3132abdbd10cbffa2
  references:
  - concepts/index.md
  - concepts/validation.md
  - examples/index.md
  summary: This comprehensive guide provides instructions on utilizing Instructor
    with Fireworks AI models to create structured, type-safe outputs. It covers installation,
    basic synchronous and asynchronous user examples, and complex nested examples,
    emphasizing high-performance and cost-effective AI capabilities. The guide also
    demonstrates streaming support, including iterables and partial streaming, using
    Pydantic models for type validation. Key points include integration with `Fireworks`,
    usage of `instructor` modes for structured outputs, and maintaining compatibility
    with the latest Fireworks API versions. Essential keywords include Fireworks AI,
    Instructor, structured outputs, type-safe, streaming support, and Pydantic.
integrations/genai.md:
  ai_references:
  - '[official Google AI documentation for the GenAI SDK](https://googleapis.github.io/python-genai/)'
  - '[official documentation](https://ai.google.dev/gemini-api/docs/thinking)'
  - '[documentation for models](https://ai.google.dev/gemini-api/docs/models)'
  cross_links: []
  hash: 7ca74881599d14d3795d4e09e0723e84
  keywords:
  - Google GenAI
  - structured outputs
  - Gemini models
  - Python SDK
  - multimodal processing
  - data extraction
  - Instructor
  - Pydantic models
  references: []
  summary: This guide provides step-by-step instructions on using Google's Generative
    AI SDK (genai) with Instructor to extract structured data from Gemini models.
    It covers essential modes, installation instructions, message formatting, and
    multimodal capabilities, enabling users to efficiently handle various input types
    such as audio, images, and PDFs.
  topics: []
integrations/google.md:
  ai_references:
  - '[Google''s documentation on Gemini configuration parameters](https://cloud.google.com/vertex-ai/generative-ai/docs/samples/generativeaionvertexai-gemini-pro-config-example)'
  - '[Using Geminin To Extract Travel Video Recommendations](../blog/posts/multimodal-gemini.md)'
  - '[Parsing PDFs with Gemini](../blog/posts/chat-with-your-pdf-with-gemini.md)'
  - '[Generating Citations with Gemini](../blog/posts/generating-pdf-citations.md)'
  - '[Google AI Documentation](https://ai.google.dev/)'
  - '[Instructor Core Concepts](../concepts/index.md)'
  - '[Type Validation Guide](../concepts/validation.md)'
  - '[Advanced Usage Examples](../examples/index.md)'
  - '[changelog](https://github.com/jxnl/instructor/blob/main/CHANGELOG.md)'
  cross_links:
  - blog/posts/chat-with-your-pdf-with-gemini.md
  - blog/posts/generating-pdf-citations.md
  - blog/posts/multimodal-gemini.md
  - concepts/index.md
  - concepts/validation.md
  - examples/index.md
  - index.md
  hash: 469bedc93eaca35e535263d257d81094
  keywords:
  - Google Gemini
  - structured data extraction
  - Instructor library
  - multimodal AI
  - type-safe outputs
  - configuration options
  - async support
  - response models
  references:
  - blog/posts/multimodal-gemini.md
  - blog/posts/chat-with-your-pdf-with-gemini.md
  - blog/posts/generating-pdf-citations.md
  - concepts/index.md
  - concepts/validation.md
  - examples/index.md
  summary: "This tutorial provides a comprehensive guide on using Google's Gemini\
    \ models\u2014Pro, Flash, and Ultra\u2014with the Instructor library for structured\
    \ data extraction. Learn to process multimodal inputs, customize model behavior,\
    \ and utilize type-safe outputs effectively through detailed examples and configurations."
  topics: []
integrations/groq.md:
  cross_links: []
  hash: 1b2b59a31e2e4ce05dff63e482192a95
  references: []
  summary: The article provides a detailed guide on using Groq AI with Pydantic to
    generate structured outputs in Python. It highlights using the `llama-3-groq-70b-8192-tool-use-preview`
    model to create type-safe, structured responses via synchronous and asynchronous
    examples. The guide emphasizes setting up with an API key, employing Groq's LLM
    models, and integrating Pydantic for defining response structures. It also demonstrates
    creating nested object responses for complex data extraction. Key terms include
    Groq AI, Pydantic, structured outputs, type-safe responses, and Python API integration.
integrations/index.md:
  ai_references:
  - '[openai.md'
  - openai-responses.md
  - azure.md
  - anthropic.md
  - google.md
  - vertex.md
  - bedrock.md
  - genai.md
  - cohere.md
  - mistral.md
  - deepseek.md
  - together.md
  - groq.md
  - fireworks.md
  - cerebras.md
  - writer.md
  - perplexity.md
  - sambanova.md
  - ollama.md
  - llama-cpp-python.md
  - patching.md
  - models.md
  - validation.md
  - partial.md
  - iterable.md
  - hooks.md
  - modes-comparison.md
  - examples/index.md]
  cross_links:
  - blog/posts/anthropic.md
  - blog/posts/structured-output-anthropic.md
  - concepts/hooks.md
  - concepts/iterable.md
  - concepts/models.md
  - concepts/partial.md
  - concepts/patching.md
  - concepts/reask_validation.md
  - concepts/semantic_validation.md
  - concepts/validation.md
  - examples/groq.md
  - examples/index.md
  - examples/mistral.md
  - examples/ollama.md
  - index.md
  - integrations/anthropic.md
  - integrations/azure.md
  - integrations/bedrock.md
  - integrations/cerebras.md
  - integrations/cohere.md
  - integrations/deepseek.md
  - integrations/fireworks.md
  - integrations/genai.md
  - integrations/google.md
  - integrations/groq.md
  - integrations/litellm.md
  - integrations/llama-cpp-python.md
  - integrations/mistral.md
  - integrations/ollama.md
  - integrations/openai-responses.md
  - integrations/openai.md
  - integrations/openrouter.md
  - integrations/perplexity.md
  - integrations/sambanova.md
  - integrations/together.md
  - integrations/vertex.md
  - integrations/writer.md
  - learning/getting_started/response_models.md
  - learning/patterns/field_validation.md
  - learning/validation/field_level_validation.md
  - modes-comparison.md
  hash: 0cd377c30ed32c1e1436c3194f87f72c
  keywords:
  - '[LLM integration'
  - AI model providers
  - structured output
  - OpenAI
  - Anthropic
  - Google Gemini
  - local models
  - Pydantic
  - cloud services]
  references:
  - integrations/openai.md
  - integrations/openai-responses.md
  - integrations/azure.md
  - integrations/anthropic.md
  - integrations/google.md
  - integrations/vertex.md
  - integrations/bedrock.md
  - integrations/genai.md
  - integrations/cohere.md
  - integrations/mistral.md
  - integrations/deepseek.md
  - integrations/together.md
  - integrations/groq.md
  - integrations/fireworks.md
  - integrations/cerebras.md
  - integrations/writer.md
  - integrations/perplexity.md
  - integrations/sambanova.md
  - integrations/ollama.md
  - integrations/llama-cpp-python.md
  - integrations/litellm.md
  - integrations/openrouter.md
  - concepts/patching.md
  - concepts/models.md
  - concepts/validation.md
  - concepts/partial.md
  - concepts/iterable.md
  - concepts/hooks.md
  - modes-comparison.md
  - examples/index.md
  - examples/index.md
  summary: This documentation provides comprehensive tutorials for integrating the
    Instructor framework with over 15 LLM providers, including major names like OpenAI,
    Anthropic, and Google. Users can learn to utilize structured data extraction and
    various integration modes through clear examples and feature descriptions.
  topics:
  - '[Integration with AI providers'
  - Core features
  - Provider modes
  - Getting started
  - Troubleshooting]
integrations/litellm.md:
  cross_links:
  - concepts/index.md
  - concepts/validation.md
  - examples/index.md
  hash: d6fc058af4b92fbded142d630ec90055
  references:
  - concepts/index.md
  - concepts/validation.md
  - examples/index.md
  summary: This comprehensive guide explains how to use Instructor with LiteLLM's
    unified interface to generate structured, type-safe outputs across multiple LLM
    providers like GPT-3.5 and Claude-3. It covers both synchronous and asynchronous
    implementations, demonstrating how to create validated responses using Pydantic
    models. Additionally, the guide details cost calculation via response cost attributes
    and emphasizes LiteLLM's compatibility and easy model switching. Key topics include
    structured output generation, response validation, cost tracking, and integration
    with various LLM providers.
integrations/llama-cpp-python.md:
  cross_links:
  - examples/index.md
  - index.md
  - why.md
  hash: d4baa4f29b79ed75acefbd1acaec8481
  references:
  - index.md
  - why.md
  - examples/index.md
  summary: This comprehensive guide explores how to generate structured, type-safe
    outputs using llama-cpp-python with Instructor, focusing on JSON schema mode and
    speculative decoding. By leveraging open-source LLMs, users can achieve structured
    outputs with constrained sampling techniques and avoid network dependencies using
    an OpenAI-compatible client. The guide highlights features such as the `response_model`
    and `max_retries` for enhanced functionality in `create` calls, showcasing the
    use of Pydantic for efficient data validation. An advanced example using JSON
    schema to extract data within a streaming context is also presented. Key terms
    include llama-cpp-python, JSON schema mode, speculative decoding, Pydantic, and
    structured outputs.
integrations/mistral.md:
  cross_links:
  - concepts/index.md
  - concepts/validation.md
  - examples/index.md
  hash: f821daf9ad84fd47d59dd265143b200b
  references:
  - concepts/index.md
  - concepts/validation.md
  - examples/index.md
  summary: This comprehensive guide explains how to use Mistral AI's Large model with
    Instructor to generate structured, type-safe outputs and JSON schema-based function
    calling. It covers setup instructions, including API key configuration, and showcases
    how to utilize Mistral's capabilities in both synchronous and asynchronous modes,
    with support for nested models, streaming, and multimodal PDF analysis. Key features
    include modes for structured outputs, partial response streaming, iterable responses,
    and advanced multimodal extraction, making it an essential resource for leveraging
    Mistral's powerful AI models with Instructor for reliable data extraction and
    structured AI responses.
integrations/ollama.md:
  ai_references:
  - '[../index.md'
  - ../why.md]
  cross_links:
  - index.md
  - why.md
  hash: 0e9679037802bdef503c474201b3e5dd
  keywords:
  - '[Ollama'
  - Instructor
  - JSON schema
  - structured outputs
  - timeout handling
  - open source
  - local LLMs
  - Pydantic]
  references:
  - index.md
  - why.md
  summary: This comprehensive guide teaches you how to leverage Ollama with Instructor
    to generate structured outputs using JSON schema, enhancing response safety and
    reliability. You will explore key features like timeout handling and automated
    client modes for optimal performance when working with local LLMs.
  topics:
  - '[Using Ollama with Instructor'
  - Patching
  - Timeout Handling
  - Quick Start with Auto Client
  - Manual Setup]
integrations/openai-responses.md:
  ai_references:
  - '[OpenAI Documentation](https://platform.openai.com/docs)'
  - '[Instructor Core Concepts](../concepts/index.md)'
  - '[Type Validation Guide](../concepts/validation.md)'
  - '[Advanced Usage Examples](../examples/index.md)'
  cross_links:
  - concepts/index.md
  - concepts/validation.md
  - examples/index.md
  - index.md
  hash: d79a5a73ed7e5610674465ceb9217177
  keywords:
  - OpenAI
  - Responses API
  - structured outputs
  - Python
  - examples
  - web search
  - file search
  - type-safe
  - validated outputs
  references:
  - concepts/index.md
  - concepts/validation.md
  - examples/index.md
  summary: The OpenAI Responses API Guide provides comprehensive instructions on leveraging
    the new API for structured outputs with OpenAI models, focusing on best practices
    and examples. This guide highlights various response modes, core methods, and
    built-in tools to enhance functionality, making it ideal for developers looking
    to implement type-safe, validated outputs in their applications.
  topics: []
integrations/openai.md:
  cross_links:
  - concepts/index.md
  - concepts/multimodal.md
  - concepts/validation.md
  - examples/batch_job_oai.md
  - examples/index.md
  hash: e590f98025395a6720663e19033615a5
  references:
  - concepts/multimodal.md
  - examples/batch_job_oai.md
  - concepts/index.md
  - concepts/validation.md
  - examples/index.md
  summary: This comprehensive guide explores using OpenAI models with Instructor for
    structured, type-safe outputs, including GPT-4, GPT-3.5, and multimodal capabilities
    with images, audio, and PDFs. It covers setup, both synchronous and asynchronous
    examples, nested data extraction, multimodal analysis, streaming, batching, and
    various response modes like tools and JSON modes. The tutorial emphasizes best
    practices for model selection, performance optimization, and common use cases
    such as data extraction, document analysis, form parsing, and API response structuring.
    Keywords include OpenAI, Instructor, structured outputs, GPT-4, multimodal, streaming,
    batch API, data extraction, type-safe responses, and API integrations.
integrations/openrouter.md:
  cross_links: []
  hash: bd8e8fdd749c0da0250180d12cc97e4e
  references: []
  summary: 'This comprehensive guide explains how to use Instructor with OpenRouter
    to achieve structured, type-safe outputs across multiple large language model
    (LLM) providers. It details how to integrate Instructor with the OpenAI client,
    supporting synchronous and asynchronous usage, nested object extraction, and various
    modes including Structured Outputs and JSON. The guide emphasizes the importance
    of model compatibility with tool calling and structured outputs, provides code
    examples for different scenarios, and highlights how to enable streaming responses.
    Key topics include multi-provider API switching, schema validation with Pydantic
    models, handling models without tool calling support, and leveraging OpenRouter''s
    unified API for enhanced LLM integrations. Core keywords: OpenRouter, Instructor,
    LLM, structured outputs, tool calling, API integration, type-safe responses, multi-provider,
    GPT models, JSON mode, streaming.'
integrations/perplexity.md:
  ai_references:
  - '[Perplexity API Documentation](https://docs.perplexity.ai/)'
  - '[Perplexity API Reference](https://docs.perplexity.ai/reference/post_chat_completions)'
  cross_links: []
  hash: 75d7e6c97db652b39aa3eeafad8db003
  keywords:
  - Perplexity AI
  - Instructor
  - structured outputs
  - Pydantic
  - JSON
  - API key
  - type-safe
  - validated responses
  - nested objects
  references: []
  summary: This guide explains how to utilize Perplexity AI with the Instructor library
    to create structured JSON outputs using Pydantic models in Python. It covers both
    synchronous and asynchronous examples, as well as details on creating nested objects
    for type-safe and validated responses from Perplexity's Sonar models.
  topics: []
integrations/sambanova.md:
  cross_links: []
  hash: 81003730e09b4b43bccdd04a11b7f3ae
  references: []
  summary: SambaNova integration with Instructor allows users to leverage SambaNova's
    LLM API for structured output generation in Python. The setup involves installing
    the `instructor[openai]` package and configuring the client with the SambaNova
    API endpoint and API key. It supports both synchronous and asynchronous usage,
    enabling detailed prompt and response modeling with Pydantic. Key models include
    Meta-Llama-3.1-405B-Instruct, and users can explore additional options via SambaNova's
    documentation. This integration facilitates advanced AI workflows with SambaNova's
    large language models for enhanced NLP applications.
integrations/together.md:
  cross_links:
  - index.md
  - why.md
  hash: 39d3ac703bab17e5ad0cb06d6c0cafd6
  references:
  - index.md
  - why.md
  summary: 'This comprehensive guide explains how to use Together AI with Instructor
    to generate structured, type-safe outputs through function calling. It highlights
    open-source LLM support, patching features like response models and retries, and
    demonstrates how to integrate Instructor with Together''s models using Python.
    Key topics include leveraging Pydantic for data validation, utilizing Together
    AI''s API, and creating custom models for accurate output extraction. Keywords:
    Together AI, Instructor, structured outputs, function calling, open-source LLMs,
    Python, Pydantic, type-safe responses, API integration.'
integrations/vertex.md:
  ai_references:
  - '[../concepts/index.md'
  - ../concepts/validation.md
  - ../examples/index.md
  - https://cloud.google.com/vertex-ai/docs
  - https://github.com/jxnl/instructor/blob/main/CHANGELOG.md]
  cross_links:
  - concepts/index.md
  - concepts/validation.md
  - examples/index.md
  - index.md
  hash: 555e953a46c2db86bfd7ae9ff1a071f3
  keywords:
  - '[Vertex AI'
  - Instructor
  - structured outputs
  - type-safe responses
  - asynchronous streaming
  - Python examples
  - Google Cloud
  - generative models]
  references:
  - concepts/index.md
  - concepts/validation.md
  - examples/index.md
  summary: This comprehensive guide demonstrates how to utilize Instructor with Google
    Cloud's Vertex AI to generate structured, type-safe outputs. It explores synchronous
    and asynchronous usage, provides concrete examples, and highlights the newly added
    streaming capabilities for efficient data handling.
  topics:
  - '[Getting Started'
  - Synchronous User Example
  - Asynchronous User Example
  - Streaming Support
  - Updates and Compatibility]
integrations/writer.md:
  cross_links: []
  hash: 27299f8967d9a30443039b93e1d233dd
  references: []
  summary: 'This guide provides a comprehensive overview of using Writer for structured
    outputs with the latest Palmyra-X-004 model, which enhances reliability using
    tool-calling functionality. It includes setup instructions, such as obtaining
    an API key and integrating with Python using Writer''s `instructor` module. The
    guide offers synchronous and asynchronous examples for extracting structured data,
    including support for nested objects and streaming responses with iterables and
    partial streaming. Key topics include structured data extraction, API integration,
    Python scripting, and advanced data handling with Writer''s Palmyra-X-004 model.
    Keywords: Writer, Palmyra-X-004, structured outputs, API key, data extraction,
    nested objects, streaming support, Python integration.'
jobs.md:
  cross_links: []
  hash: d41d8cd98f00b204e9800998ecf8427e
  references: []
  summary: Of course! Please provide the text that you would like me to summarize,
    and I'll be happy to assist you.
learning/getting_started/client_setup.md:
  ai_references:
  - '[../patterns/simple_object.md'
  - ../patterns/list_extraction.md
  - ../patterns/nested_structure.md
  - ../validation/basics.md]
  cross_links:
  - learning/patterns/list_extraction.md
  - learning/patterns/nested_structure.md
  - learning/patterns/optional_fields.md
  - learning/patterns/simple_object.md
  - learning/validation/basics.md
  hash: 7d7ea676cc2058a2fa58216ab56d366c
  keywords:
  - '[client setup'
  - Instructor
  - OpenAI
  - Anthropic
  - Google Gemini
  - Cohere
  - Mistral
  - async clients
  - modes]
  references:
  - learning/patterns/simple_object.md
  - learning/patterns/list_extraction.md
  - learning/patterns/nested_structure.md
  - learning/validation/basics.md
  - learning/patterns/optional_fields.md
  summary: This guide provides step-by-step instructions on setting up various client
    configurations for utilizing the Instructor with multiple LLM providers, including
    OpenAI, Anthropic, Google, Cohere, and Mistral. It covers default and JSON modes,
    async client usage, and advanced configurations for better integration with these
    providers.
  topics:
  - '[Client configuration'
  - Modes of operation
  - Asynchronous clients
  - Advanced configurations
  - Compatibility with other providers]
learning/getting_started/first_extraction.md:
  ai_references:
  - '[response_models.md'
  - client_setup.md
  - ../patterns/simple_object.md]
  cross_links:
  - learning/getting_started/client_setup.md
  - learning/getting_started/response_models.md
  - learning/patterns/simple_object.md
  hash: b253293c79f241efc1338bd19fddfee4
  keywords:
  - LLM extraction
  - structured data
  - Pydantic
  - Instructor
  - OpenAI
  - Python objects
  - data validation
  - field descriptions
  - optional data
  references:
  - learning/getting_started/response_models.md
  - learning/getting_started/client_setup.md
  - learning/patterns/simple_object.md
  - learning/getting_started/response_models.md
  summary: This tutorial guides users through extracting structured data using LLMs
    with Instructor, focusing on converting unstructured text into validated Python
    objects. It includes step-by-step instructions for configuring the model and emphasizes
    the importance of using Pydantic for type-safe extraction.
  topics:
  - LLM extraction process
  - Pydantic models
  - configuring an LLM client
  - handling optional data
  - common extraction patterns
learning/getting_started/installation.md:
  ai_references:
  - '[first_extraction.md'
  - response_models.md
  - client_setup.md]
  cross_links:
  - learning/getting_started/client_setup.md
  - learning/getting_started/first_extraction.md
  - learning/getting_started/response_models.md
  hash: ffd0b4e3d308c123750dc4648591c9fc
  keywords:
  - Instructor
  - LLM
  - structured outputs
  - Python
  - installation
  - OpenAI
  - Claude
  - Gemini
  - Pydantic
  references:
  - learning/getting_started/first_extraction.md
  - learning/getting_started/response_models.md
  - learning/getting_started/client_setup.md
  - learning/getting_started/first_extraction.md
  summary: This guide provides step-by-step instructions on installing the Instructor
    library for extracting structured data from various large language models (LLMs)
    including OpenAI's GPT-4, Anthropic's Claude, and Google's Gemini. It covers installation
    steps, configuration for different LLM providers, and verification of the setup
    for beginners looking to enhance their LLM application development.
  topics:
  - Installation guide
  - LLM provider setup
  - API configuration
  - verification tests
  - common issues
learning/getting_started/response_models.md:
  ai_references:
  - '[../patterns/field_validation.md'
  - ../validation/basics.md
  - ../patterns/nested_structure.md
  - ../patterns/optional_fields.md
  - ../patterns/list_extraction.md
  - ../validation/custom_validators.md
  - client_setup.md]
  cross_links:
  - learning/getting_started/client_setup.md
  - learning/patterns/field_validation.md
  - learning/patterns/list_extraction.md
  - learning/patterns/nested_structure.md
  - learning/patterns/optional_fields.md
  - learning/validation/basics.md
  - learning/validation/custom_validators.md
  hash: fe9bd1a857fd36a269a55a0b05c8f7e5
  keywords:
  - '[response models'
  - Pydantic
  - field validation
  - nested models
  - enums
  - optional fields
  - model documentation
  - data extraction]
  references:
  - learning/patterns/field_validation.md
  - learning/validation/basics.md
  - learning/patterns/nested_structure.md
  - learning/patterns/optional_fields.md
  - learning/patterns/list_extraction.md
  - learning/validation/custom_validators.md
  - learning/getting_started/client_setup.md
  summary: This guide provides an in-depth look at response models in Instructor,
    outlining how to create, validate, and document different types of models using
    Pydantic. It covers basic and advanced topics including field metadata, validation
    rules, nested models, enums, optional fields, and more to effectively extract
    data for various use cases.
  topics:
  - '[Basic Models'
  - Field Metadata
  - Field Validation
  - Nested Models
  - Using Enums]
learning/getting_started/structured_outputs.md:
  ai_references:
  - '[first_extraction.md'
  - response_models.md
  - client_setup.md]
  cross_links:
  - learning/getting_started/client_setup.md
  - learning/getting_started/first_extraction.md
  - learning/getting_started/response_models.md
  hash: e909556e4995fb3ac4ae5cc34a0c901e
  keywords:
  - structured outputs
  - large language models
  - data extraction
  - Pydantic
  - consistency
  - validation
  - type safety
  - Instructor
  references:
  - learning/getting_started/first_extraction.md
  - learning/getting_started/response_models.md
  - learning/getting_started/client_setup.md
  summary: This guide introduces the concept of structured outputs for large language
    models, emphasizing the benefits of using Pydantic models to enforce data consistency,
    validation, and type safety. It provides examples of extracting structured data
    from LLMs and discusses the installation and setup of the Instructor package for
    improved data handling.
  topics:
  - structured data extraction
  - Pydantic models
  - handling unstructured outputs
  - installation and setup
  - complex data structures
learning/index.md:
  ai_references:
  - '[getting_started/installation.md'
  - getting_started/first_extraction.md
  - getting_started/response_models.md
  - getting_started/client_setup.md
  - patterns/simple_object.md
  - patterns/list_extraction.md
  - patterns/nested_structure.md
  - patterns/optional_fields.md
  - patterns/field_validation.md
  - patterns/prompt_templates.md
  - validation/basics.md
  - validation/field_level_validation.md
  - validation/custom_validators.md
  - validation/retry_mechanisms.md
  - streaming/basics.md
  - streaming/lists.md]
  cross_links:
  - installation.md
  - learning/getting_started/client_setup.md
  - learning/getting_started/first_extraction.md
  - learning/getting_started/installation.md
  - learning/getting_started/response_models.md
  - learning/patterns/field_validation.md
  - learning/patterns/list_extraction.md
  - learning/patterns/nested_structure.md
  - learning/patterns/optional_fields.md
  - learning/patterns/prompt_templates.md
  - learning/patterns/simple_object.md
  - learning/streaming/basics.md
  - learning/streaming/lists.md
  - learning/validation/basics.md
  - learning/validation/custom_validators.md
  - learning/validation/field_level_validation.md
  - learning/validation/retry_mechanisms.md
  hash: 3e793197ba6ac51caef1d12f465dd1d6
  keywords:
  - Instructor library
  - LLM integration
  - structured outputs
  - data extraction
  - Python tutorial
  - AI applications
  - output validation
  - real-time processing
  references:
  - learning/getting_started/installation.md
  - learning/getting_started/first_extraction.md
  - learning/getting_started/response_models.md
  - learning/getting_started/client_setup.md
  - learning/patterns/simple_object.md
  - learning/patterns/list_extraction.md
  - learning/patterns/nested_structure.md
  - learning/patterns/optional_fields.md
  - learning/patterns/field_validation.md
  - learning/patterns/prompt_templates.md
  - learning/validation/basics.md
  - learning/validation/field_level_validation.md
  - learning/validation/custom_validators.md
  - learning/validation/retry_mechanisms.md
  - learning/streaming/basics.md
  - learning/streaming/lists.md
  - learning/getting_started/installation.md
  summary: This comprehensive tutorial for the Instructor library provides a complete
    guide on utilizing LLMs for structured outputs, covering everything from installation
    to advanced data extraction patterns. It is designed for developers aiming to
    create reliable AI applications using various language models like GPT-4, Claude,
    and Gemini.
  topics:
  - LLM integration basics
  - structured output patterns
  - data extraction tutorials
  - output validation
  - streaming LLM responses
learning/patterns/field_validation.md:
  ai_references:
  - '[Fields](../../concepts/fields.md)'
  - '[Custom Validators](../validation/custom_validators.md)'
  - '[Nested Structure](nested_structure.md)'
  - '[Validation Basics](../validation/basics.md)'
  - '[Field-level Validation](../validation/field_level_validation.md)'
  - '[Retry Mechanisms](../validation/retry_mechanisms.md)'
  - '[Enums](../../concepts/enums.md)'
  - '[Optional Fields](optional_fields.md)'
  cross_links:
  - concepts/enums.md
  - concepts/fields.md
  - learning/patterns/list_extraction.md
  - learning/patterns/nested_structure.md
  - learning/patterns/optional_fields.md
  - learning/validation/basics.md
  - learning/validation/custom_validators.md
  - learning/validation/field_level_validation.md
  - learning/validation/retry_mechanisms.md
  hash: cbe2f1fced3d98448d736783e49fcd08
  keywords:
  - field validation
  - Pydantic
  - data quality
  - validation logic
  - structured data extraction
  - custom validators
  - model validation
  - error handling
  - instructor
  references:
  - concepts/fields.md
  - learning/validation/custom_validators.md
  - learning/patterns/nested_structure.md
  - learning/patterns/list_extraction.md
  - concepts/enums.md
  - learning/validation/retry_mechanisms.md
  - learning/validation/basics.md
  - learning/validation/custom_validators.md
  - learning/validation/field_level_validation.md
  - learning/validation/retry_mechanisms.md
  - concepts/fields.md
  - concepts/enums.md
  - learning/patterns/optional_fields.md
  - learning/validation/custom_validators.md
  - learning/patterns/nested_structure.md
  summary: This guide explains how to implement field validation for structured data
    extraction using the Instructor framework, leveraging Pydantic's validation features
    to ensure data quality and compliance with defined criteria. It discusses basic
    and complex validation methods, including field-level, model-level, and validation
    with enumerations, while providing practical code examples.
  topics:
  - field validation methods
  - basic field constraints
  - complex validation logic
  - validation in nested structures
  - error handling
learning/patterns/list_extraction.md:
  ai_references:
  - '[../streaming/basics.md'
  - ../streaming/lists.md
  - ./field_validation.md
  - ../validation/basics.md
  - ./simple_object.md
  - ./nested_structure.md
  - ../../concepts/lists.md
  - ../../examples/action_items.md]
  cross_links:
  - concepts/lists.md
  - examples/action_items.md
  - learning/patterns/field_validation.md
  - learning/patterns/nested_structure.md
  - learning/patterns/simple_object.md
  - learning/streaming/basics.md
  - learning/streaming/lists.md
  - learning/validation/basics.md
  hash: 85a3d3e972d716f00c22f1128ae94c7e
  keywords:
  - '[list extraction'
  - LLM
  - GPT-4
  - Pydantic
  - data validation
  - streaming
  - Python
  - nested lists
  - Instructor
  - structured data]
  references:
  - learning/streaming/basics.md
  - learning/streaming/lists.md
  - learning/patterns/field_validation.md
  - learning/validation/basics.md
  - examples/action_items.md
  - learning/patterns/simple_object.md
  - learning/patterns/nested_structure.md
  - learning/streaming/lists.md
  - concepts/lists.md
  - learning/patterns/nested_structure.md
  - learning/streaming/lists.md
  - learning/patterns/field_validation.md
  summary: This tutorial provides a comprehensive guide on extracting lists and arrays
    from language models like GPT-4, Claude, and Gemini using the Instructor package.
    It covers basic list extraction, nested lists, streaming capabilities, validation
    techniques, and constraints on list properties, making it an essential resource
    for developers working with structured data extraction.
  topics:
  - '[Basic List Extraction'
  - Nested Lists
  - List Validation
  - Direct List Extraction
  - Real-world Example]
learning/patterns/nested_structure.md:
  ai_references:
  - '[list_extraction.md'
  - optional_fields.md
  - field_validation.md
  - recursive.md
  - simple_object.md]
  cross_links:
  - examples/recursive.md
  - learning/patterns/field_validation.md
  - learning/patterns/list_extraction.md
  - learning/patterns/optional_fields.md
  - learning/patterns/simple_object.md
  - learning/validation/basics.md
  hash: 3e670af52d96c2ad1f78e1c8c38a4eb0
  keywords:
  - nested structures
  - hierarchical data
  - data extraction
  - Pydantic
  - Instructor library
  - validation
  - optional fields
  - recursive structures
  - Python
  references:
  - learning/patterns/list_extraction.md
  - learning/patterns/optional_fields.md
  - learning/patterns/field_validation.md
  - learning/validation/basics.md
  - examples/recursive.md
  - learning/patterns/simple_object.md
  - learning/patterns/list_extraction.md
  - learning/patterns/optional_fields.md
  - examples/recursive.md
  - learning/patterns/field_validation.md
  summary: This guide provides comprehensive instructions on extracting nested structured
    data using the Instructor library. It covers various topics such as basic nested
    structures, multiple levels of nesting, handling optional fields, and validating
    nested structures, making it a valuable resource for developers working with hierarchical
    data relationships.
  topics:
  - nested structures
  - multiple levels of nesting
  - optional nested fields
  - nested structure validation
  - recursive structures
learning/patterns/optional_fields.md:
  ai_references:
  - '[Missing Concepts](../../concepts/maybe.md)'
  - '[Simple Object Extraction](./simple_object.md)'
  - '[Field Validation](./field_validation.md)'
  - '[Nested Structure](./nested_structure.md)'
  - '[Prompt Templates](./prompt_templates.md)'
  cross_links:
  - concepts/maybe.md
  - learning/patterns/field_validation.md
  - learning/patterns/nested_structure.md
  - learning/patterns/prompt_templates.md
  - learning/patterns/simple_object.md
  hash: 5912fc79517ab7b3180183d20e725802
  keywords:
  - optional fields
  - Python
  - Pydantic
  - data models
  - validation
  - Maybe type
  - nested structures
  - default values
  references:
  - concepts/maybe.md
  - learning/patterns/simple_object.md
  - learning/patterns/field_validation.md
  - learning/patterns/nested_structure.md
  - concepts/maybe.md
  - learning/patterns/field_validation.md
  - learning/patterns/nested_structure.md
  - learning/patterns/prompt_templates.md
  summary: This guide provides an overview of how to implement optional fields in
    data models using Python and Pydantic. It explains their benefits, how to set
    default values, and discusses validation techniques, including handling nested
    structures and uncertain fields with the Maybe type.
  topics:
  - working with optional fields
  - setting default values
  - validation techniques
  - handling uncertain fields
  - using nested structures
learning/patterns/prompt_templates.md:
  ai_references:
  - '[simple_object.md'
  - list_extraction.md
  - optional_fields.md
  - prompting.md
  - templating.md
  - field_validation.md
  - nested_structure.md]
  cross_links:
  - concepts/prompting.md
  - concepts/templating.md
  - learning/patterns/field_validation.md
  - learning/patterns/list_extraction.md
  - learning/patterns/nested_structure.md
  - learning/patterns/optional_fields.md
  - learning/patterns/simple_object.md
  - prompting/thought_generation/chain_of_thought_zero_shot/analogical_prompting.md
  - prompting/thought_generation/chain_of_thought_zero_shot/step_back_prompting.md
  - prompting/zero_shot/emotion_prompting.md
  - prompting/zero_shot/role_prompting.md
  - prompting/zero_shot/style_prompting.md
  hash: ea4ae44a438b1728732ed8bdc0573961
  keywords:
  - prompt templates
  - structured data extraction
  - parameterized prompts
  - Python
  - OpenAI
  references:
  - learning/patterns/simple_object.md
  - learning/patterns/list_extraction.md
  - learning/patterns/optional_fields.md
  - concepts/prompting.md
  - concepts/templating.md
  - learning/patterns/field_validation.md
  - learning/patterns/list_extraction.md
  - learning/patterns/nested_structure.md
  summary: This guide provides an overview of using prompt templates with Instructor
    for structured data extraction. It outlines the benefits of prompt templates,
    demonstrates how to create basic and complex templates using Python, and shares
    best practices for effective prompt engineering.
  topics:
  - importance of prompt templates
  - creating basic and complex templates
  - best practices for prompts
  - using f-strings
  - template functions
learning/patterns/simple_object.md:
  ai_references:
  - '[list_extraction.md'
  - nested_structure.md
  - field_validation.md]
  cross_links:
  - learning/patterns/field_validation.md
  - learning/patterns/list_extraction.md
  - learning/patterns/nested_structure.md
  hash: 80d435d6ae347a1e8d1ef3dfa715526c
  keywords:
  - '[LLM extraction'
  - Pydantic
  - structured data
  - Python
  - GPT-4
  - data validation
  - object extraction
  - schema definition]
  references:
  - learning/patterns/list_extraction.md
  - learning/patterns/nested_structure.md
  - learning/patterns/field_validation.md
  summary: This tutorial provides a comprehensive guide on extracting structured data
    from unstructured text using Large Language Models (LLMs) like GPT-4 and Claude.
    It covers various topics including schema definitions, handling missing data,
    and validation with Pydantic, as well as offers practical code examples and common
    use cases for LLM object extraction.
  topics:
  - '[LLM Object Extraction'
  - Pydantic Validation
  - Handling Missing Data
  - Nested Object Extraction
  - Common Use Cases]
learning/streaming/basics.md:
  ai_references:
  - '[lists.md'
  - ../validation/basics.md]
  cross_links:
  - learning/streaming/lists.md
  - learning/validation/basics.md
  hash: b5246fcd0ecaf2a3d6cb1c7c2bf0f8b7
  keywords:
  - '[streaming'
  - structured response
  - user interface
  - real-time updates
  - Python example
  - OpenAI
  - progressive updates
  - data processing
  - completion tracking]
  references:
  - learning/streaming/lists.md
  - learning/validation/basics.md
  summary: Streaming enables immediate receipt of structured data responses, enhancing
    user experience with faster perceived responses and dynamic UI updates. By leveraging
    streaming, users can begin to process information as soon as it is available,
    rather than waiting for a complete response.
  topics:
  - '[Streaming benefits'
  - Python implementation
  - progress tracking
  - data processing
  - structured responses]
learning/streaming/lists.md:
  ai_references:
  - '[basics.md'
  - ../../learning/patterns/list_extraction.md
  - ../../learning/validation/basics.md
  - ../../concepts/partial.md
  - ../../learning/validation/field_level_validation.md
  - ../../integrations/index.md]
  cross_links:
  - concepts/partial.md
  - index.md
  - integrations/index.md
  - learning/patterns/list_extraction.md
  - learning/streaming/basics.md
  - learning/validation/basics.md
  - learning/validation/field_level_validation.md
  hash: e761179dfde4bfb077da2e8da9b5ed15
  keywords:
  - streaming lists
  - structured data
  - Pydantic model
  - OpenAI
  - responsiveness
  - task generation
  - Python typing
  - project tasks
  - validation
  references:
  - learning/streaming/basics.md
  - learning/patterns/list_extraction.md
  - learning/validation/basics.md
  - concepts/partial.md
  - learning/validation/basics.md
  - learning/validation/field_level_validation.md
  - integrations/index.md
  summary: This guide explains how to stream lists of structured data using Instructor,
    enabling the processing of collection items as they are generated for enhanced
    responsiveness, especially with larger outputs. It includes detailed examples
    demonstrating the streaming of books and tasks, while highlighting the integration
    with Python's typing and Pydantic models.
  topics:
  - list streaming
  - data processing
  - real-world examples
  - Pydantic and typing
  - validation concepts
learning/validation/basics.md:
  ai_references:
  - '[custom_validators.md'
  - retry_mechanisms.md
  - field_level_validation.md]
  cross_links:
  - learning/validation/custom_validators.md
  - learning/validation/field_level_validation.md
  - learning/validation/retry_mechanisms.md
  hash: 81731be3c79784e84c33b91d626d2ca4
  keywords:
  - LLM validation
  - data integrity
  - business compliance
  - structured data
  - Pydantic
  - constraint validation
  - automatic retry
  - age verification
  - validation rules
  references:
  - learning/validation/custom_validators.md
  - learning/validation/retry_mechanisms.md
  - learning/validation/field_level_validation.md
  summary: This tutorial guides users through the process of validating outputs from
    Language Learning Models (LLMs) using Instructor's validation system. It ensures
    that LLM-generated structured data meets data integrity, business compliance,
    and production reliability standards.
  topics:
  - LLM output validation
  - validation pipeline
  - constraint validation patterns
  - common use cases
  - error messaging
learning/validation/custom_validators.md:
  ai_references:
  - '[Validation Basics](../../concepts/validation.md)'
  - '[Retrying](../../concepts/retrying.md)'
  - '[Field-level Validation](../../concepts/fields.md)'
  - '[Validators](../../concepts/reask_validation.md)'
  - '[Contact Information Extraction](../../examples/extract_contact_info.md)'
  - '[Semantic Validation](../../concepts/semantic_validation.md)'
  - '[Self-Correction](../../examples/self_critique.md)'
  - '[Fields](../../concepts/fields.md)'
  - '[Models](../../concepts/models.md)'
  - '[Types](../../concepts/types.md)'
  cross_links:
  - concepts/fields.md
  - concepts/models.md
  - concepts/reask_validation.md
  - concepts/retrying.md
  - concepts/semantic_validation.md
  - concepts/types.md
  - concepts/validation.md
  - examples/extract_contact_info.md
  - examples/self_critique.md
  hash: 9185a575da5ee54cba0ee4af777506dc
  keywords:
  - custom validators
  - data quality
  - Pydantic
  - semantic validation
  - GPT-4
  - Claude
  - validation techniques
  - rule-based validation
  - validation failures
  references:
  - concepts/validation.md
  - concepts/retrying.md
  - concepts/fields.md
  - concepts/reask_validation.md
  - examples/extract_contact_info.md
  - concepts/semantic_validation.md
  - concepts/retrying.md
  - examples/self_critique.md
  - concepts/validation.md
  - concepts/fields.md
  - concepts/models.md
  - concepts/types.md
  summary: This tutorial provides a comprehensive guide on building custom validators
    for outputs from language models like GPT-4 and Claude, focusing on rule-based
    and semantic validation techniques. By utilizing Pydantic, it demonstrates effective
    validation strategies to enhance data quality and ensure compliance with specific
    requirements when working with LLMs.
  topics: []
learning/validation/field_level_validation.md:
  ai_references:
  - '[Fields](../../concepts/fields.md)'
  - '[Custom Validators](../../concepts/reask_validation.md)'
  - '[Validation Basics](../../concepts/validation.md)'
  - '[Retry Mechanisms](../../concepts/retrying.md)'
  - '[Fallback Strategies](../../concepts/error_handling.md)'
  - '[Types](../../concepts/types.md)'
  cross_links:
  - concepts/error_handling.md
  - concepts/fields.md
  - concepts/reask_validation.md
  - concepts/retrying.md
  - concepts/types.md
  - concepts/validation.md
  hash: 035cef4c2f6e04d1df6a9474fee288cd
  keywords:
  - field-level validation
  - Pydantic
  - custom validators
  - validation errors
  - data models
  - business rules
  - error handling
  - data cleaning
  references:
  - concepts/fields.md
  - concepts/fields.md
  - concepts/reask_validation.md
  - concepts/validation.md
  - concepts/retrying.md
  - concepts/error_handling.md
  - concepts/types.md
  summary: This guide provides an overview of field-level validation using Instructor
    and Pydantic, detailing how to create specific validation rules for individual
    fields in data models, including custom validators and handling validation errors.
    It offers practical examples and best practices to ensure effective validation
    processes for your applications.
  topics:
  - field-level validation
  - basic field validation
  - custom field validators
  - validating multiple fields
  - best practices
learning/validation/retry_mechanisms.md:
  ai_references:
  - '[Retrying](../../concepts/retrying.md)'
  - '[Fallback Strategies](../../concepts/error_handling.md)'
  - '[Custom Validators](custom_validators.md)'
  - '[Field-level Validation](field_level_validation.md)'
  - '[Validation](../../concepts/validation.md)'
  - '[Self Critique](../../examples/self_critique.md)'
  cross_links:
  - concepts/error_handling.md
  - concepts/reask_validation.md
  - concepts/retrying.md
  - concepts/validation.md
  - examples/self_critique.md
  - learning/validation/custom_validators.md
  - learning/validation/field_level_validation.md
  hash: edfb05018be5a6e42122afbdf99d2292
  keywords:
  - retry mechanisms
  - validation failures
  - feedback loop
  - customization options
  - error handling
  - Pydantic model
  - validation messages
  - complex schemas
  references:
  - concepts/retrying.md
  - concepts/error_handling.md
  - learning/validation/custom_validators.md
  - learning/validation/field_level_validation.md
  - concepts/retrying.md
  - concepts/validation.md
  - concepts/reask_validation.md
  - concepts/error_handling.md
  - examples/self_critique.md
  - learning/validation/field_level_validation.md
  - learning/validation/custom_validators.md
  summary: This guide provides an overview of retry mechanisms in Instructor that
    manage validation failures, allowing the LLM to generate valid responses by reattempting
    with feedback. It includes examples and customization options for retry behavior,
    error handling strategies, and advanced validation patterns for complex schemas.
  topics:
  - Retry Mechanisms
  - Customizing Retry Behavior
  - Handling Retry Failures
  - Error Messages and Feedback
  - Advanced Validation Patterns
modes-comparison.md:
  cross_links: []
  hash: 34ad27dd0581822450f815a8043699ce
  references: []
  summary: This Mode Comparison Guide explains the different structured data extraction
    modes available in Instructor for various large language model (LLM) providers,
    including OpenAI, Anthropic, Google Gemini, Vertex AI, and more. It highlights
    key modes such as `TOOLS`, `JSON`, `MD_JSON`, and provider-specific options, detailing
    their best use cases, advantages, and compatibility. The guide offers practical
    recommendations for selecting the appropriate mode based on complexity, reliability,
    and provider capabilities, with a focus on optimizing data extraction, structured
    output, and multi-modal inputs. Key keywords include LLM, Instructor modes, AI
    tool calling, JSON output, structured data, OpenAI, Anthropic, Google Gemini,
    Vertex AI, AI prompt engineering, and API integration.
newsletter.md:
  cross_links: []
  hash: c286128e131ad3635534c9cd9bae2668
  references: []
  summary: "Subscribe to the Instructor Newsletter to stay updated on AI tips, blog\
    \ posts, research, and new features. The newsletter provides insights into AI\
    \ development, structured outputs, LLM research, and community tricks to enhance\
    \ your projects. Stay informed about Instructor\u2019s latest updates and community\
    \ insights to improve your AI skills and leverage Instructor effectively. Keywords\
    \ include AI updates, Instructor features, structured outputs, LLM research, AI\
    \ development, and community tips."
prompting/decomposition/decomp.md:
  cross_links: []
  hash: dd1d49ee871acabb8d368a16ea3150fe
  references: []
  summary: 'Decomposed Prompting leverages a Language Model (LLM) to break down complex
    tasks into manageable sub-tasks, streamlining the problem-solving process. By
    implementing a system of data models and functions, such as `Split`, `StrPos`,
    and `Merge`, this approach enables systematic handling of intricate problems.
    The `derive_action_plan` function orchestrates action plans using specified functions,
    executed step-by-step to achieve the task goals. This modular method optimizes
    LLM performance for challenging tasks, demonstrating effective AI-driven automation
    and problem decomposition. Key terms: Decomposed Prompting, Language Model (LLM),
    task decomposition, AI automation, action plan, modular approach.'
prompting/decomposition/faithful_cot.md:
  cross_links: []
  hash: f5dd3db43b8242151bac111cab990918
  references: []
  summary: 'The concept of "Faithful Chain of Thought" in language models focuses
    on enhancing the accuracy of reasoning by dividing the process into two stages:
    Translation and Problem Solving. In the Translation stage, a user query is broken
    down into executable reasoning steps, which are task-specific and deterministically
    executed in the Problem Solving stage, ensuring consistency in the derived answer.
    Examples include converting math word problems into executable Python code, using
    multi-step reasoning in Multi-Hop QA with Python and Datalog, and generating plans
    with symbolic goals through a PDDL Planner. The approach aims to improve the faithfulness
    and effectiveness of language models in problem-solving tasks.'
prompting/decomposition/least_to_most.md:
  ai_references:
  - '[Least-to-Most Prompting Enables Complex Reasoning in Large Language Models](https://arxiv.org/abs/2205.10625)'
  - '[The Prompt Report: A Systematic Survey of Prompting Techniques](https://arxiv.org/abs/2406.06608)'
  cross_links: []
  hash: b2b9a6686aaa01df537e9fc5d8155f0f
  keywords:
  - Least-to-Most
  - prompting technique
  - language models
  - subproblems
  - complex reasoning
  - sequential solving
  references: []
  summary: The Least-to-Most prompting technique is designed to decompose complex
    problems into simpler, sequentially solved subproblems. This approach allows language
    models to leverage earlier answers to inform subsequent solutions effectively.
  topics:
  - prompting techniques
  - problem decomposition
  - language model solutions
  - subproblem analysis
prompting/decomposition/plan_and_solve.md:
  cross_links: []
  hash: 7efc5f74390a69beeaf130c9b6c31583
  references: []
  summary: 'Plan and Solve enhances zero-shot Chain of Thought prompting by incorporating
    detailed instructions to improve reasoning accuracy in large language models.
    This approach involves a two-step process: first, devising a comprehensive problem-solving
    plan with explicit reasoning, and second, extracting the final answer based on
    this reasoning. By guiding models to pay closer attention to intermediate calculations
    and logical steps, Plan and Solve achieves more robust performance on various
    reasoning tasks, making it a valuable technique for improving LLM reasoning capabilities
    and accuracy. Key words include zero-shot Chain of Thought, reasoning, prompt
    engineering, large language models, problem-solving, and step-by-step reasoning.'
prompting/decomposition/program_of_thought.md:
  cross_links: []
  hash: 8413ae10bbc35a4f1128759ca3e4f673
  references: []
  summary: The "Program Of Thought" is an innovative approach that leverages an external
    Python interpreter to generate intermediate reasoning steps, enhancing performance
    in mathematical and programming tasks. It involves systematically writing executable
    code within designated frameworks, such as the instructor system, to derive precise
    answers. Key features include the use of a specific program prefix, validation
    of code execution, and integration with AI models like GPT-4 to generate detailed
    problem-solving workflows, predictions, and accurate answer selection for complex
    questions. This method aims to ground AI reasoning in deterministic code execution,
    improving accuracy and transparency in problem-solving.
prompting/decomposition/recurs_of_thought.md:
  cross_links: []
  hash: 5ef001050e89f56ecc769095df6300f4
  references: []
  summary: This document is a work in progress (WIP) and currently does not contain
    specific content. Once completed, it will outline the core ideas, objectives,
    and key points for effective SEO optimization, focusing on relevant keywords and
    important details.
prompting/decomposition/skeleton_of_thought.md:
  cross_links: []
  hash: 0aa74871fabd9647ac212ef8198a86b2
  references: []
  summary: '"Skeleton-of-Thought" is a technique to decrease latency in LLM (Large
    Language Model) pipelines by generating a skeleton outline of a response before
    expanding on each point in parallel. The method involves using parallel API calls
    or batched decoding to enhance efficiency. The core process includes formulating
    a question, creating a brief skeleton outline with 3-10 points, and then expanding
    each point simultaneously. An example implementation with Python demonstrates
    how to achieve this using the `instructor` library and `AsyncOpenAI` for faster
    response generation. Key terms include "Skeleton-of-Thought," "parallel generation,"
    "LLM pipeline," and "response efficiency."'
prompting/decomposition/tree-of-thought.md:
  cross_links: []
  hash: 5ef001050e89f56ecc769095df6300f4
  references: []
  summary: The content appears to be a placeholder or work-in-progress (WIP) without
    any available details, title, or description. To optimize for search engines (SEO),
    ensure to include key concepts, objectives, and important keywords once the content
    is finalized. Focus on crafting a summary that highlights central themes or topics,
    such as the purpose of the document, its main points, and any crucial information
    it aims to convey.
prompting/ensembling/cosp.md:
  cross_links:
  - prompting/ensembling/self_consistency.md
  hash: 4b8eb058102072272fcb938bb8861a5c
  references:
  - prompting/ensembling/self_consistency.md
  summary: Consistency Based Self Adaptive Prompting (COSP) is an ensemble technique
    designed to enhance large language model (LLM) performance by generating high-quality
    few-shot examples through self-consistency and normalized entropy metrics. It
    automatically selects the most reliable responses from multiple reasoning chains
    based on answer diversity and repetitiveness, then incorporates these examples
    into prompts for improved accuracy. COSP employs strategies like cosine similarity
    for evaluating repetitiveness and aims to optimize answer correctness without
    ground truth labels, making it a key method for self-adaptive prompt engineering,
    ensemble reasoning, and LLM accuracy improvement.
prompting/ensembling/dense.md:
  cross_links: []
  hash: 4b90091a3795f75f4fc3162a22bf6ec7
  references: []
  summary: Demonstration Ensembling (DENSE) is a technique to improve language model
    performance by generating multiple responses using different subsets of training
    examples and then aggregating these outputs for a final decision. This method
    involves prompting models like GPT-4 with varied few-shot prompts, partitioning
    examples equally or sampling via embedding clustering. The approach enhances accuracy
    by leveraging self-consistent responses and ensemble methods. Implementation can
    be achieved using tools like the `instructor` library and asynchronous programming
    in Python. Key concepts include few-shot learning, in-context learning, model
    ensembling, prompt engineering, and response aggregation, making DENSE a valuable
    strategy for tasks like classification and decision-making in NLP applications.
prompting/ensembling/diverse.md:
  cross_links: []
  hash: b93329f06d2f82403fdc0efd37b286f3
  references: []
  summary: Diverse Verifier On Reasoning Step (DiVeRSe) is an advanced prompting technique
    that enhances reasoning accuracy by generating multiple diverse prompts and leveraging
    AI-based scoring to select the best response. It utilizes self-consistency through
    multiple reasoning paths, combined with a fine-tuned verifier (initially DeBERTa-V3-Large,
    now GPT-4o) to assess response quality and individual reasoning steps. DiVeRSe
    aims to improve multi-step reasoning, accuracy, and robustness in AI models, making
    it suitable for applications like question-answering, problem-solving, and reasoning
    tasks. Key concepts include diverse prompt generation, self-consistency, step-wise
    verification, and AI-based scoring for optimal decision-making in language models.
prompting/ensembling/max_mutual_information.md:
  cross_links: []
  hash: 2ec748390bb663e6c289e4ec676cb6f2
  references: []
  summary: Max Mutual Information is a prompting technique for optimizing large language
    models (LLMs) by generating multiple prompt templates and selecting the one that
    maximizes mutual information between the prompt and the model's output. It focuses
    on reducing uncertainty by calculating entropy and mutual information, which measures
    the reduction in entropy when the prompt is used. The method involves estimating
    probabilities and entropies to identify the most effective prompt for eliciting
    accurate responses, especially in complex tasks like story comprehension. Implementation
    involves generating responses with different prompts, scoring model confidence,
    and calculating mutual information to select the best prompt, enhancing LLM performance
    in applications such as the Story Cloze dataset. Key concepts include mutual information,
    entropy, prompt optimization, LLM prompting strategies, and OpenAI API integration.
prompting/ensembling/meta_cot.md:
  cross_links: []
  hash: d6d91ade7fb984ca99f6e2097c2cb08f
  references: []
  summary: 'Meta Chain Of Thought (Meta COT) is an advanced reasoning framework that
    decomposes complex queries into multiple sub-questions, aggregates responses,
    and leverages multiple reasoning chains to improve accuracy. Implemented using
    OpenAI''s models, it facilitates step-by-step problem solving by generating sub-queries,
    evaluating reasoning pathways, and synthesizing final answers through a multi-stage
    process. Key features include query decomposition, reasoning chain generation,
    and context-aware final responses, making Meta COT ideal for complex question
    answering, AI reasoning, and improving model accuracy. Keywords: Meta Chain Of
    Thought, multi-step reasoning, query decomposition, AI reasoning, OpenAI, question
    answering, model accuracy.'
prompting/ensembling/more.md:
  cross_links: []
  hash: 1f26fd2b6a81ae83f6db67299dde096c
  references: []
  summary: MoRE (Mixture of Reasoning Experts) enhances AI question-answering by combining
    diverse specialized reasoning models, such as Factual, Multihop, Math, and Commonsense
    experts. Each expert employs distinct prompts and reasoning techniques to generate
    responses, which are then scored using a classifier like a random forest to select
    the best answer or abstain if quality is low. A simplified implementation using
    OpenAI's instructor facilitates multi-expert responses and scoring, improving
    overall accuracy across varied reasoning tasks. Key keywords include reasoning
    experts, AI, question answering, multi-step reasoning, factual retrieval, mathematical
    reasoning, commonsense, prompt engineering, and model scoring.
prompting/ensembling/prompt_paraphrasing.md:
  cross_links: []
  hash: e8f28524643be6affb1b760f6e930184
  references: []
  summary: 'This guide explores using Large Language Models (LLMs) for back translation
    to enhance prompt performance and diversity. It details methods for paraphrasing
    prompts through translation into different languages and back to English, leveraging
    tools like the instructor package with OpenAI''s GPT-4. The approach improves
    prompt phrasing and robustness, especially for tasks like sentiment analysis of
    user reviews. Key techniques include multilingual translation, prompt variation,
    and leveraging AI for more effective, diverse prompt generation to optimize LLM
    responses. Keywords: Large Language Models, back translation, prompt paraphrasing,
    prompt engineering, multilingual translation, AI prompt optimization, sentiment
    analysis.'
prompting/ensembling/self_consistency.md:
  cross_links: []
  hash: 6b158b0f8d82d71ae624d4f277ef6824
  references: []
  summary: Self-Consistency is a technique aimed at improving large language model
    (LLM) performance by generating multiple potential responses and selecting the
    most common answer through majority voting. It involves sampling several candidate
    solutions in parallel and analyzing their consistency to enhance accuracy in tasks
    like question answering. The approach is implemented using Python code with the
    `instructor` library and OpenAI's API, showcasing how to generate and aggregate
    multiple responses to derive the most probable correct answer. This method leverages
    concepts from the research paper "Self-Consistency Improves Chain Of Thought Reasoning
    In Language Models" and emphasizes improved reasoning, accuracy, and model performance
    through sampling, majority voting, and ensemble techniques. Key keywords include
    Self-Consistency, large language models, multiple responses, accuracy, ensemble
    method, majority vote, and chain-of-thought reasoning.
prompting/ensembling/universal_self_consistency.md:
  cross_links: []
  hash: c56d66bc14be41f9caa4b7b50a9354cb
  references: []
  summary: Universal Self-Consistency is an advanced approach that enhances traditional
    self-consistency techniques by employing a second large language model (LLM) to
    evaluate and select the most consistent answer among multiple candidates. This
    method improves response diversity and accuracy by supporting various response
    formats and leveraging consensus-based evaluation. Implemented using tools like
    OpenAI's GPT models and the Instructor framework, it involves generating multiple
    responses, assessing their consistency, and choosing the most reliable answer.
    Key concepts include large language models, self-consistency, response evaluation,
    answer selection, and AI accuracy enhancement, making it a valuable strategy for
    improving LLM performance in complex reasoning tasks.
prompting/ensembling/usp.md:
  cross_links:
  - prompting/few_shot/cosp.md
  hash: 3a3df5b548bd422f3f7f84ef8e488300
  references:
  - prompting/few_shot/cosp.md
  summary: "Universal Self Prompting (USP) is a two-step technique for enhancing large\
    \ language models by generating and selecting exemplars from unlabeled data. The\
    \ process involves first creating candidate responses for different task types\u2014\
    classification, short form generation, and long form generation\u2014using specific\
    \ evaluation metrics tailored to each task. These metrics include normalized entropy,\
    \ pairwise ROUGE scores, and label probabilities. In the second step, the best\
    \ examples are appended as prompts for the LLM to produce final predictions with\
    \ a single inference. USP aims to improve model performance across diverse NLP\
    \ tasks through data-driven exemplar generation and selection, utilizing methods\
    \ like confidence-based sampling and task-specific scoring. Keywords include self\
    \ prompting, large language models, unlabeled data, exemplar generation, task-specific\
    \ evaluation, NLP, classification, text summarization, question answering, and\
    \ prompt optimization."
prompting/few_shot/cosp.md:
  cross_links:
  - prompting/ensembling/usp.md
  hash: c7e5e6103a5c6b02a7c30633495c3282
  references:
  - prompting/ensembling/usp.md
  summary: 'Consistency Based Self Adaptive Prompting (COSP) is an advanced technique
    for enhancing few-shot learning by selecting high-quality examples based on response
    consistency and confidence metrics such as entropy and repetitiveness. The method
    involves generating multiple responses for potential examples, calculating their
    entropy to measure variability, and evaluating repetitiveness to ensure reliability.
    COSP automates the selection of optimal examples, improving prompt effectiveness
    and model performance, while reducing manual curation. Key features include automated
    example selection, quantifiable quality metrics, and improved accuracy in few-shot
    prompting. Limitations include increased computational cost due to multiple API
    calls, but overall, COSP advances prompt engineering with a focus on consistency
    and confidence metrics for better language model outputs. Keywords: COSP, self-adaptive
    prompting, few-shot learning, response consistency, entropy, repetitiveness, prompt
    optimization, machine learning, language models.'
prompting/few_shot/example_generation/sg_icl.md:
  cross_links: []
  hash: 68c7f1b6ec1060da68f0da9a83eea8e1
  references: []
  summary: Self-Generated In-Context Learning (SG-ICL) is a technique that leverages
    large language models (LLMs) to automatically generate example prompts for tasks
    like sentiment analysis. By using tools such as the `instructor` library, SG-ICL
    creates in-context examples that improve model understanding and performance without
    manual data labeling. The method involves generating multiple example reviews
    with associated sentiments, which are then used to guide the model's predictions.
    This approach enhances prompt-based learning, utilizing GPT models like GPT-4,
    and is grounded in recent research on demonstration generation and prompt engineering.
    Key keywords include in-context learning, self-generated examples, LLM, prompt
    engineering, sentiment analysis, GPT, OpenAI, and demonstration generation.
prompting/few_shot/example_ordering.md:
  cross_links: []
  hash: 46fe78ea46e5f89593be648f251c8628
  references: []
  summary: This document highlights the significant impact of example ordering in
    few-shot prompting for large language models (LLMs), referencing studies that
    demonstrate how permutating example sequences can improve model performance. It
    discusses various methods to optimize example selection, including manual combinatorics,
    KATE (k-Nearest Example Tuning), and using unsupervised retrieval techniques to
    identify the most relevant in-context examples. These strategies aim to enhance
    few-shot learning, prompt engineering, and prompt relevance, making it essential
    for AI researchers and practitioners to consider example order and selection methods
    to maximize LLM effectiveness. Key keywords include few-shot prompting, LLM, prompt
    optimization, example ordering, KATE, unsupervised retrieval, prompt engineering,
    and in-context learning.
prompting/few_shot/exemplar_selection/knn.md:
  cross_links: []
  hash: 043cf2bc9050b9d8ac79ce9f24180ca2
  references: []
  summary: This guide demonstrates how to select effective in-context examples for
    language models using KNN and embeddings. The process involves embedding query
    examples, calculating cosine similarity-based distances, and retrieving the k
    most similar examples to improve response accuracy. The code showcases embedding
    questions, computing distances, selecting closest examples, and generating concise,
    precise answers using OpenAI's GPT-4 model. Keywords include KNN, in-context learning,
    embeddings, cosine similarity, prompt optimization, GPT-4, and language model
    tuning.
prompting/few_shot/exemplar_selection/vote_k.md:
  cross_links: []
  hash: 5ef001050e89f56ecc769095df6300f4
  references: []
  summary: The content appears to be a work in progress (wip) and does not include
    specific details or key points yet. To create an effective SEO summary, more information
    about the topic, objectives, and main ideas are needed. Once provided, I can generate
    a concise and keyword-rich summary suitable for SEO purposes.
prompting/index.md:
  ai_references:
  - '[The Prompt Report](https://trigaten.github.io/Prompt_Survey_Site)'
  - '[Learn Prompting](https://learnprompting.org)'
  cross_links:
  - prompting/decomposition/decomp.md
  - prompting/decomposition/faithful_cot.md
  - prompting/decomposition/least_to_most.md
  - prompting/decomposition/plan_and_solve.md
  - prompting/decomposition/program_of_thought.md
  - prompting/decomposition/recurs_of_thought.md
  - prompting/decomposition/skeleton_of_thought.md
  - prompting/decomposition/tree-of-thought.md
  - prompting/ensembling/cosp.md
  - prompting/ensembling/dense.md
  - prompting/ensembling/diverse.md
  - prompting/ensembling/max_mutual_information.md
  - prompting/ensembling/meta_cot.md
  - prompting/ensembling/more.md
  - prompting/ensembling/prompt_paraphrasing.md
  - prompting/ensembling/self_consistency.md
  - prompting/ensembling/universal_self_consistency.md
  - prompting/ensembling/usp.md
  - prompting/few_shot/example_generation/sg_icl.md
  - prompting/few_shot/example_ordering.md
  - prompting/few_shot/exemplar_selection/knn.md
  - prompting/few_shot/exemplar_selection/vote_k.md
  - prompting/self_criticism/chain_of_verification.md
  - prompting/self_criticism/cumulative_reason.md
  - prompting/self_criticism/reversecot.md
  - prompting/self_criticism/self_calibration.md
  - prompting/self_criticism/self_refine.md
  - prompting/self_criticism/self_verification.md
  - prompting/thought_generation/chain_of_thought_few_shot/active_prompt.md
  - prompting/thought_generation/chain_of_thought_few_shot/auto_cot.md
  - prompting/thought_generation/chain_of_thought_few_shot/complexity_based.md
  - prompting/thought_generation/chain_of_thought_few_shot/contrastive.md
  - prompting/thought_generation/chain_of_thought_few_shot/memory_of_thought.md
  - prompting/thought_generation/chain_of_thought_few_shot/prompt_mining.md
  - prompting/thought_generation/chain_of_thought_few_shot/uncertainty_routed_cot.md
  - prompting/thought_generation/chain_of_thought_zero_shot/analogical_prompting.md
  - prompting/thought_generation/chain_of_thought_zero_shot/step_back_prompting.md
  - prompting/thought_generation/chain_of_thought_zero_shot/tab_cot.md
  - prompting/thought_generation/chain_of_thought_zero_shot/thread_of_thought.md
  - prompting/zero_shot/emotion_prompting.md
  - prompting/zero_shot/rar.md
  - prompting/zero_shot/re2.md
  - prompting/zero_shot/role_prompting.md
  - prompting/zero_shot/s2a.md
  - prompting/zero_shot/self_ask.md
  - prompting/zero_shot/simtom.md
  - prompting/zero_shot/style_prompting.md
  hash: 05e6342a3a1492d7650955429328dc88
  keywords:
  - advanced prompting techniques
  - LLM performance
  - zero-shot
  - few-shot
  - reasoning methods
  - self-assessment
  - collaboration
  references:
  - prompting/zero_shot/emotion_prompting.md
  - prompting/zero_shot/role_prompting.md
  - prompting/zero_shot/style_prompting.md
  - prompting/zero_shot/s2a.md
  - prompting/zero_shot/simtom.md
  - prompting/zero_shot/rar.md
  - prompting/zero_shot/re2.md
  - prompting/zero_shot/self_ask.md
  - prompting/few_shot/example_generation/sg_icl.md
  - prompting/few_shot/example_ordering.md
  - prompting/few_shot/exemplar_selection/knn.md
  - prompting/few_shot/exemplar_selection/vote_k.md
  - prompting/thought_generation/chain_of_thought_zero_shot/analogical_prompting.md
  - prompting/thought_generation/chain_of_thought_zero_shot/step_back_prompting.md
  - prompting/thought_generation/chain_of_thought_zero_shot/thread_of_thought.md
  - prompting/thought_generation/chain_of_thought_zero_shot/tab_cot.md
  - prompting/thought_generation/chain_of_thought_few_shot/active_prompt.md
  - prompting/thought_generation/chain_of_thought_few_shot/auto_cot.md
  - prompting/thought_generation/chain_of_thought_few_shot/complexity_based.md
  - prompting/thought_generation/chain_of_thought_few_shot/contrastive.md
  - prompting/thought_generation/chain_of_thought_few_shot/memory_of_thought.md
  - prompting/thought_generation/chain_of_thought_few_shot/uncertainty_routed_cot.md
  - prompting/thought_generation/chain_of_thought_few_shot/prompt_mining.md
  - prompting/ensembling/cosp.md
  - prompting/ensembling/dense.md
  - prompting/ensembling/diverse.md
  - prompting/ensembling/max_mutual_information.md
  - prompting/ensembling/meta_cot.md
  - prompting/ensembling/more.md
  - prompting/ensembling/self_consistency.md
  - prompting/ensembling/universal_self_consistency.md
  - prompting/ensembling/usp.md
  - prompting/ensembling/prompt_paraphrasing.md
  - prompting/self_criticism/chain_of_verification.md
  - prompting/self_criticism/self_calibration.md
  - prompting/self_criticism/self_refine.md
  - prompting/self_criticism/self_verification.md
  - prompting/self_criticism/reversecot.md
  - prompting/self_criticism/cumulative_reason.md
  - prompting/decomposition/decomp.md
  - prompting/decomposition/faithful_cot.md
  - prompting/decomposition/least_to_most.md
  - prompting/decomposition/plan_and_solve.md
  - prompting/decomposition/program_of_thought.md
  - prompting/decomposition/recurs_of_thought.md
  - prompting/decomposition/skeleton_of_thought.md
  - prompting/decomposition/tree-of-thought.md
  summary: This guide offers an in-depth overview of advanced prompting techniques
    designed to enhance the performance of large language models (LLMs) through research-backed
    methods. It includes a comprehensive mapping of various strategies, including
    zero-shot, few-shot, and reasoning techniques, tailored for implementation with
    the Instructor framework.
  topics:
  - prompting techniques
  - reasoning methods
  - example usage
  - verification methods
  - implementation
prompting/self_criticism/chain_of_verification.md:
  cross_links: []
  hash: 73ebc5e56042b7f72031c9b68be3dc97
  references: []
  summary: Chain Of Verification (CoVe) is a method designed to enhance the reliability
    of large language model (LLM) responses through a multi-step validation process.
    It involves generating an initial answer, creating follow-up questions to verify
    key facts and assumptions, independently answering these questions, and finally
    using a final API call to confirm or correct the original response. This approach
    reduces hallucinations and improves accuracy, making it highly effective for ensuring
    trustworthy AI-generated content. Core keywords include LLM verification, AI validation,
    reducing hallucinations, prompt engineering, and response accuracy.
prompting/self_criticism/cumulative_reason.md:
  cross_links: []
  hash: dc7fbab50e534f394dab15dc2d13816c
  references: []
  summary: "Cumulative Reasoning enhances large language model performance by dividing\
    \ the reasoning process into three steps: propose, verify, and report. This structured\
    \ approach improves logical inference and mathematical problem-solving accuracy\
    \ by generating potential reasoning steps, validating their correctness, and determining\
    \ the conclusion. Implemented using OpenAI\u2019s API, this method ensures disciplined,\
    \ step-by-step deduction rooted in First-Order Logic, making it ideal for logical,\
    \ mathematical, and AI reasoning tasks. Key concepts include reasoning steps,\
    \ validation, logical inference, and advanced LLM prompting techniques for improved\
    \ reasoning accuracy."
prompting/self_criticism/reversecot.md:
  cross_links: []
  hash: 718094a1f90e542c567a278e52e4b731
  references: []
  summary: Reverse Chain Of Thought (RCoT) is a method for identifying logical inconsistencies
    in a large language model's reasoning process by reconstructing the original question
    from the generated solution. This three-step approach involves reconstructing
    the question, pinpointing discrepancies between original and reconstructed conditions,
    and providing targeted feedback for improvement. Implemented via a specialized
    framework, RCoT enhances prompt accuracy, logical coherence, and response quality,
    making it an effective tool for refining AI-generated reasoning and solutions.
    Key concepts include problem reconstruction, inconsistency detection, targeted
    feedback, and improving AI reasoning accuracy.
prompting/self_criticism/self_calibration.md:
  cross_links: []
  hash: 10cd8050ef8c5a0154316edb507747c1
  references: []
  summary: Self Calibration is a technique to help language models assess the confidence
    and validity of their responses. By evaluating their output using a structured
    prompt template and tools like the Instructor library, models can generate reasoning
    and determine whether answers are correct, without relying on internal hidden
    states. This approach enhances model reliability by enabling self-assessment of
    knowledge and uncertainties, which is essential for improving question-answering
    accuracy and trustworthiness in AI systems. Key concepts include self-calibration,
    confidence estimation, language model evaluation, prompt engineering, and AI reliability.
prompting/self_criticism/self_refine.md:
  ai_references:
  - '[Self-Refine: Iterative Refinement with Self-Feedback](https://arxiv.org/abs/2303.17651)'
  - '[The Prompt Report: A Systematic Survey of Prompting Techniques](https://arxiv.org/abs/2406.06608)'
  cross_links: []
  hash: 9339448f16ae6cc7645aba733b2efdcb
  keywords:
  - Self-refine
  - feedback
  - language model
  - iterative improvement
  - Python coding
  - refinement process
  - stopping condition
  - LLM
  references: []
  summary: Self-refine is a methodology that utilizes a language model to iteratively
    generate, evaluate, and improve its outputs based on user feedback. This process
    continues until specified stopping criteria are fulfilled, ensuring the output
    becomes more accurate and refined with each iteration.
  topics:
  - Iterative feedback loop
  - Generating initial responses
  - Providing feedback
  - Refining outputs
  - Implementing stopping conditions
prompting/self_criticism/self_verification.md:
  cross_links: []
  hash: 77d9f2d4e8bf08216987b11d2bf8679a
  references: []
  summary: 'This document outlines a self-verification framework for validating Large
    Language Model (LLM) responses through a two-stage process: forward reasoning
    and backward verification. The approach involves generating multiple response
    candidates using chain-of-thought reasoning, then verifying each candidate by
    rewriting the question into a declarative form and constructing verification prompts
    using True-False Item Verification (TFV) or Condition Mask Verification (CMV).
    The verification process repeats multiple times, and the candidate with the highest
    verification score is selected as the final answer. The framework is implemented
    with code examples using OpenAI''s API and aims to improve the accuracy and reliability
    of LLM outputs. Key concepts include self-verification, prompt engineering, declarative
    rewriting, LLM verification, chain-of-thought, and model prompting techniques.'
prompting/thought_generation/chain_of_thought_few_shot/active_prompt.md:
  cross_links: []
  hash: ec50ae930bfa92be2db89c937e696404
  references: []
  summary: 'Active prompting is a technique to enhance Large Language Model (LLM)
    performance by selecting effective examples for human annotation. This process
    involves four main steps: uncertainty estimation, selection, annotation, and inference.
    The uncertainty estimation step uses metrics like disagreement, entropy, and variance
    to measure how confident the LLM is in its responses. By querying the LLM multiple
    times, the differences in responses indicate areas of uncertainty. Selection involves
    choosing the most uncertain examples for human annotation, which are then used
    to improve the LLM''s inference capabilities. This method optimizes the use of
    labeled data to boost LLM accuracy and performance.'
prompting/thought_generation/chain_of_thought_few_shot/auto_cot.md:
  cross_links: []
  hash: aa45163a89881ec54d814f68e369d2df
  references: []
  summary: The article discusses improving the performance of few-shot Chain of Thought
    (CoT) reasoning by automating the selection of diverse examples. The method involves
    clustering potential examples, sorting them based on distance from cluster centers,
    and selecting those that meet predefined criteria, such as a maximum of five reasoning
    steps. This automated approach reduces reasoning errors by ensuring the examples
    are varied and representative. The implementation includes clustering with KMeans,
    encoding with Sentence Transformers, and using AI models like GPT-4 for processing.
    This technique enhances large language models' accuracy by systematically selecting
    examples for optimal performance. Key terms include few-shot CoT, clustering,
    diverse examples, reasoning error reduction, and automated example selection.
prompting/thought_generation/chain_of_thought_few_shot/complexity_based.md:
  cross_links: []
  hash: 08f5ce3a728a741234799bbaaede1acf
  references: []
  summary: 'The article discusses "Complexity Based Prompting" to enhance language
    model performance by selecting examples with more reasoning steps or longer responses
    when reasoning lengths aren''t available. This approach, known as "Complexity
    Based Consistency," involves sampling multiple responses and selecting the most
    complex ones based on reasoning step length. The process is implemented using
    tools like `instructor` and `AsyncOpenAI`, leveraging structured reasoning steps
    in query responses. By generating and ranking multiple responses, the method identifies
    top responses to derive accurate answers, as demonstrated with a practical example.
    Keywords: Complexity Based Prompting, language models, multi-step reasoning, AI
    performance, Complexity Based Consistency, `instructor`, `AsyncOpenAI`.'
prompting/thought_generation/chain_of_thought_few_shot/contrastive.md:
  cross_links: []
  hash: 607e1e5586ac745bccb961f0df089c17
  references: []
  summary: The document discusses the technique of Contrastive Chain Of Thought (CoT)
    to enhance language model performance by deliberately including incorrect reasoning
    examples alongside correct ones during training. This method helps the AI learn
    from mistakes and improve its response generation. The approach involves using
    a specific template with correct and incorrect examples to guide the AI in providing
    accurate answers. An example implementation is provided using Python and the `instructor`
    package to demonstrate the process. Key concepts include chain-of-thought prompting,
    incorrect reasoning, language model training, and AI performance enhancement.
prompting/thought_generation/chain_of_thought_few_shot/memory_of_thought.md:
  cross_links: []
  hash: 5ef001050e89f56ecc769095df6300f4
  references: []
  summary: It seems like the content is still a work in progress, as indicated by
    the "[wip]" tag. Since the title, description, and keywords are left empty, more
    information is needed to provide an accurate SEO summary. To optimize for SEO,
    consider focusing on the main topic of the content, its objectives, and any unique
    selling points or important details. Once more details are available, including
    keywords relevant to the content's subject, an effective summary can be crafted
    to improve search visibility.
prompting/thought_generation/chain_of_thought_few_shot/prompt_mining.md:
  cross_links: []
  hash: 214b95070291158fec9b154f77370f57
  references: []
  summary: 'The article discusses "Prompt Mining," a technique used to enhance the
    performance of Large Language Models (LLMs) by discovering effective prompt formats
    from text corpora, such as Wikipedia. The approach aims to identify better prompt
    structures that allow LLMs to respond more accurately. It contrasts manual prompts
    with mined prompts, presenting examples of both to illustrate improved prompt
    efficiency. The document outlines a method using the `instructor` library, demonstrating
    how to implement Prompt Mining to generate concise and clear prompt templates.
    Key points include the importance of prompt formatting, the use of placeholder
    templates, and the effectiveness of automated prompt discovery in improving language
    model outputs. Keywords: Prompt Mining, Large Language Models, prompt templates,
    language model performance, automated prompt discovery, `instructor` library.'
prompting/thought_generation/chain_of_thought_few_shot/uncertainty_routed_cot.md:
  cross_links: []
  hash: b90fa988c085d0dde6594aa75eac0544
  references: []
  summary: "The Uncertainty-Routed Chain Of Thought technique, detailed in the Gemini\
    \ Paper, enhances traditional Chain Of Thought methods by generating multiple\
    \ reasoning chains\u2014either 8 or 32\u2014and selecting the majority answer\
    \ only if it meets a specified threshold of agreement. Implemented in Python with\
    \ OpenAI's models, this approach involves using asynchronous prompts to create\
    \ a batch of responses, counting the majority vote, and comparing it to the confidence\
    \ threshold (e.g., 0.6) to determine the final answer. This technique is designed\
    \ to improve the accuracy and reliability of AI-generated answers in complex decision-making\
    \ scenarios. Key elements include uncertainty routing, batch processing, majority\
    \ voting, and threshold evaluation."
prompting/thought_generation/chain_of_thought_zero_shot/analogical_prompting.md:
  cross_links: []
  hash: daa15bd030a6f2d0584e310e29f781c0
  references: []
  summary: 'Analogical Prompting is a method designed to enhance the accuracy of large
    language models (LLMs) by prompting the model to generate relevant examples before
    addressing a user''s query. This technique leverages the extensive knowledge acquired
    by the LLM during training, encouraging it to recall pertinent problems and solutions.
    The process involves providing a problem, recalling three relevant and distinct
    problems with their solutions, and then solving the initial problem. A Python
    implementation using the `instructor` module demonstrates this method with an
    example query about calculating the area of a square using given vertices. This
    approach is based on research into LLMs as analogical reasoners, aimed at improving
    problem-solving capabilities. Key points include the use of templates, structured
    recall of problem-solving instances, and enhanced accuracy in query responses.
    Keywords: Analogical Prompting, large language models, LLMs, problem-solving,
    language model training, accuracy enhancement, Python implementation, example
    generation, query response.'
prompting/thought_generation/chain_of_thought_zero_shot/step_back_prompting.md:
  cross_links: []
  hash: 266f50f0729c9faf17ee37f0ee9ef6a2
  references: []
  summary: Step-back prompting is a two-step technique utilized with Large Language
    Models (LLMs) to improve contextual understanding and reasoning capabilities.
    The method involves first asking a high-level, topic-specific question, known
    as the "step-back question," to gather broader context. This is followed by "abstracted-grounded
    reasoning," where the LLM answers the initial query within the context provided
    by the step-back response. This technique has proven effective in enhancing performance
    on reasoning benchmarks for models like PaLM-2L and GPT-4. The implementation
    often involves generating step-back questions with LLM queries to ensure precise
    abstract questioning.
prompting/thought_generation/chain_of_thought_zero_shot/tab_cot.md:
  cross_links: []
  hash: 9d53b891d95c8c14d3bd15758757e736
  references: []
  summary: 'The text discusses the concept of Tabular Chain of Thought (Tab-CoT),
    a method to improve the reasoning and output quality of language models by structuring
    their reasoning in the form of markdown tables. It introduces a process using
    Python, OpenAI, and the `instructor` library to generate structured reasoning
    responses. This approach involves defining reasoning steps as objects, breaking
    down queries into subquestions, and detailing procedures and results, thus enhancing
    clarity and precision in model outputs. The example provided calculates the remaining
    loaves of bread at a bakery, showcasing the structured reasoning process. Keywords:
    Tabular Chain of Thought, Tab-CoT, language models, structured reasoning, markdown
    tables, Python, OpenAI, reasoning steps.'
prompting/thought_generation/chain_of_thought_zero_shot/thread_of_thought.md:
  cross_links: []
  hash: 2549f9996ba2068ab4cfd1b7f23cb083
  references: []
  summary: The article introduces the "Thread of Thought" technique, which enhances
    AI model responses by systematically focusing on relevant context and ignoring
    irrelevant information. This method improves reasoning performance and response
    quality by encouraging models to analyze and summarize information incrementally.
    The implementation involves using templates in Python with the OpenAI API to assess
    each piece of context for its significance. Key phrases and approaches are suggested
    for guiding models through the context effectively. This technique can be particularly
    useful for complex question-answering tasks that involve large datasets or lengthy
    documents.
prompting/zero_shot/emotion_prompting.md:
  cross_links: []
  hash: a9ad30ffe419f260e612691bf23edf9f
  references: []
  summary: This article explores the use of emotional stimuli in prompts to enhance
    the performance of language models. It highlights how adding emotionally significant
    phrases, such as "This is very important to my career," can influence model responses.
    The implementation example demonstrates prompting GPT-4 with emotional cues to
    generate curated outputs, like a list of musical albums from the 2000s. The content
    references research on emotional stimuli's impact on large language models and
    provides code snippets for practical application. Keywords include emotion prompting,
    language models, emotional stimuli, prompt engineering, GPT-4, AI performance,
    and AI enhancement.
prompting/zero_shot/rar.md:
  ai_references:
  - '[Rephrase and Respond: Let Large Language Models Ask Better Questions for Themselves](https://arxiv.org/abs/2311.04205)'
  cross_links: []
  hash: 90516c9b6f140155c4c52e871db56b47
  keywords:
  - Rephrase and Respond
  - ambiguous prompts
  - human intention
  - Python implementation
  - model interpretation
  - OpenAI
  - query clarification
  references: []
  summary: This documentation details the Rephrase and Respond (RaR) approach, designed
    to help models accurately interpret ambiguous prompts. It discusses identifying
    ambiguities in questions and provides an implementation example using Python code
    to demonstrate how to rephrase and respond effectively to queries.
  topics:
  - Ambiguity in language
  - Implementation guide
  - Python code example
  - Model interaction
  - Query rephrasing
prompting/zero_shot/re2.md:
  ai_references:
  - '[Re-Reading Improves Reasoning in Large Language Models](https://arxiv.org/abs/2309.06275)'
  cross_links: []
  hash: 75c4357d9ceaf62751ff55b2a874ac36
  keywords:
  - Re2
  - Re-Reading
  - query understanding
  - critical thinking
  - OpenAI
  - reasoning
  - implementation
  - Python
  - prompt template
  references: []
  summary: Re2 (Re-Reading) is a technique designed to enhance a model's comprehension
    of queries by prompting it to read the question again, encouraging critical thinking
    and step-by-step reasoning. This technique can be implemented using OpenAI's API
    to improve response accuracy in applications requiring deeper understanding.
  topics:
  - Re2 technique
  - model enhancement
  - critical thinking prompts
  - Python implementation
  - querying with OpenAI
prompting/zero_shot/role_prompting.md:
  ai_references:
  - '[RoleLLM](https://arxiv.org/abs/2310.00746)'
  - '[social roles evaluation](https://arxiv.org/abs/2311.10054)'
  - '[Multi-Persona Self-Collaboration](https://arxiv.org/abs/2307.05300)'
  cross_links: []
  hash: 42f69bb3b65ab7208e766d80c96c50ac
  keywords:
  - role prompting
  - persona prompting
  - model performance
  - open-ended tasks
  - AI assistant
  - poetry generation
  - social roles
  - multi-persona collaboration
  references: []
  summary: Role prompting, also known as persona prompting, enhances model performance
    on open-ended tasks by assigning specific roles to the model. This approach allows
    models to adopt a particular persona, which can significantly influence the quality
    and style of the output generated.
  topics:
  - role prompting implementation
  - influence of roles on AI output
  - examples of role assignments
  - systematic approach to choosing roles
prompting/zero_shot/s2a.md:
  cross_links: []
  hash: f3b55fc1bf5a617fa1dd82134ecaa495
  references: []
  summary: 'The System 2 Attention (S2A) technique enhances prompt relevance by auto-refining
    user input through a two-step process: rewriting prompts to include only pertinent
    information and then generating accurate responses. Implemented using GPT-4, S2A
    leverages prompt engineering inspired by recent research (arXiv:2311.11829) to
    improve model focus and answer precision. Key features include extracting relevant
    context from user queries and minimizing irrelevant data, making it valuable for
    optimized AI communication, prompt refinement, and advanced language model applications.
    Keywords: System 2 Attention, prompt refinement, AI prompt engineering, GPT-4,
    relevance extraction, model focus, arXiv 2311.11829.'
prompting/zero_shot/self_ask.md:
  cross_links: []
  hash: f25cf054eea8c90dcca3ab21a56f51b7
  references: []
  summary: Self-Ask is an innovative prompting technique designed to improve language
    model reasoning by addressing the compositionality gap. It encourages models to
    determine if follow-up questions are needed, generate and answer those questions,
    and then use these answers to produce a more accurate overall solution. Implemented
    using a zero-shot prompt with the instructor framework, Self-Ask enhances the
    ability of models like GPT-4 to handle complex queries through dynamic sub-problem
    solving. Key concepts include compositionality gap, follow-up questions, zero-shot
    prompting, and sub-problem answering for improved reasoning accuracy.
prompting/zero_shot/simtom.md:
  cross_links: []
  hash: b6f1003c8f869a54c705cd1f71861c44
  references: []
  summary: SimToM (Simulated Theory of Mind) is a two-step prompting technique designed
    to enhance large language models' ability to consider specific perspectives. It
    involves first isolating relevant information related to an entity within a context,
    and then asking the model to answer questions solely based on those facts from
    the entity's viewpoint. This method is especially useful for complex scenarios
    with multiple entities, improving the model's understanding and reasoning about
    different perspectives. Implementation includes structured prompts and code examples
    using OpenAI's GPT-4, focusing on perspective-taking and context-specific responses.
    Key concepts include perspective-taking, multi-entity reasoning, and advanced
    prompt engineering for improved model comprehension.
prompting/zero_shot/style_prompting.md:
  ai_references:
  - '[Bounding the Capabilities of Large Language Models in Open Text Generation with
    Prompt Constraints](https://arxiv.org/abs/2302.09185)'
  cross_links: []
  hash: 279e6d51353749a88799508a048d3213
  keywords:
  - style prompting
  - model response
  - writing style
  - tone
  - mood
  - genre
  - email generation
  - OpenAI
  references: []
  summary: The "Style Prompting" documentation explains how to constrain a model's
    responses using stylistic guidelines, including writing style, tone, mood, and
    genre. By specifying these elements, users can ensure that the generated outputs
    align with their intended context and purpose. Code implementation for generating
    tailored email responses is also provided.
  topics:
  - stylistic constraints
  - implementation example
  - code usage
  - email generation
repository-overview.md:
  cross_links: []
  hash: 16a893aa592a4478f0bd70ce059ce714
  references: []
  summary: The Instructor repository provides a comprehensive codebase for structured
    output management, featuring core libraries in the `instructor/` directory, and
    command-line tools in `cli/`. It also includes documentation sources in `docs/`,
    practical examples in `examples/`, and testing scripts in `tests/`. This layout
    supports efficient development, usage, and evaluation of Instructor's functionalities
    for clients, adapters, utilities, and job management, making it essential for
    developers working on structured output tasks.
start-here.md:
  ai_references:
  - '[getting-started.md'
  - examples/index.md
  - concepts/validation.md
  - concepts/partial.md
  - integrations/index.md
  - faq.md]
  cross_links:
  - concepts/index.md
  - concepts/partial.md
  - concepts/validation.md
  - examples/index.md
  - faq.md
  - getting-started.md
  - index.md
  - integrations/index.md
  hash: f92d563955521efd3c8a1b98ef845dd2
  keywords:
  - Instructor
  - Python library
  - structured outputs
  - language models
  - data extraction
  - API integration
  - Pydantic
  - validation
  - OpenAI
  - Claude
  references:
  - getting-started.md
  - examples/index.md
  - concepts/validation.md
  - concepts/partial.md
  - integrations/index.md
  - faq.md
  - examples/index.md
  - concepts/index.md
  summary: This guide provides beginners with an introduction to Instructor, a Python
    library designed for obtaining structured outputs from language models such as
    GPT-4 and Claude. It explains how to use Instructor to define response structures,
    validate outputs, and solve common challenges related to data extraction from
    language models.
  topics: []
templates/concept_template.md:
  ai_references:
  - '[../concepts/related1.md'
  - ../concepts/related2.md
  - ../examples/example1.md
  - ../examples/example2.md]
  cross_links: []
  hash: 07c431f7b4a798b09df99bc65c26543a
  keywords:
  - '[Concept Name'
  - Instructor
  - OpenAI
  - advanced usage
  - best practices
  - error handling
  - language models
  - JSON mode
  - model examples]
  references:
  - concepts/related1.md
  - concepts/related2.md
  - examples/example1.md
  - examples/example2.md
  summary: This documentation covers the [Concept Name], a key feature in the Instructor
    framework designed to enhance user interactions with language models. It provides
    an overview, use cases, basic and advanced implementation examples, and best practices
    for effectively utilizing this concept within various contexts.
  topics:
  - '[Overview'
  - Usage Scenarios
  - Basic and Advanced Usage
  - Working with Different Providers
  - Common Patterns and Best Practices]
templates/cookbook_template.md:
  ai_references:
  - '[related1.md'
  - related2.md
  - related1.md
  - related2.md]
  cross_links: []
  hash: 6e692507cf928faa03b61bf27ca6722d
  keywords:
  - Instructor library
  - OpenAI API
  - data processing
  - Python code
  - structured output
  - API keys
  - implementation steps
  - error handling
  references:
  - concepts/related1.md
  - concepts/related2.md
  - examples/related1.md
  - examples/related2.md
  summary: This example provides a practical guide on how to utilize the Instructor
    library to process data with OpenAI's API effectively. It covers installation,
    prerequisites, step-by-step implementation, and customization options to enhance
    the solution's functionality.
  topics:
  - Use case scenarios
  - prerequisites for setup
  - implementation steps
  - customization options
  - limitations
templates/provider_template.md:
  ai_references: []
  cross_links: []
  hash: 10ab7b29ad592ebc6b4fe5f9bbf88415
  keywords:
  - '[Provider Name'
  - instructor toolkit
  - data extraction
  - API key
  - asynchronous programming]
  references: []
  summary: This guide provides a comprehensive overview of using the instructor toolkit
    with [Provider Name], detailing installation, authentication, and both synchronous
    and asynchronous examples for data extraction. It also covers supported modes,
    streaming support, and the models offered by the provider.
  topics:
  - '[Installation'
  - Authentication
  - Synchronous Example
  - Asynchronous Example
  - Supported Modes]
tutorials/index.md:
  ai_references:
  - '[core concepts](../concepts/index.md)'
  - '[frequently asked questions](../faq.md)'
  - '[practical examples](../examples/index.md)'
  cross_links:
  - concepts/index.md
  - examples/index.md
  - faq.md
  - index.md
  hash: 4da1d02c578cd8b59a99a83811f38f6b
  keywords:
  - Instructor
  - tutorials
  - Jupyter notebooks
  - AI applications
  - learning path
  - structured extraction
  - validation techniques
  - running options
  - Python environment
  - support
  references:
  - concepts/index.md
  - faq.md
  - examples/index.md
  summary: The Instructor Tutorials provide an interactive platform for learning how
    to effectively use the Instructor tool through a structured learning path. Users
    can engage in various tutorials that range from basic concepts to advanced applications,
    building practical skills in AI and LLMs (Large Language Models) along the way.
  topics: []
why.md:
  ai_references:
  - '[../index.md]'
  cross_links:
  - index.md
  hash: 0c27bf9a45800a453a61a41fdb9df8ac
  keywords:
  - '[Instructor'
  - LLMs
  - structured outputs
  - JSON parsing
  - API integration
  - error handling
  - user model
  - retries
  - provider-specific code]
  references: []
  summary: Instructor is an innovative tool designed to streamline the interaction
    with LLMs by providing structured outputs without the usual complexities. It minimizes
    issues such as JSON parsing, retries, and provider-specific code, making it an
    ideal solution for developers needing reliable integration with various LLM providers.
  topics:
  - '[unstructured outputs'
  - benefits of Instructor
  - simplification of LLM integration
  - error handling in LLM applications
  - user modeling with Pydantic]
